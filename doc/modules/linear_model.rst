
.. _linear_model:

=============
النماذج الخطية
=============

.. currentmodule:: sklearn.linear_model

فيما يلي مجموعة من الأساليب المُخصصة للانحدار حيث
من المُتوقع أن تكون القيمة المستهدفة عبارة عن تركيبة خطية من الميزات.
في الصيغة الرياضية، إذا كانت :math:`\hat{y}` هي القيمة
المُتوقعة.

.. math::    \hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p

عبر الوحدة، نُشير إلى المتجه :math:`w = (w_1,
..., w_p)` كـ ``coef_`` و :math:`w_0` كـ ``intercept_``.

لإجراء التصنيف باستخدام النماذج الخطية المُعممة، انظر
:ref:`Logistic_regression`.

.. _ordinary_least_squares:

المربعات الصغرى العادية
=======================

:class:`LinearRegression` يُناسب نموذجًا خطيًا بمعاملات
:math:`w = (w_1, ..., w_p)` لتقليل مجموع البواقي
من المربعات بين الأهداف المُلاحظة في مجموعة البيانات، و
الأهداف التي تنبأ بها التقريب الخطي. رياضيًا، يحل
مشكلة بالشكل:

.. math:: \min_{w} || X w - y||_2^2

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_ols_001.png
   :target: ../auto_examples/linear_model/plot_ols.html
   :align: center
   :scale: 50%

سيأخذ :class:`LinearRegression` في أسلوبه ``fit`` مصفوفات ``X`` و ``y``
ويُخزن المعاملات :math:`w` للنموذج الخطي في عضويته
``coef_``::

    >>> from sklearn import linear_model
    >>> reg = linear_model.LinearRegression()
    >>> reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
    LinearRegression()
    >>> reg.coef_
    array([0.5, 0.5])

تعتمد تقديرات المعاملات للمربعات الصغرى العادية على
استقلال الميزات. عندما تكون الميزات مُرتبطة و
أعمدة مصفوفة التصميم :math:`X` لها تبعية خطية تقريبية،
تصبح مصفوفة التصميم قريبة من المفردة
ونتيجة لذلك، يصبح تقدير المربعات الصغرى حساسًا للغاية
للأخطاء العشوائية في الهدف المُلاحظ، مما يُنتج تباينًا
كبيرًا. يمكن أن تنشأ حالة *التعدد الخطي* هذه، على
سبيل المثال، عندما يتم جمع البيانات بدون تصميم تجريبي.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_ols.py`

المربعات الصغرى غير السالبة
--------------------------

من الممكن تقييد جميع المعاملات لتكون غير سالبة، وهو ما قد
يكون مفيدًا عندما تُمثِّل بعض الكميات المادية أو غير السالبة
بشكل طبيعي (على سبيل المثال، عدد الترددات أو أسعار السلع).
:class:`LinearRegression` يقبل معلمة منطقية ``positive``:
عند تعيينها إلى `True`، يتم تطبيق `المربعات الصغرى غير السالبة
<https://en.wikipedia.org/wiki/Non-negative_least_squares>`_.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_nnls.py`

تعقيد المربعات الصغرى العادية
---------------------------------

يتم حساب حل المربعات الصغرى باستخدام تحليل القيمة
المنفردة لـ X. إذا كانت X مصفوفة ذات شكل `(n_samples, n_features)`،
فإن هذه الطريقة لها تكلفة
:math:`O(n_{\text{samples}} n_{\text{features}}^2)`، بافتراض أن
:math:`n_{\text{samples}} \geq n_{\text{features}}`.

.. _ridge_regression:

انحدار ريدج والتصنيف
===================================

الانحدار
----------

يعالج انحدار :class:`Ridge` بعض مشاكل
:ref:`المربعات الصغرى العادية` عن طريق فرض عقوبة على حجم
المعاملات. تُقلل معاملات ريدج مجموع البواقي
المُعاقب من المربعات:


.. math::

   \min_{w} || X w - y||_2^2 + \alpha ||w||_2^2


تتحكم معلمة التعقيد :math:`\alpha \geq 0` في مقدار
الانكماش: كلما زادت قيمة :math:`\alpha`، زاد مقدار
الانكماش وبالتالي أصبحت المعاملات أكثر قوة للتعددية الخطية.

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_ridge_path_001.png
   :target: ../auto_examples/linear_model/plot_ridge_path.html
   :align: center
   :scale: 50%


كما هو الحال مع النماذج الخطية الأخرى، سيأخذ :class:`Ridge` في أسلوبه ``fit``
مصفوفات ``X`` و ``y`` ويُخزن المعاملات :math:`w` للنموذج الخطي في
عضويته ``coef_``::

    >>> from sklearn import linear_model
    >>> reg = linear_model.Ridge(alpha=.5)
    >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
    Ridge(alpha=0.5)
    >>> reg.coef_
    array([0.34545455, 0.34545455])
    >>> reg.intercept_
    0.13636...

لاحظ أن الفئة :class:`Ridge` تسمح للمستخدم بتحديد أنه
يجب اختيار المحلل تلقائيًا عن طريق تعيين `solver="auto"`. عند تحديد هذا الخيار،
سيختار :class:`Ridge` بين محللات `"lbfgs"` و `"cholesky"`
و `"sparse_cg"`. سيبدأ :class:`Ridge` في فحص الشروط
الموضحة في الجدول التالي من أعلى إلى أسفل. إذا كان الشرط صحيحًا،
فسيتم اختيار المحلل المُقابل.

+-------------+----------------------------------------------------+
| **المحلل**  | **الشرط**                                      |
+-------------+----------------------------------------------------+
| 'lbfgs'     | تم تحديد الخيار ``positive=True``.         |
+-------------+----------------------------------------------------+
| 'cholesky'  | مصفوفة الإدخال X ليست متفرقة.                   |
+-------------+----------------------------------------------------+
| 'sparse_cg' | لم يتم استيفاء أي من الشروط أعلاه.        |
+-------------+----------------------------------------------------+


التصنيف
--------------

يحتوي مُنحدِر :class:`Ridge` على مُتغير مُصنف:
:class:`RidgeClassifier`. يُحوِّل هذا المُصنف أولاً الأهداف الثنائية إلى
``{-1, 1}`` ثم يُعامل المشكلة كمهمة انحدار، ويُحسِّن
نفس الهدف على النحو الوارد أعلاه. تُقابل الفئة المُتوقعة إشارة
تنبؤ المُنحدِر. بالنسبة للتصنيف متعدد الفئات، يتم مُعالجة المشكلة
على أنها انحدار متعدد المخرجات، وتُقابل الفئة المُتوقعة
الإخراج ذي القيمة الأعلى.

قد يبدو من المشكوك فيه استخدام خسارة المربعات الصغرى (المُعاقبة) لملاءمة
نموذج تصنيف بدلاً من خسائر اللوجستية أو المفصلية
الأكثر تقليدية. ومع ذلك، من الناحية العملية، يمكن أن تؤدي جميع هذه النماذج إلى درجات
تحقق متبادل مُتشابهة من حيث الدقة أو الدقة / الاستدعاء، بينما
خسارة المربعات الصغرى المُعاقبة التي يستخدمها :class:`RidgeClassifier` تسمح
باختيار مُختلف تمامًا للمحللات العددية مع ملفات تعريف أداء
حسابية مُتميزة.

يمكن أن يكون :class:`RidgeClassifier` أسرع بكثير من على سبيل المثال
:class:`LogisticRegression` مع عدد كبير من الفئات لأنه يمكنه
حساب مصفوفة الإسقاط :math:`(X^T X)^{-1} X^T` مرة واحدة فقط.

يُشار أحيانًا إلى هذا المُصنف باسم `آلات متجه دعم المربعات الصغرى
<https://en.wikipedia.org/wiki/Least-squares_support-vector_machine>`_
مع نواة خطية.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_ridge_path.py`
* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`
* :ref:`sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py`

تعقيد ريدج
----------------

هذه الطريقة لها نفس ترتيب التعقيد مثل
:ref:`المربعات الصغرى العادية`.

.. FIXME:
.. ليس صحيحًا تمامًا: يتم حل OLS بواسطة SVD، بينما يتم حل Ridge بواسطة
.. أسلوب المعادلات العادية (Cholesky)، هناك فرق كبير في التقليب
.. بين هذه


تعيين معلمة التنظيم: التحقق المتبادل لترك واحد
--------------------------------------------------------------------

:class:`RidgeCV` و :class:`RidgeClassifierCV` يُطبقان
انحدار / تصنيف ريدج مع التحقق المتبادل المُدمج لمعلمة alpha.
يعملان بنفس طريقة :class:`~sklearn.model_selection.GridSearchCV` باستثناء
أنه يتم تعيينه افتراضيًا إلى التحقق المتبادل الفعال لترك واحد.
عند استخدام التحقق :term:`المتبادل` الافتراضي، لا يمكن أن تكون alpha 0 بسبب
الصيغة المُستخدمة لحساب خطأ ترك واحد. انظر [RL2007]_ للتفاصيل.

مثال على الاستخدام::

    >>> import numpy as np
    >>> from sklearn import linear_model
    >>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
    >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
    RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,
          1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))
    >>> reg.alpha_
    0.01

سيؤدي تحديد قيمة سمة :term:`cv` إلى تشغيل استخدام
التحقق المتبادل باستخدام :class:`~sklearn.model_selection.GridSearchCV`،
على سبيل المثال `cv=10` للتحقق المتبادل من 10 طيات، بدلاً من التحقق
المتبادل لترك واحد.

.. dropdown:: المراجع

  .. [RL2007] "ملاحظات حول المربعات الصغرى المُنظّمة", Rifkin & Lippert (التقرير
    الفني <http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf>`_،
    `شرائح الدورة <https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf>`_).

.. _lasso:

Lasso
=====

:class:`Lasso` هو نموذج خطي يُقدِّر معاملات متفرقة.
إنه مفيد في بعض السياقات نظرًا لميله إلى تفضيل الحلول
التي تحتوي على عدد أقل من المعاملات غير الصفرية، مما يُقلل بشكل فعال من عدد
الميزات التي يعتمد عليها الحل المُعين. لهذا السبب،
يُعد Lasso ومتغيراته أساسيين في مجال الاستشعار
المضغوط.
في ظل ظروف مُعينة، يمكنه استرداد مجموعة المعاملات
غير الصفرية الدقيقة (انظر
:ref:`sphx_glr_auto_examples_applications_plot_tomography_l1_reconstruction.py`).

رياضيًا، يتكون من نموذج خطي مع مُصطلح تنظيم مُضاف.
دالة الهدف التي يجب تقليلها هي:

.. math::  \min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}

وبالتالي، يحل تقدير lasso تقليل
عقوبة المربعات الصغرى مع إضافة :math:`\alpha ||w||_1`، حيث
:math:`\alpha` ثابت و :math:`||w||_1` هي قاعدة :math:`\ell_1` لـ
متجه المعاملات.

يستخدم التطبيق في الفئة :class:`Lasso` النزول الإحداثي كـ
خوارزمية لملاءمة المعاملات. انظر :ref:`least_angle_regression`
للحصول على تطبيق آخر::

    >>> from sklearn import linear_model
    >>> reg = linear_model.Lasso(alpha=0.1)
    >>> reg.fit([[0, 0], [1, 1]], [0, 1])
    Lasso(alpha=0.1)
    >>> reg.predict([[1, 1]])
    array([0.8])

تُعد دالة :func:`lasso_path` مفيدة للمهام ذات المستوى الأدنى،
حيث إنها تحسب المعاملات على طول المسار الكامل للقيم المُمكنة.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`
* :ref:`sphx_glr_auto_examples_applications_plot_tomography_l1_reconstruction.py`
* :ref:`sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py`


.. note:: **اختيار الميزات باستخدام Lasso**

      نظرًا لأن انحدار Lasso يُعطي نماذج متفرقة، فإنه
      يمكن استخدامه لاختيار الميزات، كما هو مُفصَّل في
      :ref:`l1_feature_selection`.

.. dropdown:: المراجع

  يشرح المرجعان التاليان التكرارات
  المُستخدمة في محلل النزول الإحداثي لـ scikit-learn، وكذلك
  حساب فجوة الازدواجية المُستخدم للتحكم في التقارب.

  * "مسار التنظيم للنماذج الخطية المُعممة عن طريق النزول الإحداثي",
    Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (`الورقة
    <https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf>`__).
  * "أسلوب نقطة داخلية للمربعات الصغرى المُنظّمة بـ L1 واسعة النطاق,"
    S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
    في IEEE Journal of Selected Topics in Signal Processing، 2007
    (`الورقة <https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf>`__)

تعيين معلمة التنظيم
--------------------------------

تتحكم معلمة ``alpha`` في درجة تفرق المعاملات
المُقدّرة.

استخدام التحقق المتبادل
^^^^^^^^^^^^^^^^^^^^^^^

يُظهِر scikit-learn الكائنات التي تُعيِّن معلمة Lasso ``alpha`` عن طريق
التحقق المتبادل: :class:`LassoCV` و :class:`LassoLarsCV`.
:class:`LassoLarsCV` يعتمد على خوارزمية :ref:`least_angle_regression`
الموضحة أدناه.

بالنسبة لمجموعات البيانات عالية الأبعاد ذات العديد من الميزات الخطية،
:class:`LassoCV` غالبًا ما يكون مُفضلًا. ومع ذلك، فإن :class:`LassoLarsCV` له
ميزة استكشاف المزيد من قيم معلمة `alpha` ذات الصلة، و
إذا كان عدد العينات صغيرًا جدًا مُقارنةً بعدد
الميزات، فإنه غالبًا ما يكون أسرع من :class:`LassoCV`.

.. |lasso_cv_1| image:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_002.png
    :target: ../auto_examples/linear_model/plot_lasso_model_selection.html
    :scale: 48%

.. |lasso_cv_2| image:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_003.png
    :target: ../auto_examples/linear_model/plot_lasso_model_selection.html
    :scale: 48%

.. centered:: |lasso_cv_1| |lasso_cv_2|

.. _lasso_lars_ic:

اختيار النموذج القائم على معيار المعلومات
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

بدلاً من ذلك، يقترح المُقدِّر :class:`LassoLarsIC` استخدام
معيار معلومات Akaike (AIC) ومعيار معلومات Bayes (BIC).
إنه بديل أقل تكلفة من الناحية الحسابية للعثور على القيمة المثلى لـ alpha
حيث يتم حساب مسار التنظيم مرة واحدة فقط بدلاً من k + 1 مرة
عند استخدام التحقق المتبادل k-fold.

في الواقع، يتم حساب هذه المعايير على مجموعة التدريب داخل العينة. باختصار،
يعاقبون الدرجات المُتفائلة جدًا لنماذج Lasso المختلفة بواسطة
مرونتها (راجع قسم "التفاصيل الرياضية" أدناه).

ومع ذلك، تحتاج هذه المعايير إلى تقدير مناسب لدرجات حرية
الحل، ويتم اشتقاقها لعينات كبيرة (نتائج مقاربة) وتفترض أن
النموذج الصحيح هو مرشح قيد التحقيق. كما أنها تميل إلى الانهيار عندما
تكون المشكلة سيئة الشرط (على سبيل المثال، المزيد من الميزات من العينات).

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_lars_ic_001.png
    :target: ../auto_examples/linear_model/plot_lasso_lars_ic.html
    :align: center
    :scale: 50%

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_lars_ic.py`

.. _aic_bic:

معايير AIC و BIC
^^^^^^^^^^^^^^^^^^^^

قد يختلف تعريف AIC (وبالتالي BIC) في الأدبيات. في هذا
القسم، نُقدم المزيد من المعلومات حول المعيار المحسوب في
scikit-learn.

.. dropdown:: التفاصيل الرياضية

  يتم تعريف معيار AIC على النحو التالي:

  .. math::
      AIC = -2 \log(\hat{L}) + 2 d

  حيث :math:`\hat{L}` هو أقصى احتمالية للنموذج و
  :math:`d` هو عدد المعلمات (يُشار إليها أيضًا بدرجات الحرية
  في القسم السابق).

  يستبدل تعريف BIC الثابت :math:`2` بـ :math:`\log(N)`:

  .. math::
      BIC = -2 \log(\hat{L}) + \log(N) d

  حيث :math:`N` هو عدد العينات.

  بالنسبة لنموذج غاوسي خطي، يتم تعريف أقصى احتمالية للسجل على النحو التالي:

  .. math::
      \log(\hat{L}) = - \frac{n}{2} \log(2 \pi) - \frac{n}{2} \ln(\sigma^2) - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{2\sigma^2}

  حيث :math:`\sigma^2` هو تقدير لتباين الضوضاء،
  :math:`y_i` و :math:`\hat{y}_i` هما الأهداف الحقيقية والمتوقعة
  على التوالي، و :math:`n` هو عدد العينات.

  يؤدي توصيل أقصى احتمالية للسجل في صيغة AIC إلى:

  .. math::
      AIC = n \log(2 \pi \sigma^2) + \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sigma^2} + 2 d

  يتم تجاهل المصطلح الأول من التعبير أعلاه أحيانًا لأنه
  ثابت عندما يتم توفير :math:`\sigma^2`. بالإضافة إلى ذلك،
  يُذكر أحيانًا أن AIC يُعادل إحصائية :math:`C_p`
  [12]_. ومع ذلك، بمعنى دقيق، فإنه يُعادل فقط حتى ثابت
  مُعين وعامل ضربي.

  أخيرًا، ذكرنا أعلاه أن :math:`\sigma^2` هو تقدير لـ
  تباين الضوضاء. في :class:`LassoLarsIC` عندما لا يتم توفير معلمة
  `noise_variance` (افتراضي)، يتم تقدير تباين الضوضاء عبر
  المُقدِّر غير المُتحيز [13]_ المُعرَّف على النحو التالي:

  .. math::
      \sigma^2 = \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n - p}

  حيث :math:`p` هو عدد الميزات و :math:`\hat{y}_i` هو
  الهدف المتوقع باستخدام انحدار المربعات الصغرى العادية. لاحظ أن هذه
  الصيغة صالحة فقط عندما `n_samples > n_features`.

  .. rubric:: المراجع

  .. [12] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.
          "حول درجات حرية lasso."
          The Annals of Statistics 35.5 (2007): 2173-2192.
          <0712.0881.pdf>`

  .. [13] :doi:`Cherkassky, Vladimir, and Yunqian Ma.
          "مقارنة اختيار النموذج للانحدار."
          Neural computation 15.7 (2003): 1691-1714.
          <10.1162/089976603321891864>`

مقارنة مع معلمة التنظيم لـ SVM
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

التكافؤ بين ``alpha`` ومعلمة التنظيم لـ SVM،
``C`` مُعطى بواسطة ``alpha = 1 / C`` أو ``alpha = 1 / (n_samples * C)``،
اعتمادًا على المُقدِّر ودالة الهدف الدقيقة التي يُحسِّنها
النموذج.

.. _multi_task_lasso:

Lasso متعدد المهام
================

:class:`MultiTaskLasso` هو نموذج خطي يُقدِّر المعاملات المتفرقة
لمشاكل الانحدار المتعددة بشكل مُشترك: ``y`` هي مصفوفة ثنائية الأبعاد،
ذات شكل ``(n_samples, n_tasks)``. القيد هو أن الميزات
المحددة هي نفسها لجميع مشاكل الانحدار، وتسمى أيضًا المهام.

تُقارن الصورة التالية موقع الإدخالات غير الصفرية في
مصفوفة المعاملات W التي تم الحصول عليها باستخدام Lasso بسيط أو MultiTaskLasso.
تُعطي تقديرات Lasso قيمًا غير صفرية مُشتتة بينما القيم غير الصفرية لـ
MultiTaskLasso هي أعمدة كاملة.

.. |multi_task_lasso_1| image:: ../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_001.png
    :target: ../auto_examples/linear_model/plot_multi_task_lasso_support.html
    :scale: 48%

.. |multi_task_lasso_2| image:: ../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_002.png
    :target: ../auto_examples/linear_model/plot_multi_task_lasso_support.html
    :scale: 48%

.. centered:: |multi_task_lasso_1| |multi_task_lasso_2|

.. centered:: ملاءمة نموذج سلسلة زمنية، بفرض أن تكون أي ميزة نشطة نشطة في جميع الأوقات.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_multi_task_lasso_support.py`


.. dropdown:: التفاصيل الرياضية

  رياضيًا، يتكون من نموذج خطي مُدرَّب بمزيج من
  قاعدة :math:`\ell_1` :math:`\ell_2` لتنظيم.
  دالة الهدف التي يجب تقليلها هي:

  .. math::  \min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}} ^ 2 + \alpha ||W||_{21}}

  حيث يُشير :math:`\text{Fro}` إلى قاعدة Frobenius

  .. math:: ||A||_{\text{Fro}} = \sqrt{\sum_{ij} a_{ij}^2}

  و :math:`\ell_1` :math:`\ell_2` تقرأ

  .. math:: ||A||_{2 1} = \sum_i \sqrt{\sum_j a_{ij}^2}.

  يستخدم التطبيق في الفئة :class:`MultiTaskLasso`
  النزول الإحداثي كخوارزمية لملاءمة المعاملات.

.. _elastic_net:

Elastic-Net
===========
:class:`ElasticNet` هو نموذج انحدار خطي مُدرَّب بكل من
تنظيم :math:`\ell_1` و :math:`\ell_2` للمعاملات.
يسمح هذا المزيج بتعلم نموذج متفرق حيث يكون عدد قليل من
الأوزان غير صفري مثل :class:`Lasso`، مع الحفاظ على
خصائص التنظيم لـ :class:`Ridge`. نتحكم في التركيبة
المُحدبة لـ :math:`\ell_1` و :math:`\ell_2` باستخدام معلمة ``l1_ratio``.

Elastic-net مفيد عندما يكون هناك العديد من الميزات
المُرتبطة ببعضها البعض. من المرجح أن يختار Lasso واحدًا من هذه
بشكل عشوائي، بينما من المرجح أن يختار Elastic-Net كليهما.

ميزة عملية للمُفاضلة بين Lasso و Ridge هي أنه
يسمح لـ Elastic-Net بوراثة بعض استقرار Ridge تحت الدوران.

دالة الهدف التي يجب تقليلها هي في هذه الحالة

.. math::

    \min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +
    \frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}


.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_lasso_lars_elasticnet_path_002.png
   :target: ../auto_examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.html
   :align: center
   :scale: 50%

يمكن استخدام الفئة :class:`ElasticNetCV` لضبط المعلمات
``alpha`` (:math:`\alpha`) و ``l1_ratio`` (:math:`\rho`) عن طريق التحقق
المتبادل.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py`

.. dropdown:: المراجع

  يشرح المرجعان التاليان التكرارات
  المُستخدمة في محلل النزول الإحداثي لـ scikit-learn، وكذلك
  حساب فجوة الازدواجية المُستخدم للتحكم في التقارب.

  * "مسار التنظيم للنماذج الخطية المُعممة عن طريق النزول الإحداثي",
    Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (`الورقة
    <https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf>`__).
  * "أسلوب نقطة داخلية للمربعات الصغرى المُنظّمة بـ L1 واسعة النطاق,"
    S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
    في IEEE Journal of Selected Topics in Signal Processing، 2007
    (`الورقة <https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf>`__)

.. _multi_task_elastic_net:

Elastic-Net متعدد المهام
======================

:class:`MultiTaskElasticNet` هو نموذج شبكة مرنة يُقدِّر المعاملات
المتفرقة لمشاكل الانحدار المتعددة بشكل مُشترك: ``Y`` هي مصفوفة ثنائية
الأبعاد ذات شكل ``(n_samples, n_tasks)``. القيد هو أن
الميزات المحددة هي نفسها لجميع مشاكل الانحدار، وتسمى أيضًا
المهام.

رياضيًا، يتكون من نموذج خطي مُدرَّب بمزيج من
قاعدة :math:`\ell_1` :math:`\ell_2` وقاعدة :math:`\ell_2` للتنظيم.
دالة الهدف التي يجب تقليلها هي:

.. math::

    \min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}}^2 + \alpha \rho ||W||_{2 1} +
    \frac{\alpha(1-\rho)}{2} ||W||_{\text{Fro}}^2}

يستخدم التطبيق في الفئة :class:`MultiTaskElasticNet` النزول الإحداثي كـ
خوارزمية لملاءمة المعاملات.

يمكن استخدام الفئة :class:`MultiTaskElasticNetCV` لضبط المعلمات
``alpha`` (:math:`\alpha`) و ``l1_ratio`` (:math:`\rho`) عن طريق التحقق
المتبادل.

.. _least_angle_regression:

انحدار الزاوية الصغرى
======================

انحدار الزاوية الصغرى (LARS) هو خوارزمية انحدار لـ
البيانات عالية الأبعاد، طورتها برادلي إيفرون، تريفور هاستي، إيان
جونستون وروبرت تيبيشيراني. LARS مُشابه لانحدار
الخطوة الأمامية. في كل خطوة، يجد الميزة الأكثر ارتباطًا بـ
الهدف. عندما يكون هناك العديد من الميزات ذات الارتباط المتساوي، بدلاً من
الاستمرار على طول نفس الميزة، فإنه يتقدم في اتجاه متساوي الزوايا
بين الميزات.

مزايا LARS هي:

- إنه فعال عدديًا في السياقات التي يكون فيها عدد الميزات
  أكبر بكثير من عدد العينات.

- إنه سريع حسابيًا مثل الاختيار الأمامي وله
  نفس ترتيب التعقيد مثل المربعات الصغرى العادية.

- إنه يُنتج مسار حل خطي مُتكسر كامل، وهو
  مفيد في التحقق المتبادل أو المحاولات المُشابهة لضبط النموذج.

- إذا كانت ميزتان مُرتبطتين تقريبًا بنفس القدر بالهدف،
  فإن معاملاتهما يجب أن تزداد بنفس المعدل تقريبًا.
  وبالتالي تتصرف الخوارزمية كما يتوقع الحدس، و
  هي أيضًا أكثر استقرارًا.

- من السهل تعديله لإنتاج حلول لمُقدِّرات أخرى،
  مثل Lasso.

تشمل عيوب أسلوب LARS ما يلي:

- لأن LARS يعتمد على إعادة ملاءمة متكررة لـ
  البواقي، سيبدو حساسًا بشكل خاص لـ
  آثار الضوضاء. تمت مناقشة هذه المشكلة بالتفصيل بواسطة Weisberg
  في قسم المناقشة من مقالة Efron وآخرون (2004) Annals of
  Statistics.

يمكن استخدام نموذج LARS عبر المُقدِّر :class:`Lars`، أو تطبيقه
منخفض المستوى :func:`lars_path` أو :func:`lars_path_gram`.


LARS Lasso
==========

:class:`LassoLars` هو نموذج lasso مُطبق باستخدام خوارزمية LARS،
وعلى عكس التطبيق القائم على النزول الإحداثي،
يُعطي هذا الحل الدقيق، وهو خطي متعدد التعريف كـ
دالة لقاعدة معاملاته.

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_lasso_lars_elasticnet_path_001.png
   :target: ../auto_examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.html
   :align: center
   :scale: 50%

::

   >>> from sklearn import linear_model
   >>> reg = linear_model.LassoLars(alpha=.1)
   >>> reg.fit([[0, 0], [1, 1]], [0, 1])
   LassoLars(alpha=0.1)
   >>> reg.coef_
   array([0.6..., 0.        ])

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py`

تُوفر خوارزمية Lars المسار الكامل للمعاملات على طول
معلمة التنظيم مجانًا تقريبًا، وبالتالي فإن العملية الشائعة
هي استرداد المسار باستخدام إحدى الدالتين :func:`lars_path`
أو :func:`lars_path_gram`.

.. dropdown:: الصيغة الرياضية

  الخوارزمية مُشابهة لانحدار الخطوة الأمامية، ولكن بدلاً من
  تضمين الميزات في كل خطوة، يتم
  زيادة المعاملات المُقدَّرة في اتجاه متساوي الزوايا لكل ارتباط واحد
  مع الباقي.

  بدلاً من إعطاء نتيجة متجه، يتكون حل LARS من
  منحنى يُشير إلى الحل لكل قيمة لقاعدة :math:`\ell_1` لـ
  متجه المعلمة. يتم تخزين مسار المعاملات الكامل في المصفوفة
  ``coef_path_`` ذات الشكل `(n_features, max_features + 1)`. العمود الأول
  دائمًا صفر.

  .. rubric:: المراجع

  * تم تفصيل الخوارزمية الأصلية في الورقة `انحدار الزاوية الصغرى
    <https://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf>`_
    بواسطة Hastie وآخرون.

.. _omp:

مطاردة التطابق المتعامد (OMP)
=================================
:class:`OrthogonalMatchingPursuit` و :func:`orthogonal_mp` يُطبقان
خوارزمية OMP لتقريب ملاءمة نموذج خطي مع قيود مفروضة على
عدد المعاملات غير الصفرية (أي قاعدة :math:`\ell_0` الزائفة).

نظرًا لكونه أسلوب اختيار ميزات أمامي مثل :ref:`least_angle_regression`،
يمكن لمطاردة التطابق المتعامد تقريب متجه الحل الأمثل بعدد
ثابت من العناصر غير الصفرية:

.. math::
    \underset{w}{\operatorname{arg\,min\,}}  ||y - Xw||_2^2 \text{ subject to } ||w||_0 \leq n_{\text{nonzero_coefs}}

بدلاً من ذلك، يمكن لمطاردة التطابق المتعامد استهداف خطأ مُحدد بدلاً من
عدد مُحدد من المعاملات غير الصفرية. يمكن التعبير عن هذا كـ:

.. math::
    \underset{w}{\operatorname{arg\,min\,}} ||w||_0 \text{ subject to } ||y-Xw||_2^2 \leq \text{tol}


يعتمد OMP على خوارزمية جشعة تتضمن في كل خطوة الذرة الأكثر
ارتباطًا بالباقي الحالي. إنه مُشابه لأسلوب
مطاردة التطابق (MP) الأبسط، ولكنه أفضل من حيث أنه في كل تكرار،
يتم إعادة حساب الباقي باستخدام إسقاط متعامد على فضاء
عناصر القاموس المختارة سابقًا.


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_omp.py`

.. dropdown:: المراجع

  * https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf

  * `مطاردات التطابق مع قواميس التردد الزمني
    <https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf>`_،
    S. G. Mallat، Z. Zhang،

.. _bayesian_regression:

الانحدار البايزي
===================

يمكن استخدام تقنيات الانحدار البايزي لتضمين معلمات التنظيم
في إجراء التقدير: لا يتم تعيين معلمة التنظيم بمعنى صارم
ولكن يتم ضبطها على البيانات الموجودة.

يمكن القيام بذلك عن طريق إدخال `مُسبقات غير إعلامية
<https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors>`__
على المعلمات الفائقة للنموذج.
تنظيم :math:`\ell_{2}` المُستخدم في :ref:`ridge_regression`
يُعادل إيجاد أقصى تقدير لاحق في ظل مُسبق غاوسي
على المعاملات :math:`w` بدقة :math:`\lambda^{-1}`.
بدلاً من تعيين `\lambda` يدويًا، من الممكن مُعالجتها كمتغير
عشوائي يتم تقديره من البيانات.

للحصول على نموذج احتمالي بالكامل، يُفترض أن يكون الإخراج :math:`y`
موزعًا غاوسيًا حول :math:`X w`:

.. math::  p(y|X,w,\alpha) = \mathcal{N}(y|X w,\alpha^{-1})

حيث يتم مُعالجة :math:`\alpha` مرة أخرى كمتغير عشوائي يتم
تقديره من البيانات.

مزايا الانحدار البايزي هي:

- يتكيف مع البيانات الموجودة.

- يمكن استخدامه لتضمين معلمات التنظيم في
  إجراء التقدير.

تشمل عيوب الانحدار البايزي ما يلي:

- يمكن أن يكون استدلال النموذج مُستهلكًا للوقت.

.. dropdown:: المراجع

  * تم تقديم مقدمة جيدة للأساليب البايزية في C. Bishop: Pattern
    Recognition and Machine learning

  * تم تفصيل الخوارزمية الأصلية في كتاب `التعلم البايزي للشبكات
    العصبية` بواسطة Radford M. Neal

.. _bayesian_ridge_regression:

انحدار ريدج البايزي
-------------------------

:class:`BayesianRidge` يُقدِّر نموذجًا احتماليًا لـ
مشكلة الانحدار كما هو موضح أعلاه.
المُسبق للمعامل :math:`w` مُعطى بواسطة توزيع غاوسي كروي:

.. math:: p(w|\lambda) =
    \mathcal{N}(w|0,\lambda^{-1}\mathbf{I}_{p})

يتم اختيار المُسبقات على :math:`\alpha` و :math:`\lambda` لتكون `توزيعات جاما
<https://en.wikipedia.org/wiki/Gamma_distribution>`__،
المُسبق المُقترن لدقة التوزيع الغاوسي. يُسمى النموذج الناتج
*انحدار ريدج البايزي*، وهو مُشابه لـ :class:`Ridge` الكلاسيكي.

يتم تقدير المعلمات :math:`w` و :math:`\alpha` و :math:`\lambda`
بشكل مُشترك أثناء ملاءمة النموذج، مع تقدير معلمات التنظيم
:math:`\alpha` و :math:`\lambda` عن طريق تعظيم
*احتمالية السجل الهامشي*. يعتمد تطبيق scikit-learn
على الخوارزمية الموضحة في الملحق أ من (Tipping، 2001)
حيث يتم تحديث المعلمات :math:`\alpha` و :math:`\lambda` كما هو مُقترح
في (MacKay، 1992). يمكن تعيين القيمة الأولية لإجراء التعظيم
باستخدام المعلمات الفائقة ``alpha_init`` و ``lambda_init``.

هناك أربع معلمات فائقة أخرى، :math:`\alpha_1` و :math:`\alpha_2`
و :math:`\lambda_1` و :math:`\lambda_2` لتوزيعات جاما السابقة على
:math:`\alpha` و :math:`\lambda`. عادةً ما يتم اختيار هذه لتكون
*غير إعلامية*. افتراضيًا :math:`\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}`.

يُستخدم انحدار ريدج البايزي للانحدار::

    >>> from sklearn import linear_model
    >>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
    >>> Y = [0., 1., 2., 3.]
    >>> reg = linear_model.BayesianRidge()
    >>> reg.fit(X, Y)
    BayesianRidge()

بعد ملاءمته، يمكن استخدام النموذج للتنبؤ بقيم جديدة::

    >>> reg.predict([[1, 0.]])
    array([0.50000013])

يمكن الوصول إلى معاملات :math:`w` للنموذج::

    >>> reg.coef_
    array([0.49999993, 0.49999993])

نظرًا لإطار العمل البايزي، فإن الأوزان التي تم العثور عليها تختلف قليلاً عن
تلك التي تم العثور عليها بواسطة :ref:`المربعات الصغرى العادية`. ومع ذلك، فإن انحدار
ريدج البايزي أكثر قوة للمشاكل سيئة الوضع.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`

.. dropdown:: المراجع

  * القسم 3.3 في Christopher M. Bishop: Pattern Recognition and Machine Learning، 2006

  * David J. C. MacKay، `الاستيفاء البايزي <https://citeseerx.ist.psu.edu/doc_view/pid/b14c7cc3686e82ba40653c6dff178356a33e5e2c>`_، 1992.

  * Michael E. Tipping، `التعلم البايزي المتفرق وآلة متجه الصلة <https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf>`_، 2001.

.. _automatic_relevance_determination:

تحديد الصلة التلقائي - ARD
---------------------------------------

تحديد الصلة التلقائي (كما هو مُطبق في
:class:`ARDRegression`) هو نوع من النماذج الخطية يُشبه جدًا
`انحدار ريدج البايزي`_، ولكنه يؤدي إلى معاملات :math:`w` أكثر تفرقًا
[1]_ [2]_.

:class:`ARDRegression` يضع مُسبقًا مُختلفًا على :math:`w`: إنه يُسقط
التوزيع الغاوسي الكروي لتوزيع غاوسي بيضاوي مُتمركز. هذا يعني أنه يمكن
رسم كل معامل :math:`w_{i}` من
توزيع غاوسي، مُتمركز على الصفر وبدقة
:math:`\lambda_{i}`:

.. math:: p(w|\lambda) = \mathcal{N}(w|0,A^{-1})

مع كون :math:`A` مصفوفة قطرية موجبة مُحددة و
:math:`\text{diag}(A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}`.

على عكس `انحدار ريدج البايزي`_، كل إحداثي من
:math:`w_{i}` له انحرافه المعياري الخاص :math:`\frac{1}{\lambda_i}`.
يتم اختيار المُسبق على جميع :math:`\lambda_i` ليكون نفس توزيع جاما
المُعطى بواسطة المعلمات الفائقة :math:`\lambda_1` و :math:`\lambda_2`.

يُعرف ARD أيضًا في الأدبيات باسم *التعلم البايزي المتفرق* و *آلة
متجه الصلة* [3]_ [4]_. لمقارنة مُفصلة بين ARD و `انحدار
ريدج البايزي`_، انظر المثال أدناه.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py`


.. rubric:: المراجع

.. [1] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1

.. [2] David Wipf and Srikantan Nagarajan: `نظرة جديدة على تحديد الصلة التلقائي <https://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf>`_

.. [3] Michael E. Tipping: `التعلم البايزي المتفرق وآلة متجه الصلة <https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf>`_

.. [4] Tristan Fletcher: `شرح آلات متجه الصلة <https://citeseerx.ist.psu.edu/doc_view/pid/3dc9d625404fdfef6eaccc3babddefe4c176abd4>`_

.. _Logistic_regression:

الانحدار اللوجستي
===================

يتم تطبيق الانحدار اللوجستي في :class:`LogisticRegression`. على الرغم من
اسمه، فإنه يتم تطبيقه كنموذج خطي للتصنيف بدلاً من
الانحدار من حيث تسمية scikit-learn / ML. يُعرف الانحدار
اللوجستي أيضًا في الأدبيات باسم انحدار logit،
تصنيف الحد الأقصى للإنتروبيا (MaxEnt) أو المُصنف اللوغاريتمي الخطي. في هذا
النموذج، يتم نمذجة الاحتمالات التي تصف النتائج المُمكنة لتجربة
واحدة باستخدام `دالة لوجستية
<https://en.wikipedia.org/wiki/Logistic_function>`_.

يمكن لهذا التطبيق ملاءمة الانحدار اللوجستي الثنائي أو واحد مقابل البقية أو
متعدد الحدود مع تنظيم :math:`\ell_1` أو :math:`\ell_2` أو Elastic-Net
اختياري.

.. note:: **التنظيم**

    يتم تطبيق التنظيم افتراضيًا، وهو أمر شائع في التعلم
    الآلي ولكن ليس في الإحصاء. ميزة أخرى للتنظيم هي
    أنه يُحسِّن الاستقرار العددي. لا يُعادل التنظيم
    تعيين C إلى قيمة عالية جدًا.

.. note:: **الانحدار اللوجستي كحالة خاصة من النماذج الخطية المُعممة (GLM)**

    الانحدار اللوجستي هو حالة خاصة من
    :ref:`generalized_linear_models` مع توزيع شرطي ذي حدين / برنولي
    وربط Logit. يمكن استخدام الناتج العددي للانحدار
    اللوجستي، وهو الاحتمال المُتوقع، كمُصنف
    عن طريق تطبيق عتبة (افتراضيًا 0.5) عليه. هذه هي الطريقة التي يتم
    تطبيقها في scikit-learn، لذلك يتوقع هدفًا فئويًا، مما
    يجعل الانحدار اللوجستي مُصنفًا.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_l1_l2_sparsity.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_multinomial.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_20newsgroups.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_mnist.py`
* :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`

الحالة الثنائية
-----------

لتسهيل التدوين، نفترض أن الهدف :math:`y_i` يأخذ قيمًا في
المجموعة :math:`\{0, 1\}` لنقطة البيانات :math:`i`.
بمجرد ملاءمته، أسلوب :meth:`~sklearn.linear_model.LogisticRegression.predict_proba`
لـ :class:`~sklearn.linear_model.LogisticRegression` يتنبأ
باحتمالية الفئة الإيجابية :math:`P(y_i=1|X_i)` كـ

.. math:: \hat{p}(X_i) = \operatorname{expit}(X_i w + w_0) = \frac{1}{1 + \exp(-X_i w - w_0)}.


كمشكلة تحسين، يُقلل الانحدار اللوجستي
الثنائي مع مُصطلح التنظيم :math:`r(w)` دالة التكلفة التالية:

.. math::
    :name: regularized-logistic-loss

    \min_{w} \frac{1}{S}\sum_{i=1}^n s_i
    \left(-y_i \log(\hat{p}(X_i)) - (1 - y_i) \log(1 - \hat{p}(X_i))\right)
    + \frac{r(w)}{S C}\,,

حيث :math:`{s_i}` تُقابل الأوزان المُعيَّنة من قبل المستخدم لـ
عينة تدريب مُحددة (يتم تشكيل المتجه :math:`s` عن طريق
ضرب أوزان الفئة وأوزان العينة حسب العنصر)،
والمجموع :math:`S = \sum_{i=1}^n s_i`.

نُقدم حاليًا أربعة خيارات لمُصطلح التنظيم :math:`r(w)` عبر
وسيطة `penalty`:

+----------------+-------------------------------------------------+
| penalty        | :math:`r(w)`                                    |
+================+=================================================+
| `None`         | :math:`0`                                       |
+----------------+-------------------------------------------------+
| :math:`\ell_1` | :math:`\|w\|_1`                                 |
+----------------+-------------------------------------------------+
| :math:`\ell_2` | :math:`\frac{1}{2}\|w\|_2^2 = \frac{1}{2}w^T w` |
+----------------+-------------------------------------------------+
| `ElasticNet`   | :math:`\frac{1 - \rho}{2}w^T w + \rho \|w\|_1`  |
+----------------+-------------------------------------------------+

بالنسبة لـ ElasticNet، :math:`\rho` (التي تُقابل معلمة `l1_ratio`)
تتحكم في قوة تنظيم :math:`\ell_1` مقابل تنظيم
:math:`\ell_2`. Elastic-Net يُعادل :math:`\ell_1` عندما
:math:`\rho = 1` ويُعادل :math:`\ell_2` عندما :math:`\rho=0`.

لاحظ أن مقياس أوزان الفئة وأوزان العينة سيؤثر على
مشكلة التحسين. على سبيل المثال، ضرب أوزان العينة في
ثابت :math:`b>0` يُعادل ضرب قوة التنظيم (العكسية) `C` في
:math:`b`.

حالة متعددة الحدود
----------------

يمكن تمديد الحالة الثنائية إلى :math:`K` فئات مما يؤدي إلى
الانحدار اللوجستي متعدد الحدود، انظر أيضًا `النموذج اللوغاريتمي الخطي
<https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model>`_.

.. note::
   من الممكن تحديد معلمات نموذج تصنيف :math:`K` فئة
   باستخدام :math:`K-1` متجهات وزن فقط، وترك احتمال فئة واحدة مُحددًا
   تمامًا بواسطة احتمالات الفئات الأخرى من خلال الاستفادة من حقيقة أن جميع
   احتمالات الفئات يجب أن يصل مجموعها إلى واحد. نختار عمدًا زيادة معلمات
   النموذج باستخدام :math:`K` متجهات وزن لسهولة التطبيق وللحفاظ على
   التحيز الاستقرائي المتماثل فيما يتعلق بترتيب الفئات، انظر [16]_. يصبح هذا التأثير
   مهمًا بشكل خاص عند استخدام التنظيم. قد يكون اختيار زيادة المعلمات
   ضارًا بالنماذج غير المُعاقبة لأنه قد لا يكون الحل فريدًا، كما هو موضح
   في [16]_.

.. dropdown:: التفاصيل الرياضية

  افترض أن :math:`y_i \in {1, \ldots, K}` هو متغير الهدف المُرمَّز (ترتيبيًا)
  للملاحظة :math:`i`.
  بدلاً من متجه معاملات واحد، لدينا الآن
  مصفوفة من المعاملات :math:`W` حيث يتوافق كل متجه صف :math:`W_k` مع الفئة
  :math:`k`. نهدف إلى التنبؤ باحتمالات الفئة :math:`P(y_i=k|X_i)` عبر
  :meth:`~sklearn.linear_model.LogisticRegression.predict_proba` كـ:

  .. math:: \hat{p}_k(X_i) = \frac{\exp(X_i W_k + W_{0, k})}{\sum_{l=0}^{K-1} \exp(X_i W_l + W_{0, l})}.

  يصبح الهدف للتحسين

  .. math::
    \min_W -\frac{1}{S}\sum_{i=1}^n \sum_{k=0}^{K-1} s_{ik} [y_i = k] \log(\hat{p}_k(X_i))
    + \frac{r(W)}{S C}\,,

  حيث :math:`[P]` يُمثل قوس إيفرسون الذي يُقيَّم إلى :math:`0`
  إذا كان :math:`P` خطأ، وإلا فإنه يُقيَّم إلى :math:`1`.

  مرة أخرى، :math:`s_{ik}` هي الأوزان التي يُعيِّنها المستخدم (ضرب أوزان
  العينة وأوزان الفئة) مع مجموعها :math:`S = \sum_{i=1}^n \sum_{k=0}^{K-1} s_{ik}`.

  نُقدم حاليًا أربعة خيارات
  لمُصطلح التنظيم :math:`r(W)` عبر وسيطة `penalty`، حيث :math:`m`
  هو عدد الميزات:

  +----------------+----------------------------------------------------------------------------------+
  | penalty        | :math:`r(W)`                                                                     |
  +================+==================================================================================+
  | `None`         | :math:`0`                                                                        |
  +----------------+----------------------------------------------------------------------------------+
  | :math:`\ell_1` | :math:`\|W\|_{1,1} = \sum_{i=1}^m\sum_{j=1}^{K}|W_{i,j}|`                        |
  +----------------+----------------------------------------------------------------------------------+
  | :math:`\ell_2` | :math:`\frac{1}{2}\|W\|_F^2 = \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^{K} W_{i,j}^2`   |
  +----------------+----------------------------------------------------------------------------------+
  | `ElasticNet`   | :math:`\frac{1 - \rho}{2}\|W\|_F^2 + \rho \|W\|_{1,1}`                           |
  +----------------+----------------------------------------------------------------------------------+

.. _logistic_regression_solvers:

المحللات
-------

المحللات المُطبقة في الفئة :class:`LogisticRegression`
هي "lbfgs" و "liblinear" و "newton-cg" و "newton-cholesky" و "sag" و "saga":

يُلخص الجدول التالي العقوبات ومتعدد الحدود متعدد الفئات المدعومة
من قِبل كل محلل:

+------------------------------+-----------------+-------------+-----------------+-----------------------+-----------+------------+
|                              |                       **المحللات**                                                                |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| **العقوبات**                | **'lbfgs'** | **'liblinear'** | **'newton-cg'** | **'newton-cholesky'** | **'sag'** | **'saga'** |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| عقوبة L2                   |     نعم     |       لا        |       نعم       |     لا                |    نعم    |    نعم     |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| عقوبة L1                   |     لا      |       نعم       |       لا        |     لا                |    لا     |    نعم     |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| Elastic-Net (L1 + L2)        |     لا      |       لا        |       لا        |     لا                |    لا     |    نعم     |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| لا عقوبة ('none')          |     نعم     |       لا        |       نعم       |     نعم               |    نعم    |    نعم     |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| **دعم متعدد الفئات**       |                                                                                                  |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| متعدد الحدود متعدد الفئات       |     نعم     |       لا        |       نعم       |     لا                |    نعم    |    نعم     |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| **السلوكيات**                |                                                                                                  |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| مُعاقبة التقاطع (سيئ) |     لا      |       نعم       |       لا        |     لا                |    لا     |    لا      |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| أسرع لمجموعات البيانات الكبيرة    |     لا      |       لا        |       لا        |     لا                |    نعم    |    نعم     |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
| قوي لمجموعات البيانات غير المُقيَّسة  |     نعم     |       نعم       |       نعم       |     نعم               |    لا     |    لا      |
+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+

يتم استخدام محلل "lbfgs" افتراضيًا لمتانته. بالنسبة لمجموعات البيانات
الكبيرة، عادةً ما يكون محلل "saga" أسرع.
لمجموعة البيانات الكبيرة، يمكنك أيضًا التفكير في استخدام :class:`SGDClassifier`
مع `loss="log_loss"`، والذي قد يكون أسرع ولكنه يتطلب المزيد من الضبط.

.. _liblinear_differences:

الاختلافات بين المحللات
^^^^^^^^^^^^^^^^^^^^^^^^^^^

قد يكون هناك اختلاف في الدرجات التي تم الحصول عليها بين
:class:`LogisticRegression` مع ``solver=liblinear`` أو
:class:`~sklearn.svm.LinearSVC` ومكتبة liblinear الخارجية
مباشرةً، عندما ``fit_intercept=False`` و ``coef_`` المُناسب (أو)
البيانات التي سيتم التنبؤ بها هي أصفار. هذا لأن للعينة (العينات) ذات
``decision_function`` صفر، يتنبأ :class:`LogisticRegression` و
:class:`~sklearn.svm.LinearSVC` بالفئة
السالبة، بينما يتنبأ liblinear بالفئة الموجبة. لاحظ أن النموذج
مع ``fit_intercept=False`` ويحتوي على العديد من العينات مع ``decision_function``
صفر، من المرجح أن يكون نموذجًا سيئًا مُفرطًا في التعميم، ويُنصح بتعيين
``fit_intercept=True`` وزيادة ``intercept_scaling``.

.. dropdown:: تفاصيل المحللات

  * يستخدم محلل "liblinear" خوارزمية النزول الإحداثي (CD)، ويعتمد
    على مكتبة C ++ `LIBLINEAR الممتازة
    <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`_، التي يتم شحنها مع
    scikit-learn. ومع ذلك، لا يمكن لخوارزمية CD المُطبقة في liblinear تعلم
    نموذج متعدد الحدود (متعدد الفئات) حقيقي؛ بدلاً من ذلك، يتم
    تحليل مشكلة التحسين بطريقة "واحد مقابل البقية" لذلك يتم
    تدريب مُصنِّفات ثنائية مُنفصلة لجميع الفئات. يحدث هذا تحت الغطاء،
    لذلك تتصرف مثيلات :class:`LogisticRegression` التي تستخدم هذا المحلل
    كمُصنِّفات متعددة الفئات. بالنسبة لتنظيم :math:`\ell_1`، يسمح :func:`sklearn.svm.l1_min_c`
    بحساب الحد الأدنى لـ C من أجل الحصول على نموذج غير "فارغ" (جميع أوزان
    الميزات إلى الصفر).

  * تدعم محللات "lbfgs" و "newton-cg" و "sag" فقط تنظيم :math:`\ell_2`
    أو عدم التنظيم، ووجد أنها تتقارب بشكل أسرع بالنسبة لبعض
    البيانات عالية الأبعاد. يؤدي تعيين `multi_class` إلى "multinomial" مع هذه المحللات
    إلى تعلم نموذج انحدار لوجستي متعدد الحدود حقيقي [5]_، مما يعني أن
    تقديرات احتماله يجب أن تكون مُعايرة بشكل أفضل من إعداد "واحد مقابل
    البقية" الافتراضي.

  * يستخدم محلل "sag" نزول التدرج العشوائي المتوسط [6]_. إنه أسرع
    من المحللات الأخرى لمجموعات البيانات الكبيرة، عندما يكون كل من عدد العينات وعدد
    الميزات كبيرًا.

  * محلل "saga" [7]_ هو مُتغير من "sag" يدعم أيضًا
    `penalty="l1"` غير السلس. لذلك هذا هو المحلل المُفضل
    للانحدار اللوجستي متعدد الحدود المتفرق. وهو أيضًا المحلل الوحيد الذي يدعم
    `penalty="elasticnet"`.

  * "lbfgs" هي خوارزمية تحسين تُقارب
    خوارزمية Broyden-Fletcher-Goldfarb-Shanno [8]_، التي تنتمي إلى
    أساليب شبه نيوتن. على هذا النحو، يمكنها التعامل مع مجموعة واسعة من بيانات
    التدريب المُختلفة، وبالتالي فهي المحلل الافتراضي. ومع ذلك، فإن أدائها يُعاني
    على مجموعات البيانات ذات المقياس السيئ وعلى مجموعات البيانات ذات الميزات الفئوية
    المُرمَّزة أحاديًا مع فئات نادرة.

  * محلل "newton-cholesky" هو محلل نيوتن دقيق يحسب مصفوفة
    هيسيان ويحل النظام الخطي الناتج. إنه خيار جيد جدًا
    لـ `n_samples` >> `n_features`، لكن لديه بعض أوجه القصور: يتم دعم
    تنظيم :math:`\ell_2` فقط. علاوة على ذلك، نظرًا لحساب مصفوفة
    هيسيان صراحةً، فإن استخدام الذاكرة له تبعية تربيعية على `n_features`
    وكذلك على `n_classes`. ونتيجة لذلك، يتم تطبيق مخطط واحد مقابل البقية فقط
    للحالة متعددة الفئات.

  لمقارنة بعض هذه المحللات، انظر [9]_.

  .. rubric:: المراجع

  .. [5] Christopher M. Bishop: Pattern Recognition and Machine Learning، الفصل 4.3.4

  .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `تقليل المجاميع المحدودة باستخدام متوسط التدرج العشوائي. <https://hal.inria.fr/hal-00860051/document>`_

  .. [7] Aaron Defazio, Francis Bach, Simon Lacoste-Julien:
      :arxiv:`SAGA: أسلوب تدرج تزايدي سريع مع دعم لـ
      أهداف مُركبة غير مُحدبة بقوة. <1407.0202>`

  .. [8] https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm

  .. [9] Thomas P. Minka `"مقارنة بين المُحسِّنات العددية للانحدار اللوجستي"
          <https://tminka.github.io/papers/logreg/minka-logreg.pdf>`_

  .. [16] :arxiv:`Simon, Noah, J. Friedman and T. Hastie.
      "خوارزمية نزول الكتل للانحدار متعدد الاستجابات
      و متعدد الحدود المُعاقب للمجموعة." <1311.6529>`


.. note:: **اختيار الميزات مع الانحدار اللوجستي المتفرق**

   يُعطي الانحدار اللوجستي مع عقوبة :math:`\ell_1` نماذج متفرقة، ويمكن
   بالتالي استخدامه لإجراء اختيار الميزات، كما هو مُفصل في
   :ref:`l1_feature_selection`.

.. note:: **تقدير قيمة p**

    من الممكن الحصول على قيم p وفترات ثقة لـ
    المعاملات في حالات الانحدار بدون عقوبة. تدعم حزمة `statsmodels
    <https://pypi.org/project/statsmodels/>`_ هذا أصلاً.
    داخل sklearn، يمكن للمرء استخدام التمهيد بدلاً من ذلك أيضًا.


:class:`LogisticRegressionCV` يُطبق الانحدار اللوجستي مع
دعم التحقق المتبادل المُدمج، لإيجاد معلمات `C` و `l1_ratio`
المثلى وفقًا للسمة ``scoring``. تم العثور على محللات "newton-cg" و "sag"
و "saga" و "lbfgs" لتكون أسرع للبيانات الكثيفة عالية الأبعاد،
بسبب البدء الدافئ (انظر :term:`المُصطلحات <warm_start>`).

.. _Generalized_linear_regression:

.. _Generalized_linear_models:

النماذج الخطية المُعممة
=========================

تُوسِّع النماذج الخطية المُعممة (GLM) النماذج الخطية بطريقتين
[10]_. أولاً، ترتبط القيم المُتوقعة :math:`\hat{y}` بتركيبة
خطية من متغيرات الإدخال :math:`X` عبر دالة ربط عكسية
:math:`h` كـ

.. math::    \hat{y}(w, X) = h(Xw).

ثانيًا، يتم استبدال دالة الخسارة التربيعية بالانحراف
الوحيد :math:`d` لتوزيع في الأسرة الأسية (أو بشكل أكثر دقة،
نموذج تشتت أسي تناسلي (EDM) [11]_).

تصبح مشكلة التصغير:

.. math::    \min_{w} \frac{1}{2 n_{\text{samples}}} \sum_i d(y_i, \hat{y}_i) + \frac{\alpha}{2} ||w||_2^2,

حيث :math:`\alpha` هي عقوبة تنظيم L2. عندما يتم توفير أوزان
العينة، يصبح المتوسط متوسطًا موزونًا.

يسرد الجدول التالي بعض EDMs المُحددة وانحرافها الوحيد:

================= ================================  ============================================
التوزيع       مجال الهدف                    الانحراف الوحيد :math:`d(y, \hat{y})`
================= ================================  ============================================
عادي            :math:`y \in (-\infty, \infty)`   :math:`(y-\hat{y})^2`
برنولي         :math:`y \in \{0, 1\}`            :math:`2({y}\log\frac{y}{\hat{y}}+({1}-{y})\log\frac{{1}-{y}}{{1}-\hat{y}})`
فئوي       :math:`y \in \{0, 1, ..., k\}`    :math:`2\sum_{i \in \{0, 1, ..., k\}} I(y = i) y_\text{i}\log\frac{I(y = i)}{\hat{I(y = i)}}`
بويسون           :math:`y \in [0, \infty)`         :math:`2(y\log\frac{y}{\hat{y}}-y+\hat{y})`
جاما             :math:`y \in (0, \infty)`         :math:`2(\log\frac{\hat{y}}{y}+\frac{y}{\hat{y}}-1)`
غاوسي عكسي  :math:`y \in (0, \infty)`         :math:`\frac{(y-\hat{y})^2}{y\hat{y}^2}`
================= ================================  ============================================

يتم توضيح دوال كثافة الاحتمال (PDF) لهذه التوزيعات
في الشكل التالي،

.. figure:: ./glm_data/poisson_gamma_tweedie_distributions.png
   :align: center
   :scale: 100%

   PDF لمتغير عشوائي Y يتبع توزيعات بواسون وتويدي (القوة = 1.5) وجاما
   بقيم متوسطة مختلفة (:math:`\mu`). لاحظ كتلة النقطة
   عند :math:`Y=0` لتوزيع بواسون وتوزيع تويدي (القوة = 1.5)،
   ولكن ليس لتوزيع جاما الذي له مجال هدف موجب تمامًا.

توزيع برنولي هو توزيع احتمالي منفصل يُنمذج تجربة
برنولي - حدث له نتيجتان فقط متنافيتان.
التوزيع الفئوي هو تعميم لتوزيع برنولي
لمتغير عشوائي فئوي. بينما يحتوي متغير عشوائي في توزيع برنولي
على نتيجتين مُمكنتين، يمكن لمتغير عشوائي فئوي أن يأخذ
إحدى الفئات K المُمكنة، مع تحديد احتمال كل فئة
بشكل مُنفصل.

يعتمد اختيار التوزيع على المشكلة المطروحة:

* إذا كانت القيم المستهدفة :math:`y` عبارة عن أعداد (ذات قيم صحيحة غير
  سالبة) أو ترددات نسبية (غير سالبة)، فقد تستخدم توزيع بواسون
  بربط سجل.
* إذا كانت القيم المستهدفة ذات قيم موجبة ومنحرفة، فقد تُجرب توزيع جاما
  بربط سجل.
* إذا بدت القيم المستهدفة أثقل من توزيع جاما، فقد
  تُجرب توزيع غاوسي عكسي (أو حتى قوى تباين أعلى
  لعائلة تويدي).
* إذا كانت القيم المستهدفة :math:`y` احتمالات، يمكنك استخدام توزيع
  برنولي. يمكن استخدام توزيع برنولي مع ربط logit للتصنيف
  الثنائي. يمكن استخدام التوزيع الفئوي مع ربط softmax لـ
  التصنيف متعدد الفئات.


.. dropdown:: أمثلة على حالات الاستخدام

  * نمذجة الزراعة / الطقس: عدد أحداث المطر سنويًا (بويسون)،
    كمية هطول الأمطار لكل حدث (جاما)، إجمالي هطول الأمطار سنويًا (تويدي /
    مُركب بواسون جاما).
  * نمذجة المخاطر / تسعير بوليصة التأمين: عدد أحداث المطالبة /
    حامل البوليصة سنويًا (بويسون)، التكلفة لكل حدث (جاما)، التكلفة الإجمالية لكل
    حامل بوليصة سنويًا (تويدي / مُركب بواسون جاما).
  * التخلف عن سداد الائتمان: احتمال عدم سداد قرض (برنولي).
  * الكشف عن الاحتيال: احتمال أن تكون معاملة مالية مثل تحويل نقدي
    معاملة احتيالية (برنولي).
  * الصيانة التنبؤية: عدد أحداث انقطاع الإنتاج سنويًا
    (بويسون)، مدة الانقطاع (جاما)، إجمالي وقت الانقطاع سنويًا
    (تويدي / مُركب بواسون جاما).
  * اختبار الأدوية الطبية: احتمال علاج مريض في مجموعة من التجارب أو
    احتمال تعرض المريض لآثار جانبية (برنولي).
  * تصنيف الأخبار: تصنيف المقالات الإخبارية إلى ثلاث فئات
    وهي أخبار الأعمال والسياسة وأخبار الترفيه (فئوي).

.. rubric:: المراجع

.. [10] McCullagh, Peter; Nelder, John (1989). النماذج الخطية
    المُعممة، الطبعة الثانية. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.

.. [11] Jørgensen, B. (1992). نظرية نماذج التشتت الأسي
    وتحليل الانحراف. Monografias de matemática, no. 51.  انظر أيضًا
    `نموذج التشتت الأسي.
    <https://en.wikipedia.org/wiki/Exponential_dispersion_model>`_

الاستخدام
-----

:class:`TweedieRegressor` يُطبق نموذجًا خطيًا مُعممًا لـ
توزيع تويدي، والذي يسمح بنمذجة أي من التوزيعات المذكورة أعلاه
باستخدام معلمة ``power`` المناسبة. على وجه الخصوص:

- ``power = 0``: التوزيع العادي. مُقدِّرات مُحددة مثل
  :class:`Ridge`، :class:`ElasticNet` تكون أكثر ملاءمة بشكل عام في
  هذه الحالة.
- ``power = 1``: توزيع بواسون. :class:`PoissonRegressor` مُعرَّض
  للراحة. ومع ذلك، فهو يُعادل تمامًا
  `TweedieRegressor(power=1, link='log')`.
- ``power = 2``: توزيع جاما. :class:`GammaRegressor` مُعرَّض لـ
  الراحة. ومع ذلك، فهو يُعادل تمامًا
  `TweedieRegressor(power=2, link='log')`.
- ``power = 3``: التوزيع الغاوسي العكسي.

يتم تحديد دالة الربط بواسطة معلمة `link`.

مثال على الاستخدام::

    >>> from sklearn.linear_model import TweedieRegressor
    >>> reg = TweedieRegressor(power=1, alpha=0.5, link='log')
    >>> reg.fit([[0, 0], [0, 1], [2, 2]], [0, 1, 2])
    TweedieRegressor(alpha=0.5, link='log', power=1)
    >>> reg.coef_
    array([0.2463..., 0.4337...])
    >>> reg.intercept_
    -0.7638...


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_poisson_regression_non_normal_loss.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_tweedie_regression_insurance_claims.py`

.. dropdown:: اعتبارات عملية

  يجب توحيد مصفوفة الميزات `X` قبل الملاءمة. هذا يضمن
  أن العقوبة تُعامل الميزات على قدم المساواة.

  نظرًا لأن المُتنبئ الخطي :math:`Xw` يمكن أن يكون سالبًا وبويسون و
  جاما والتوزيعات الغاوسية العكسية لا تدعم القيم السالبة، فمن
  الضروري تطبيق دالة ربط عكسية تضمن
  عدم السلبية. على سبيل المثال، مع `link='log'`، تصبح دالة الربط العكسي
  :math:`h(Xw)=\exp(Xw)`.

  إذا كنت تُريد نمذجة تردد نسبي، أي عدد لكل تعرض (وقت،
  حجم، ...) يمكنك القيام بذلك عن طريق استخدام توزيع بواسون وتمرير
  :math:`y=\frac{\mathrm{counts}}{\mathrm{exposure}}` كقيم مستهدفة
  مع :math:`\mathrm{exposure}` كأوزان عينة. للحصول على مثال
  ملموس، انظر على سبيل المثال
  :ref:`sphx_glr_auto_examples_linear_model_plot_tweedie_regression_insurance_claims.py`.

  عند إجراء التحقق المتبادل لمعلمة `power` لـ
  `TweedieRegressor`، يُنصح بتحديد دالة `scoring` صريحة،
  لأن المُسجل الافتراضي :meth:`TweedieRegressor.score` هو دالة
  لـ `power` نفسها.

نزول التدرج العشوائي - SGD
=================================

نزول التدرج العشوائي هو نهج بسيط ولكنه فعال للغاية
لملاءمة النماذج الخطية. إنه مفيد بشكل خاص عندما يكون عدد العينات
(وعدد الميزات) كبيرًا جدًا.
يسمح أسلوب ``partial_fit`` بالتعلم على الإنترنت / خارج النواة.

تُوفر الفئات :class:`SGDClassifier` و :class:`SGDRegressor`
وظائف لملاءمة النماذج الخطية للتصنيف والانحدار
باستخدام دوال خسارة (مُحدبة) مُختلفة وعقوبات مُختلفة.
على سبيل المثال، مع ``loss="log"``، :class:`SGDClassifier`
يُناسب نموذج انحدار لوجستي،
بينما مع ``loss="hinge"`` يُناسب آلة متجه دعم خطية (SVM).

يمكنك الرجوع إلى قسم الوثائق :ref:`sgd` المُخصص لمزيد من التفاصيل.

.. _perceptron:

Perceptron
==========

:class:`Perceptron` هو خوارزمية تصنيف بسيطة أخرى مُناسبة لـ
التعلم على نطاق واسع. افتراضيًا:

- لا يتطلب مُعدل تعلم.

- إنه غير مُنظّم (مُعاقب).

- يُحدِّث نموذجه فقط على الأخطاء.

الخاصية الأخيرة تعني أن Perceptron أسرع قليلاً في
التدريب من SGD مع خسارة المفصلة وأن النماذج الناتجة
متفرقة.

في الواقع، :class:`Perceptron` هو غلاف حول فئة :class:`SGDClassifier`
باستخدام خسارة perceptron ومُعدل تعلم ثابت. راجع
:ref:`القسم الرياضي <sgd_mathematical_formulation>` لإجراء SGDلمزيد من التفاصيل.

.. _passive_aggressive:

خوارزميات عدوانية سلبية
=============================

الخوارزميات العدوانية السلبية هي عائلة من الخوارزميات للتعلم
واسع النطاق. إنها تُشبه Perceptron من حيث أنها لا تتطلب
مُعدل تعلم. ومع ذلك، على عكس Perceptron، فإنها تتضمن معلمة
تنظيم ``C``.

للتصنيف، يمكن استخدام :class:`PassiveAggressiveClassifier` مع
``loss='hinge'`` (PA-I) أو ``loss='squared_hinge'`` (PA-II). للانحدار،
يمكن استخدام :class:`PassiveAggressiveRegressor` مع
``loss='epsilon_insensitive'`` (PA-I) أو
``loss='squared_epsilon_insensitive'`` (PA-II).

.. dropdown:: المراجع

  * `"خوارزميات عدوانية سلبية على الإنترنت"
    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>`_
    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)

انحدار المتانة: القيم المتطرفة وأخطاء النمذجة
=====================================================

يهدف الانحدار القوي إلى ملاءمة نموذج انحدار في
وجود بيانات تالفة: إما قيم متطرفة، أو خطأ في النموذج.

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png
   :target: ../auto_examples/linear_model/plot_theilsen.html
   :scale: 50%
   :align: center

سيناريوهات مختلفة ومفاهيم مفيدة
----------------------------------------

هناك أشياء مختلفة يجب وضعها في الاعتبار عند التعامل مع البيانات
التالفة بواسطة القيم المتطرفة:

.. |y_outliers| image:: ../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_003.png
   :target: ../auto_examples/linear_model/plot_robust_fit.html
   :scale: 60%

.. |X_outliers| image:: ../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_002.png
   :target: ../auto_examples/linear_model/plot_robust_fit.html
   :scale: 60%

.. |large_y_outliers| image:: ../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_005.png
   :target: ../auto_examples/linear_model/plot_robust_fit.html
   :scale: 60%

* **القيم المتطرفة في X أو في y**؟

  ==================================== ====================================
  القيم المتطرفة في اتجاه y          القيم المتطرفة في اتجاه X
  ==================================== ====================================
  |y_outliers|                         |X_outliers|
  ==================================== ====================================

* **جزء القيم المتطرفة مقابل سعة الخطأ**

  يهم عدد النقاط المتطرفة، ولكن أيضًا مقدار تطرفها.

  ==================================== ====================================
  القيم المتطرفة الصغيرة                       القيم المتطرفة الكبيرة
  ==================================== ====================================
  |y_outliers|                         |large_y_outliers|
  ==================================== ====================================

مفهوم مهم للملاءمة القوية هو نقطة الانهيار: جزء
البيانات التي يمكن أن تكون متطرفة حتى تبدأ الملاءمة في فقدان
البيانات الداخلية.

لاحظ أنه بشكل عام، الملاءمة القوية في الإعداد عالي الأبعاد (كبير
`n_features`) صعبة للغاية. من المحتمل ألا تعمل النماذج القوية هنا
في هذه الإعدادات.


.. topic:: المفاضلات: أي مُقدِّر؟

  يُوفر Scikit-learn 3 مُقدِّرات انحدار قوية:
  :ref:`RANSAC <ransac_regression>`،
  :ref:`Theil Sen <theil_sen_regression>` و
  :ref:`HuberRegressor <huber_regression>`.

  * يجب أن يكون :ref:`HuberRegressor <huber_regression>` أسرع من
    :ref:`RANSAC <ransac_regression>` و :ref:`Theil Sen <theil_sen_regression>`
    إلا إذا كان عدد العينات كبيرًا جدًا، أي ``n_samples`` >> ``n_features``.
    هذا لأن :ref:`RANSAC <ransac_regression>` و :ref:`Theil Sen <theil_sen_regression>`
    يُناسبان مجموعات فرعية أصغر من البيانات. ومع ذلك، من غير المُرجح أن يكون
    كل من :ref:`Theil Sen <theil_sen_regression>`
    و :ref:`RANSAC <ransac_regression>` بنفس قوة
    :ref:`HuberRegressor <huber_regression>` للمعلمات الافتراضية.

  * :ref:`RANSAC <ransac_regression>` أسرع من :ref:`Theil Sen <theil_sen_regression>`
    ويتناسب بشكل أفضل مع عدد العينات.

  * :ref:`RANSAC <ransac_regression>` سيتعامل بشكل أفضل مع
    القيم المتطرفة الكبيرة في اتجاه y (الحالة الأكثر شيوعًا).

  * :ref:`Theil Sen <theil_sen_regression>` سيتعامل بشكل أفضل مع
    القيم المتطرفة متوسطة الحجم في اتجاه X، لكن هذه الخاصية
    ستختفي في الإعدادات عالية الأبعاد.

  عند الشك، استخدم :ref:`RANSAC <ransac_regression>`.

.. _ransac_regression:

RANSAC: توافق العينة العشوائي
--------------------------------

يُناسب RANSAC (RANdom SAmple Consensus) نموذجًا من مجموعات فرعية عشوائية من
القيم الداخلية من مجموعة البيانات الكاملة.

RANSAC هي خوارزمية غير حتمية تُنتج نتيجة معقولة فقط مع
احتمالية مُعينة، والتي تعتمد على عدد التكرارات (انظر
معلمة `max_trials`). يتم استخدامه عادةً لمشاكل الانحدار الخطي وغير الخطي
وهو شائع بشكل خاص في مجال رؤية الكمبيوتر التصويرية.

تقسم الخوارزمية بيانات عينة الإدخال الكاملة إلى مجموعة من القيم
الداخلية، والتي قد تخضع للضوضاء، والقيم المتطرفة، والتي تنتج
على سبيل المثال عن قياسات خاطئة أو فرضيات غير صالحة حول البيانات. ثم
يتم تقدير النموذج الناتج فقط من القيم الداخلية المُحددة.

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_ransac_001.png
   :target: ../auto_examples/linear_model/plot_ransac.html
   :align: center
   :scale: 50%

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_ransac.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_robust_fit.py`

.. dropdown:: تفاصيل الخوارزمية

  يُجري كل تكرار الخطوات التالية:

  1. حدد ``min_samples`` عينات عشوائية من البيانات الأصلية وتحقق
     مما إذا كانت مجموعة البيانات صالحة (انظر ``is_data_valid``).
  2. قم بملاءمة نموذج على المجموعة الفرعية العشوائية (``estimator.fit``) وتحقق
     مما إذا كان النموذج المُقدَّر صالحًا (انظر ``is_model_valid``).
  3. صنف جميع البيانات على أنها قيم داخلية أو قيم متطرفة عن طريق حساب البواقي
     إلى النموذج المُقدَّر (``estimator.predict(X) - y``) - جميع عينات
     البيانات ذات البواقي المُطلقة الأصغر من أو تساوي
     ``residual_threshold`` تُعتبر قيمًا داخلية.
  4. احفظ النموذج المُناسب كأفضل نموذج إذا كان عدد عينات القيم الداخلية
     أقصى. في حالة احتواء النموذج المُقدَّر الحالي على نفس عدد
     القيم الداخلية، فإنه يُعتبر فقط كأفضل نموذج إذا كان لديه درجة أفضل.

  يتم تنفيذ هذه الخطوات إما لعدد أقصى من المرات (``max_trials``) أو
  حتى يتم استيفاء أحد معايير الإيقاف الخاصة (انظر ``stop_n_inliers`` و
  ``stop_score``). يتم تقدير النموذج النهائي باستخدام جميع عينات القيم الداخلية
  (مجموعة التوافق) لأفضل نموذج مُحدد مسبقًا.

  تسمح الدالتان ``is_data_valid`` و ``is_model_valid`` بتحديد ورفض
  مجموعات عينات فرعية عشوائية مُتدهورة. إذا لم يكن النموذج المُقدَّر
  مطلوبًا لتحديد الحالات المتدهورة، فيجب استخدام ``is_data_valid``
  حيث يتم استدعاؤه قبل ملاءمة النموذج، مما يؤدي إلى أداء
  حسابي أفضل.

.. dropdown:: المراجع

  * https://en.wikipedia.org/wiki/RANSAC
  * `"توافق العينة العشوائي: نموذج للملاءمة مع التطبيقات لـ
    تحليل الصور ورسم الخرائط التلقائي"
    <https://www.cs.ait.ac.th/~mdailey/cvreadings/Fischler-RANSAC.pdf>`_
    Martin A. Fischler and Robert C. Bolles - SRI International (1981)
  * `"تقييم أداء عائلة RANSAC"
    <http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf>`_
    Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)

.. _theil_sen_regression:

مُقدِّر Theil-Sen: مُقدِّر قائم على الوسيط المُعمم
--------------------------------------------------------

يستخدم مُقدِّر :class:`TheilSenRegressor` تعميمًا للوسيط في
أبعاد متعددة. وبالتالي فهو قوي ضد القيم المتطرفة متعددة المتغيرات. لاحظ
ومع ذلك أن قوة المُقدِّر تتناقص بسرعة مع أبعاد
المشكلة. يفقد خصائص المتانة الخاصة به ويصبح ليس
أفضل من المربعات الصغرى العادية في البُعد العالي.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_theilsen.py`
* :ref:`sphx_glr_auto_examples_linear_model_plot_robust_fit.py`


.. dropdown:: اعتبارات نظرية

  :class:`TheilSenRegressor` مُقارن لـ :ref:`المربعات الصغرى
  العادية (OLS) <ordinary_least_squares>` من حيث الكفاءة المقاربة وكمُقدِّر
  غير مُتحيز. على عكس OLS، فإن Theil-Sen هو أسلوب غير معلمي
  مما يعني أنه لا يضع أي افتراض حول التوزيع
  الأساسي للبيانات. نظرًا لأن Theil-Sen هو مُقدِّر قائم على الوسيط، فإنه
  أكثر قوة ضد البيانات التالفة المعروفة أيضًا باسم القيم المتطرفة. في
  إعداد أحادي المتغير، Theil-Sen لديه نقطة انهيار تبلغ حوالي 29.3% في حالة
  الانحدار الخطي البسيط مما يعني أنه يمكنه تحمل
  بيانات تالفة عشوائية تصل إلى 29.3%.

  .. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png
    :target: ../auto_examples/linear_model/plot_theilsen.html
    :align: center
    :scale: 50%

  يتبع تطبيق :class:`TheilSenRegressor` في scikit-learn تعميمًا لنموذج
  الانحدار الخطي متعدد المتغيرات [#f1]_ باستخدام الوسيط المكاني
  وهو تعميم للوسيط لأبعاد متعددة [#f2]_.

  من حيث تعقيد الوقت والمساحة، يتناسب Theil-Sen وفقًا لـ

  .. math::
      \binom{n_{\text{samples}}}{n_{\text{subsamples}}}

  مما يجعله غير مُجدي للتطبيق بشكل شامل على المشاكل التي تحتوي على
  عدد كبير من العينات والميزات. لذلك، يمكن اختيار حجم
  مجموعة فرعية لتقييد تعقيد الوقت والمساحة عن طريق
  النظر فقط في مجموعة فرعية عشوائية من جميع التوليفات الممكنة.

  .. rubric:: المراجع

  .. [#f1] Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: `مقدرات Theil-Sen في نموذج انحدار خطي متعدد. <http://home.olemiss.edu/~xdang/papers/MTSE.pdf>`_

  .. [#f2] T. Kärkkäinen and S. Äyrämö: `حول حساب الوسيط المكاني لتنقيب البيانات القوي. <http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf>`_

  انظر أيضًا `صفحة ويكيبيديا <https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator>`_


.. _huber_regression:

انحدار Huber
----------------

يختلف :class:`HuberRegressor` عن :class:`Ridge` لأنه يُطبق
خسارة خطية على العينات التي تُصنف على أنها قيم متطرفة.
يتم تصنيف عينة على أنها قيمة داخلية إذا كان الخطأ المُطلق لتلك العينة
أقل من عتبة مُعينة. إنه يختلف عن :class:`TheilSenRegressor`
و :class:`RANSACRegressor` لأنه لا يتجاهل تأثير القيم المتطرفة
ولكنه يُعطي وزنًا أقل لها.

.. figure:: /auto_examples/linear_model/images/sphx_glr_plot_huber_vs_ridge_001.png
   :target: ../auto_examples/linear_model/plot_huber_vs_ridge.html
   :align: center
   :scale: 50%

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_huber_vs_ridge.py`

.. dropdown:: التفاصيل الرياضية

  دالة الخسارة التي يُقللها :class:`HuberRegressor` مُعطاة بواسطة

  .. math::

    \min_{w, \sigma} {\sum_{i=1}^n\left(\sigma + H_{\epsilon}\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}^2}

  حيث

  .. math::

    H_{\epsilon}(z) = \begin{cases}
          z^2, & \text {if } |z| < \epsilon, \\
          2\epsilon|z| - \epsilon^2, & \text{otherwise}
    \end{cases}

  يُنصح بتعيين المعلمة ``epsilon`` إلى 1.35 لتحقيق كفاءة
  إحصائية بنسبة 95%.

  .. rubric:: المراجع

  * Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale
    estimates, pg 172

يختلف :class:`HuberRegressor` عن استخدام :class:`SGDRegressor` مع تعيين الخسارة
إلى `huber` بالطرق التالية.

- :class:`HuberRegressor` ثابت المقياس. بمجرد تعيين ``epsilon``، فإن تغيير
  مقياس ``X`` و ``y`` لأعلى أو لأسفل بقيم مُختلفة سيُنتج نفس المتانة
  للقيم المتطرفة كما كان من قبل.
  مُقارنةً بـ :class:`SGDRegressor` حيث يجب تعيين ``epsilon`` مرة أخرى عند
  تغيير مقياس ``X`` و ``y``.

- يجب أن يكون :class:`HuberRegressor` أكثر كفاءة في الاستخدام على البيانات ذات
  العدد القليل من العينات بينما يحتاج :class:`SGDRegressor` إلى عدد من
  التمريرات على بيانات التدريب لـ
  إنتاج نفس المتانة.

لاحظ أن هذا المُقدِّر يختلف عن تطبيق R للانحدار القوي
(https://stats.oarc.ucla.edu/r/dae/robust-regression/) لأن تطبيق R يُجري
تطبيقًا للمربعات الصغرى الموزونة بأوزان مُعطاة لكل عينة على أساس
مقدار الباقي أكبر من عتبة مُعينة.

.. _quantile_regression:

انحدار المُكمِّم
===================

يُقدِّر انحدار المُكمِّم الوسيط أو مُكمِّمات أخرى لـ :math:`y`
الشرطية على :math:`X`، بينما تُقدِّر المربعات الصغرى العادية (OLS)
المتوسط الشرطي.

قد يكون انحدار المُكمِّم مفيدًا إذا كان المرء مهتمًا بالتنبؤ بـ
فاصل زمني بدلاً من التنبؤ بالنقطة. في بعض الأحيان، يتم
حساب فترات التنبؤ بناءً على افتراض أن خطأ التنبؤ موزع
بشكل طبيعي بمتوسط صفري وتباين ثابت. يُوفر انحدار المُكمِّم
فترات تنبؤ معقولة حتى للأخطاء ذات التباين غير الثابت (ولكن
التنبؤي) أو التوزيع غير العادي.

.. figure:: /auto_examples/linear_model/images/sphx_glr_plot_quantile_regression_002.png
   :target: ../auto_examples/linear_model/plot_quantile_regression.html
   :align: center
   :scale: 50%

بناءً على تقليل خسارة الكرة والدبوس، يمكن أيضًا
تقدير المُكمِّمات الشرطية بواسطة نماذج أخرى غير النماذج الخطية. على سبيل المثال،
:class:`~sklearn.ensemble.GradientBoostingRegressor` يمكنه التنبؤ بـ
مُكمِّمات شرطية إذا تم تعيين معلمته ``loss`` إلى ``"quantile"`` وتم تعيين
المعلمة ``alpha`` إلى المُكمِّم الذي يجب التنبؤ به. انظر المثال في
:ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`.

تعتمد معظم تطبيقات انحدار المُكمِّم على مشكلة البرمجة
الخطية. يعتمد التطبيق الحالي على
:func:`scipy.optimize.linprog`.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_quantile_regression.py`

.. dropdown:: التفاصيل الرياضية

  كنموذج خطي، يُعطي :class:`QuantileRegressor` تنبؤات خطية
  :math:`\hat{y}(w, X) = Xw` لـ :math:`q`-th مُكمِّم، :math:`q \in (0, 1)`.
  ثم يتم إيجاد الأوزان أو المعاملات :math:`w` بواسطة مشكلة
  التصغير التالية:

  .. math::
      \min_{w} {\frac{1}{n_{\text{samples}}}
      \sum_i PB_q(y_i - X_i w) + \alpha ||w||_1}.

  يتكون هذا من خسارة الكرة والدبوس (المعروفة أيضًا باسم الخسارة الخطية)،
  انظر أيضًا :class:`~sklearn.metrics.mean_pinball_loss`،

  .. math::
      PB_q(t) = q \max(t, 0) + (1 - q) \max(-t, 0) =
      \begin{cases}
          q t, & t > 0, \\
          0,    & t = 0, \\
          (q-1) t, & t < 0
      \end{cases}

  وعقوبة L1 التي تتحكم فيها المعلمة ``alpha``، على غرار
  :class:`Lasso`.

  نظرًا لأن خسارة الكرة والدبوس خطية فقط في البواقي، فإن انحدار
  المُكمِّم أكثر قوة ضد القيم المتطرفة من التقدير القائم على
  الخطأ التربيعي للمتوسط.
  في مكان ما بينهما هو :class:`HuberRegressor`.

.. dropdown:: المراجع

  * Koenker, R., & Bassett Jr, G. (1978). `المُكمِّمات الانحدارية.
    <https://gib.people.uic.edu/RQ.pdf>`_
    Econometrica: journal of the Econometric Society, 33-50.

  * Portnoy, S., & Koenker, R. (1997). :doi:`الأرنب الغاوسي والسلحفاة
    لابلاس: قابلية حساب المُقدِّرات القائمة على الخطأ التربيعي مقابل
    المُقدِّرات القائمة على الخطأ المُطلق. Statistical Science، 12، 279-300 <10.1214/ss/1030037960>`.

  * Koenker, R. (2005). :doi:`انحدار المُكمِّم <10.1017/CBO9780511754098>`.
    Cambridge University Press.


.. _polynomial_regression:

الانحدار متعدد الحدود: توسيع النماذج الخطية مع دوال الأساس
===================================================================

.. currentmodule:: sklearn.preprocessing

أحد الأنماط الشائعة في التعلم الآلي هو استخدام النماذج الخطية المُدرَّبة
على دوال غير خطية للبيانات. يحافظ هذا النهج على الأداء
السريع بشكل عام للطرق الخطية، مع السماح لها بملاءمة
نطاق أوسع بكثير من البيانات.

.. dropdown:: التفاصيل الرياضية

  على سبيل المثال، يمكن تمديد انحدار خطي بسيط عن طريق بناء
  **ميزات متعددة الحدود** من المعاملات. في حالة الانحدار
  الخطي القياسية، قد يكون لديك نموذج يبدو كالتالي لـ
  بيانات ثنائية الأبعاد:

  .. math::    \hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2

  إذا أردنا ملاءمة قطع مكافئ للبيانات بدلاً من مستوى، فيمكننا دمج
  الميزات في كثيرات حدود من الدرجة الثانية، بحيث يبدو النموذج على النحو التالي:

  .. math::    \hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2

  الملاحظة (المُفاجئة أحيانًا) هي أن هذا *لا يزال نموذجًا خطيًا*:
  لرؤية ذلك، تخيل إنشاء مجموعة جديدة من الميزات

  .. math::  z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]

  مع إعادة تسمية البيانات هذه، يمكن كتابة مشكلتنا

  .. math::    \hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5

  نرى أن *الانحدار متعدد الحدود* الناتج في نفس فئة
  النماذج الخطية التي نظرنا فيها أعلاه (أي أن النموذج خطي في :math:`w`)
  ويمكن حله بنفس التقنيات. بالنظر إلى الملاءمات الخطية داخل
  فضاء ذي أبعاد أعلى مبني باستخدام دوال الأساس هذه، يتمتع النموذج
  بالمرونة لملاءمة نطاق أوسع بكثير من البيانات.

فيما يلي مثال على تطبيق هذه الفكرة على بيانات أحادية البعد، باستخدام
ميزات متعددة الحدود بدرجات مُتفاوتة:

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_polynomial_interpolation_001.png
   :target: ../auto_examples/linear_model/plot_polynomial_interpolation.html
   :align: center
   :scale: 50%

يتم إنشاء هذا الشكل باستخدام مُحوِّل :class:`PolynomialFeatures`، الذي
يُحوِّل مصفوفة بيانات الإدخال إلى مصفوفة بيانات جديدة من درجة مُعطاة.
يمكن استخدامه على النحو التالي::

    >>> from sklearn.preprocessing import PolynomialFeatures
    >>> import numpy as np
    >>> X = np.arange(6).reshape(3, 2)
    >>> X
    array([[0, 1],
           [2, 3],
           [4, 5]])
    >>> poly = PolynomialFeatures(degree=2)
    >>> poly.fit_transform(X)
    array([[ 1.,  0.,  1.,  0.,  0.,  1.],
           [ 1.,  2.,  3.,  4.,  6.,  9.],
           [ 1.,  4.,  5., 16., 20., 25.]])

تم تحويل ميزات ``X`` من :math:`[x_1, x_2]` إلى
:math:`[1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]`، ويمكن الآن استخدامها داخل
أي نموذج خطي.

يمكن تبسيط هذا النوع من المُعالجة المُسبقة باستخدام
أدوات :ref:`Pipeline <pipeline>`. يمكن إنشاء كائن واحد يُمثِّل
انحدارًا متعدد الحدود بسيطًا واستخدامه على النحو التالي::

    >>> from sklearn.preprocessing import PolynomialFeatures
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.pipeline import Pipeline
    >>> import numpy as np
    >>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),
    ...                   ('linear', LinearRegression(fit_intercept=False))])
    >>> # ملاءمة بيانات متعددة الحدود من الرتبة 3
    >>> x = np.arange(5)
    >>> y = 3 - 2 * x + x ** 2 - x ** 3
    >>> model = model.fit(x[:, np.newaxis], y)
    >>> model.named_steps['linear'].coef_
    array([ 3., -2.,  1., -1.])

النموذج الخطي المُدرَّب على الميزات متعددة الحدود قادر على استرداد
معاملات مُتعددة الحدود المدخلة بدقة.

في بعض الحالات، ليس من الضروري تضمين قوى أعلى لأي ميزة
فردية، ولكن فقط ما يُسمى *ميزات التفاعل*
التي تضرب معًا :math:`d` ميزات مُتميزة على الأكثر.
يمكن الحصول على هذه من :class:`PolynomialFeatures` مع الإعداد
``interaction_only=True``.

على سبيل المثال، عند التعامل مع ميزات منطقية،
:math:`x_i^n = x_i` لجميع :math:`n` وبالتالي فهي غير مُجدية؛
لكن :math:`x_i x_j` يُمثل اقتران اثنين من القيم المنطقية.
بهذه الطريقة، يمكننا حل مشكلة XOR باستخدام مُصنف خطي::

    >>> from sklearn.linear_model import Perceptron
    >>> from sklearn.preprocessing import PolynomialFeatures
    >>> import numpy as np
    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    >>> y = X[:, 0] ^ X[:, 1]
    >>> y
    array([0, 1, 1, 0])
    >>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)
    >>> X
    array([[1, 0, 0, 0],
           [1, 0, 1, 0],
           [1, 1, 0, 0],
           [1, 1, 1, 1]])
    >>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,
    ...                  shuffle=False).fit(X, y)

وتنبؤات المُصنف "مثالية"::

    >>> clf.predict(X)
    array([0, 1, 1, 0])
    >>> clf.score(X, y)
    1.0



