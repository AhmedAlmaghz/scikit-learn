.. _svm:

=======================
آلات الدعم المتجهية (SVM)
=======================

.. TODO: وصف معلمة tol
.. TODO: وصف معلمة max_iter

.. currentmodule:: sklearn.svm

**آلات الدعم المتجهية (SVM) (SVMs)** هي مجموعة من أساليب التعلم الخاضعة للإشراف تُستخدم لـ :ref:`التصنيف <svm_classification>` و :ref:`الانحدار <svm_regression>` و :ref:`كشف القيم المتطرفة <svm_outlier_detection>`.

مزايا آلات الدعم المتجهية (SVM) هي:

- فعالة في المساحات عالية الأبعاد.

- لا تزال فعالة في الحالات التي يكون فيها عدد الأبعاد أكبر من عدد العينات.

- تستخدم مجموعة فرعية من نقاط التدريب في دالة القرار (تُسمى متجهات الدعم)، لذا فهي فعالة أيضًا في الذاكرة.

- مُتعدّدة الاستخدامات: يمكن تحديد :ref:`svm_kernels` مختلفة لدالة القرار. يتم توفير النوى الشائعة، ولكن من الممكن أيضًا تحديد نوى مخصصة.


تشمل عيوب آلات الدعم المتجهية (SVM) ما يلي:

- إذا كان عدد الميزات أكبر بكثير من عدد العينات، فتجنب التوافق الزائد في اختيار :ref:`svm_kernels` ومصطلح التنظيم أمر بالغ الأهمية.

- لا تُوفر SVMs تقديرات احتمالية مُباشرةً، ويتم حسابها باستخدام تحقق متبادل خماسي مُكلف (انظر :ref:`الدرجات والاحتمالات <scores_probabilities>`، أدناه).

تدعم آلات الدعم المتجهية (SVM) في scikit-learn كلاً من متجهات العينة الكثيفة (``numpy.ndarray`` والقابلة للتحويل إلى ذلك بواسطة ``numpy.asarray``) والمتفرقة (أي ``scipy.sparse``) كمدخلات. ومع ذلك، لاستخدام SVM لإجراء تنبؤات للبيانات المتفرقة، يجب أن تكون مُناسبة لمثل هذه البيانات. للحصول على أداء مثالي، استخدم ``numpy.ndarray`` (كثيفة) مُرتبة حسب C أو ``scipy.sparse.csr_matrix`` (متفرقة) مع ``dtype=float64``.


.. _svm_classification:

التصنيف
==============

:class:`SVC` و :class:`NuSVC` و :class:`LinearSVC` هي فئات قادرة على إجراء تصنيف ثنائي ومتعدد الفئات على مجموعة بيانات.


.. figure:: ../auto_examples/svm/images/sphx_glr_plot_iris_svc_001.png
   :target: ../auto_examples/svm/plot_iris_svc.html
   :align: center


:class:`SVC` و :class:`NuSVC` طريقتان مُتشابهتان، لكنهما تقبلان مجموعات مختلفة قليلاً من المعلمات ولها صيغ رياضية مختلفة (انظر القسم :ref:`svm_mathematical_formulation`). من ناحية أخرى، :class:`LinearSVC` هو تطبيق آخر (أسرع) لتصنيف متجه الدعم في حالة النواة الخطية. كما أنه يفتقر إلى بعض سمات :class:`SVC` و :class:`NuSVC`، مثل `support_`. يستخدم :class:`LinearSVC` خسارة `squared_hinge`، وبسبب تطبيقه في `liblinear`، فإنه يُنظّم أيضًا التقاطع، إذا تم اعتباره. ومع ذلك، يمكن تقليل هذا التأثير عن طريق الضبط الدقيق لمعلمة `intercept_scaling`، والتي تسمح لمصطلح التقاطع بأن يكون له سلوك تنظيم مختلف مقارنة بالميزات الأخرى. لذلك، يمكن أن تختلف نتائج التصنيف والدرجة عن المُصنفين الآخرين.

كمُصنفات أخرى، يأخذ :class:`SVC` و :class:`NuSVC` و :class:`LinearSVC` مصفوفتين كمدخلات: مصفوفة `X` ذات شكل `(n_samples, n_features)` تحتوي على عينات التدريب، ومصفوفة `y` من تسميات الفئات (سلاسل أو أعداد صحيحة)، ذات شكل `(n_samples)`::


    >>> from sklearn import svm
    >>> X = [[0, 0], [1, 1]]
    >>> y = [0, 1]
    >>> clf = svm.SVC()
    >>> clf.fit(X, y)
    SVC()

بعد الملاءمة، يمكن استخدام النموذج للتنبؤ بقيم جديدة::


    >>> clf.predict([[2., 2.]])
    array([1])


تعتمد دالة قرار SVMs (مُفصلة في :ref:`svm_mathematical_formulation`) على مجموعة فرعية من بيانات التدريب، تُسمى متجهات الدعم. يمكن العثور على بعض خصائص متجهات الدعم هذه في السمات ``support_vectors_`` و ``support_`` و ``n_support_``::


    >>> # الحصول على متجهات الدعم
    >>> clf.support_vectors_
    array([[0., 0.],
           [1., 1.]])
    >>> # الحصول على مؤشرات متجهات الدعم
    >>> clf.support_
    array([0, 1]...)
    >>> # الحصول على عدد متجهات الدعم لكل فئة
    >>> clf.n_support_
    array([1, 1]...)



.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`
* :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`
* :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`



.. _svm_multi_class:


تصنيف متعدد الفئات
--------------------------


يُطبّق :class:`SVC` و :class:`NuSVC` نهج "واحد مقابل واحد" لتصنيف متعدد الفئات. في المجموع، يتم إنشاء ``n_classes * (n_classes - 1) / 2`` مُصنّفًا ويُدرّب كل منها بيانات من فئتين. لتوفير واجهة متسقة مع المُصنفات الأخرى، يسمح خيار ``decision_function_shape`` بالتحويل الرتيب لنتائج مُصنفات "واحد مقابل واحد" إلى دالة قرار "واحد مقابل البقية" ذات شكل ``(n_samples, n_classes)``، وهو الإعداد الافتراضي للمعلمة (default='ovr').


    >>> X = [[0], [1], [2], [3]]
    >>> Y = [0, 1, 2, 3]
    >>> clf = svm.SVC(decision_function_shape='ovo')
    >>> clf.fit(X, Y)
    SVC(decision_function_shape='ovo')
    >>> dec = clf.decision_function([[1]])
    >>> dec.shape[1] # 6 فئات: 4*3/2 = 6
    6
    >>> clf.decision_function_shape = "ovr"
    >>> dec = clf.decision_function([[1]])
    >>> dec.shape[1] # 4 فئات
    4

من ناحية أخرى، يُطبّق :class:`LinearSVC` إستراتيجية "واحد مقابل البقية" متعددة الفئات، وبالتالي يُدرّب `n_classes` نماذج.


    >>> lin_clf = svm.LinearSVC()
    >>> lin_clf.fit(X, Y)
    LinearSVC()
    >>> dec = lin_clf.decision_function([[1]])
    >>> dec.shape[1]
    4


انظر :ref:`svm_mathematical_formulation` للحصول على وصف كامل لدالة القرار.

.. dropdown:: تفاصيل حول استراتيجيات متعددة الفئات

  لاحظ أن :class:`LinearSVC` يُطبّق أيضًا إستراتيجية بديلة متعددة الفئات، ما يُسمى بـ SVM متعدد الفئات الذي وضعه كرامر وسينغر [#8]_، باستخدام الخيار ``multi_class='crammer_singer'``. في الممارسة العملية، يُفضّل عادةً التصنيف واحد مقابل البقية، حيث أن النتائج متشابهة في الغالب، لكن وقت التشغيل أقل بكثير.

  بالنسبة لـ :class:`LinearSVC` "واحد مقابل البقية"، فإن السمتين ``coef_`` و ``intercept_`` لهما شكل ``(n_classes, n_features)`` و ``(n_classes,)`` على التوالي. يتوافق كل صف من المعاملات مع واحد من مُصنفات ``n_classes`` "واحد مقابل البقية" ومثل ذلك بالنسبة للقطوع، بترتيب فئة "الواحد".


  في حالة :class:`SVC` "واحد مقابل واحد" و :class:`NuSVC`، يكون تخطيط السمات أكثر تعقيدًا بعض الشيء. في حالة النواة الخطية، يكون للسمتين ``coef_`` و ``intercept_`` شكل ``(n_classes * (n_classes - 1) / 2, n_features)`` و ``(n_classes * (n_classes - 1) / 2)`` على التوالي. هذا مشابه لتخطيط :class:`LinearSVC` الموضح أعلاه، حيث يتوافق كل صف الآن مع مُصنف ثنائي. الترتيب للفئات من 0 إلى n هو "0 مقابل 1"، "0 مقابل 2"، ... "0 مقابل n"، "1 مقابل 2"، "1 مقابل 3"، "1 مقابل n"،. . . "n-1 مقابل n".

  شكل ``dual_coef_`` هو ``(n_classes-1, n_SV)`` بتخطيط يصعب فهمه إلى حد ما. تتوافق الأعمدة مع متجهات الدعم المُشاركة في أي من مُصنفات ``n_classes * (n_classes - 1) / 2`` "واحد مقابل واحد". يحتوي كل متجه دعم ``v`` على معامل مُزدوج في كل من مُصنفات ``n_classes - 1`` التي تُقارن فئة ``v`` بفئة أخرى. لاحظ أن بعض هذه المعاملات المزدوجة، وليس كلها، قد تكون صفرًا. إدخالات ``n_classes - 1`` في كل عمود هي هذه المعاملات المزدوجة، مُرتبة حسب الفئة المُقابلة.


  قد يكون هذا أكثر وضوحًا بمثال: ضع في اعتبارك مشكلة ثلاث فئات حيث تحتوي الفئة 0 على ثلاثة متجهات دعم :math:`v^{0}_0, v^{1}_0, v^{2}_0` والفئة 1 و 2 تحتوي على متجهي دعم :math:`v^{0}_1, v^{1}_1` و :math:`v^{0}_2, v^{1}_2` على التوالي. لكل متجه دعم :math:`v^{j}_i`، هناك معاملان مُزدوجان. دعنا نُسمي مُعامل متجه الدعم :math:`v^{j}_i` في المُصنف بين الفئات :math:`i` و :math:`k` :math:`\alpha^{j}_{i,k}`. ثم ``dual_coef_`` تبدو كالتالي:


  +------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+
  |:math:`\alpha^{0}_{0,1}`|:math:`\alpha^{1}_{0,1}`|:math:`\alpha^{2}_{0,1}`|:math:`\alpha^{0}_{1,0}`|:math:`\alpha^{1}_{1,0}`|:math:`\alpha^{0}_{2,0}`|:math:`\alpha^{1}_{2,0}`|
  +------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+
  |:math:`\alpha^{0}_{0,2}`|:math:`\alpha^{1}_{0,2}`|:math:`\alpha^{2}_{0,2}`|:math:`\alpha^{0}_{1,2}`|:math:`\alpha^{1}_{1,2}`|:math:`\alpha^{0}_{2,1}`|:math:`\alpha^{1}_{2,1}`|
  +------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+
  |المعاملات                                                              |المعاملات                                     |المعاملات                                     |
  |لمتجهات الدعم للفئة 0                                                      |لمتجهات الدعم للفئة 1                           |لمتجهات الدعم للفئة 2                           |
  +--------------------------------------------------------------------------+-------------------------------------------------+-------------------------------------------------+

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_svm_plot_iris_svc.py`



.. _scores_probabilities:

الدرجات والاحتمالات
------------------------

تُعطي طريقة ``decision_function`` لـ :class:`SVC` و :class:`NuSVC` درجات لكل فئة لكل عينة (أو درجة واحدة لكل عينة في الحالة الثنائية). عندما يتم تعيين خيار المُنشئ ``probability`` إلى ``True``، يتم تمكين تقديرات احتمال عضوية الفئة (من الطريقتين ``predict_proba`` و ``predict_log_proba``). في الحالة الثنائية، تتم معايرة الاحتمالات باستخدام قياس بلات [#1]_: الانحدار اللوجستي على درجات SVM، مُناسب بواسطة تحقق متبادل إضافي على بيانات التدريب. في حالة متعددة الفئات، يتم تمديد هذا وفقًا لـ [#2]_.


.. note::

  إجراء معايرة الاحتمال نفسه مُتاح لجميع المقدرات عبر :class:`~sklearn.calibration.CalibratedClassifierCV` (انظر :ref:`calibration`). في حالة :class:`SVC` و :class:`NuSVC`، يكون هذا الإجراء مُدمجًا في `libsvm`_ الذي يُستخدم تحت الغطاء، لذا فهو لا يعتمد على :class:`~sklearn.calibration.CalibratedClassifierCV` لـ scikit-learn.


التحقق المتبادل المُشارك في قياس بلات هو عملية مُكلفة لمجموعات البيانات الكبيرة. بالإضافة إلى ذلك، قد لا تتوافق تقديرات الاحتمالية مع الدرجات:

- قد لا يكون "argmax" للدرجات هو argmax للاحتمالات
- في التصنيف الثنائي، قد يتم تسمية عينة بواسطة ``predict`` على أنها تنتمي إلى الفئة الإيجابية حتى إذا كان ناتج `predict_proba` أقل من 0.5؛ وبالمثل، يمكن تسميتها سلبية حتى لو كان ناتج `predict_proba` أكبر من 0.5.

من المعروف أيضًا أن طريقة بلات بها مشاكل نظرية. إذا كانت درجات الثقة مطلوبة، لكن لا يجب أن تكون احتمالات، فمن المستحسن تعيين ``probability=False`` واستخدام ``decision_function`` بدلاً من ``predict_proba``.


يرجى ملاحظة أنه عندما ``decision_function_shape='ovr'`` و ``n_classes > 2``، على عكس ``decision_function``، لا تحاول طريقة ``predict`` كسر الروابط افتراضيًا. يمكنك تعيين ``break_ties=True`` ليكون ناتج ``predict`` هو نفسه ``np.argmax(clf.decision_function(...), axis=1)``، وإلا فسيتم دائمًا إرجاع الفئة الأولى بين الفئات المُتعادلة؛ لكن ضع في اعتبارك أن ذلك يأتي بتكلفة حسابية. انظر :ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` للحصول على مثال لكسر الروابط.


المشاكل غير المتوازنة
--------------------


في المشاكل التي يكون من المرغوب فيه إعطاء أهمية أكبر لفئات مُعينة أو عينات فردية مُعينة، يمكن استخدام المعلمتين ``class_weight`` و ``sample_weight``.

:class:`SVC` (ولكن ليس :class:`NuSVC`) تُطبق المعلمة ``class_weight`` في طريقة ``fit``. إنه قاموس بالشكل ``{class_label : value}``، حيث القيمة هي رقم فاصلة عائمة > 0 يُعيّن المعلمة ``C`` للفئة ``class_label`` إلى ``C * value``. يُوضح الشكل أدناه حد القرار لمشكلة غير متوازنة، مع أو بدون تصحيح الوزن.

.. figure:: ../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png
   :target: ../auto_examples/svm/plot_separating_hyperplane_unbalanced.html
   :align: center
   :scale: 75


تُطبق :class:`SVC` و :class:`NuSVC` و :class:`SVR` و :class:`NuSVR` و :class:`LinearSVC` و :class:`LinearSVR` و :class:`OneClassSVM` أيضًا أوزانًا للعينات الفردية في طريقة `fit` من خلال المعلمة ``sample_weight``. على غرار ``class_weight``، يُعيّن هذا المعلمة ``C`` للمثال i إلى ``C * sample_weight[i]``، مما سيُشجع المُصنف على تصحيح هذه العينات. يُوضح الشكل أدناه تأثير ترجيح العينة على حد القرار. حجم الدوائر يتناسب مع أوزان العينة:



.. figure:: ../auto_examples/svm/images/sphx_glr_plot_weighted_samples_001.png
   :target: ../auto_examples/svm/plot_weighted_samples.html
   :align: center
   :scale: 75


.. rubric:: أمثلة


* :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py`
* :ref:`sphx_glr_auto_examples_svm_plot_weighted_samples.py`




.. _svm_regression:


الانحدار
==========

يمكن تمديد طريقة تصنيف متجه الدعم لحل مشاكل الانحدار. تُسمى هذه الطريقة انحدار متجه الدعم.

يعتمد النموذج الذي تم إنشاؤه بواسطة تصنيف متجه الدعم (كما هو موضح أعلاه) على مجموعة فرعية فقط من بيانات التدريب، لأن دالة التكلفة لبناء النموذج لا تهتم بنقاط التدريب التي تقع خارج الهامش. وبالمثل، يعتمد النموذج الذي تم إنشاؤه بواسطة انحدار متجه الدعم على مجموعة فرعية فقط من بيانات التدريب، لأن دالة التكلفة تتجاهل العينات التي يكون تنبؤها قريبًا من هدفها.

هناك ثلاثة تطبيقات مختلفة لانحدار متجه الدعم: :class:`SVR` و :class:`NuSVR` و :class:`LinearSVR`. يُوفر :class:`LinearSVR` تطبيقًا أسرع من :class:`SVR` ولكنه يأخذ في الاعتبار النواة الخطية فقط، بينما يُطبّق :class:`NuSVR` صيغة مختلفة قليلاً عن :class:`SVR` و :class:`LinearSVR`. نظرًا لتطبيقه في `liblinear`، فإن :class:`LinearSVR` يُنظّم أيضًا التقاطع، إذا تم اعتباره. ومع ذلك، يمكن تقليل هذا التأثير عن طريق الضبط الدقيق لمعلمة `intercept_scaling`، والتي تسمح لمصطلح التقاطع بأن يكون له سلوك تنظيم مختلف مقارنة بالميزات الأخرى. لذلك، يمكن أن تختلف نتائج التصنيف والدرجة عن المُصنفين الآخرين. انظر :ref:`svm_implementation_details` لمزيد من التفاصيل.

كما هو الحال مع فئات التصنيف، ستأخذ طريقة الملاءمة المتجهات X و y كوسيطات، إلا أنه في هذه الحالة، من المتوقع أن يكون لـ y قيم فاصلة عائمة بدلاً من القيم الصحيحة::


    >>> from sklearn import svm
    >>> X = [[0, 0], [2, 2]]
    >>> y = [0.5, 2.5]
    >>> regr = svm.SVR()
    >>> regr.fit(X, y)
    SVR()
    >>> regr.predict([[1, 1]])
    array([1.5])


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`



.. _svm_outlier_detection:

تقدير الكثافة، كشف الجدة
=======================================

تُطبق الفئة :class:`OneClassSVM` One-Class SVM الذي يُستخدم في كشف القيم المتطرفة.

انظر :ref:`outlier_detection` لوصف واستخدام OneClassSVM.


التعقيد
==========

تُعد آلات الدعم المتجهية (SVM) أدوات قوية، لكن متطلبات الحساب والتخزين الخاصة بها تزداد بسرعة مع عدد متجهات التدريب. جوهر SVM هو مشكلة برمجة تربيعية (QP)، تفصل متجهات الدعم عن باقي بيانات التدريب. يتدرج مُحلل QP المستخدم بواسطة التطبيق القائم على `libsvm`_ بين :math:`O(n_{features} \times n_{samples}^2)` و :math:`O(n_{features} \times n_{samples}^3)` اعتمادًا على مدى كفاءة استخدام ذاكرة التخزين المؤقت `libsvm`_ في الممارسة العملية (يعتمد على مجموعة البيانات). إذا كانت البيانات متفرقة جدًا، فيجب استبدال :math:`n_{features}` بمتوسط عدد الميزات غير الصفرية في متجه عينة.

بالنسبة للحالة الخطية، فإن الخوارزمية المستخدمة في :class:`LinearSVC` بواسطة تطبيق `liblinear`_ أكثر كفاءة بكثير من نظيرتها :class:`SVC` القائمة على `libsvm`_ ويمكنها التوسع بشكل خطي تقريبًا إلى ملايين العينات و/أو الميزات.


نصائح حول الاستخدام العملي
=====================


* **تجنب نسخ البيانات**: بالنسبة لـ :class:`SVC` و :class:`SVR` و :class:`NuSVC` و :class:`NuSVR`، إذا كانت البيانات التي تم تمريرها إلى طرق مُعينة غير متجاورة مرتبة حسب C وبدقة مُزدوجة، فسيتم نسخها قبل استدعاء تطبيق C الأساسي. يمكنك التحقق مما إذا كانت مصفوفة numpy مُعطاة متجاورة مرتبة حسب C عن طريق فحص سمة ``flags``.

  بالنسبة لـ :class:`LinearSVC` (و :class:`LogisticRegression <sklearn.linear_model.LogisticRegression>`)، سيتم نسخ أي مُدخلات تم تمريرها كمصفوفة numpy وتحويلها إلى تمثيل البيانات المتفرقة الداخلي لـ `liblinear`_ (أعداد عائمة بدقة مُزدوجة ومؤشرات int32 للمكونات غير الصفرية). إذا كنت تُريد ملاءمة مُصنف خطي واسع النطاق دون نسخ مصفوفة كثيفة مرتبة حسب C بدقة مُزدوجة كمدخلات، فإننا نقترح استخدام فئة :class:`SGDClassifier <sklearn.linear_model.SGDClassifier>` بدلاً من ذلك. يمكن تكوين دالة الهدف لتكون تقريبًا نفس نموذج :class:`LinearSVC`.

* **حجم ذاكرة التخزين المؤقت للنواة**: بالنسبة لـ :class:`SVC` و :class:`SVR` و :class:`NuSVC` و :class:`NuSVR`، فإن حجم ذاكرة التخزين المؤقت للنواة له تأثير قوي على أوقات التشغيل للمشاكل الأكبر. إذا كان لديك ذاكرة وصول عشوائي (RAM) كافية مُتاحة، فمن المُوصى به تعيين ``cache_size`` إلى قيمة أعلى من القيمة الافتراضية 200 (ميغابايت)، مثل 500 (ميغابايت) أو 1000 (ميغابايت).


* **تعيين C**: ``C`` هي ``1`` افتراضيًا وهي خيار افتراضي معقول. إذا كان لديك الكثير من المشاهدات المزعجة، فيجب عليك تقليلها: تقليل C يقابل المزيد من التنظيم.

  :class:`LinearSVC` و :class:`LinearSVR` أقل حساسية لـ ``C`` عندما تُصبح كبيرة، وتتوقف نتائج التنبؤ عن التحسن بعد عتبة مُعينة. في هذه الأثناء، ستستغرق قيم ``C`` الأكبر وقتًا أطول للتدريب، وأحيانًا تصل إلى 10 مرات أطول، كما هو موضح في [#3]_.

* خوارزميات آلة متجه الدعم ليست ثابتة المقياس، لذلك **يُوصى بشدة بقياس بياناتك**. على سبيل المثال، قم بقياس كل سمة على متجه الإدخال X إلى [0,1] أو [-1,+1]، أو قم بتوحيدها قياسيًا للحصول على متوسط 0 وتباين 1. لاحظ أنه يجب تطبيق *نفس* القياس على متجه الاختبار للحصول على نتائج ذات مغزى. يمكن القيام بذلك بسهولة باستخدام :class:`~sklearn.pipeline.Pipeline`::


      >>> from sklearn.pipeline import make_pipeline
      >>> from sklearn.preprocessing import StandardScaler
      >>> from sklearn.svm import SVC

      >>> clf = make_pipeline(StandardScaler(), SVC())



  انظر القسم :ref:`preprocessing` لمزيد من التفاصيل حول القياس والتطبيع.


.. _shrinking_svm:

* فيما يتعلق بمعلمة `shrinking`، نقلاً عن [#4]_: *وجدنا أنه إذا كان عدد التكرارات كبيرًا، فيمكن للانكماش تقصير وقت التدريب. ومع ذلك، إذا قمنا بحل مشكلة التحسين بشكل فضفاض (على سبيل المثال، باستخدام تفاوت إيقاف كبير)، فقد يكون الرمز بدون استخدام الانكماش أسرع بكثير*


* تُقارب المعلمة ``nu`` في :class:`NuSVC`/:class:`OneClassSVM`/:class:`NuSVR` جزء أخطاء متجهات الدعم والهامش.

* في :class:`SVC`، إذا كانت البيانات غير متوازنة (على سبيل المثال، الكثير من الإيجابيات وقليل من السلبيات)، فعيّن ``class_weight='balanced'`` و/أو جرب معلمات جزاء مختلفة ``C``.

* **عشوائية التطبيقات الأساسية**: تستخدم التطبيقات الأساسية لـ :class:`SVC` و :class:`NuSVC` مُولّد أرقام عشوائية فقط لخلط البيانات لتقدير الاحتمالية (عندما يتم تعيين ``probability`` إلى ``True``). يمكن التحكم في هذه العشوائية باستخدام معلمة ``random_state``. إذا تم تعيين ``probability`` إلى ``False``، فإن هذه المقدرات ليست عشوائية وليس لـ ``random_state`` أي تأثير على النتائج. تطبيق :class:`OneClassSVM` الأساسي مشابه لتطبيقات :class:`SVC` و :class:`NuSVC`. نظرًا لعدم توفير تقدير احتمالية لـ :class:`OneClassSVM`، فهو ليس عشوائيًا.

  يستخدم تطبيق :class:`LinearSVC` الأساسي مُولّد أرقام عشوائية لتحديد الميزات عند ملاءمة النموذج مع هبوط إحداثي مُزدوج (أي عندما يتم تعيين ``dual`` إلى ``True``). وبالتالي، ليس من غير المألوف الحصول على نتائج مختلفة قليلاً لنفس بيانات الإدخال. إذا حدث ذلك، فجرب معلمة `tol` أصغر. يمكن أيضًا التحكم في هذه العشوائية باستخدام معلمة ``random_state``. عندما يتم تعيين ``dual`` إلى ``False``، فإن التطبيق الأساسي لـ :class:`LinearSVC` ليس عشوائيًا وليس لـ ``random_state`` أي تأثير على النتائج.


* يؤدي استخدام جزاء L1 كما هو مُقدّم بواسطة ``LinearSVC(penalty='l1', dual=False)`` إلى حل متفرق، أي أن مجموعة فرعية فقط من أوزان الميزات تختلف عن الصفر وتُساهم في دالة القرار. تؤدي زيادة ``C`` إلى نموذج أكثر تعقيدًا (يتم تحديد المزيد من الميزات). يمكن حساب قيمة ``C`` التي تُعطي نموذجًا "فارغًا" (جميع الأوزان تساوي صفرًا) باستخدام :func:`l1_min_c`.



.. _svm_kernels:

دوال النواة
================

يمكن أن تكون *دالة النواة* أيًا مما يلي:


* خطية: :math:`\langle x, x'\rangle`.

* متعددة الحدود: :math:`(\gamma \langle x, x'\rangle + r)^d`، حيث يتم تحديد :math:`d` بواسطة المعلمة ``degree``، :math:`r` بواسطة ``coef0``.

* rbf: :math:`\exp(-\gamma \|x-x'\|^2)`، حيث يتم تحديد :math:`\gamma` بواسطة المعلمة ``gamma``، يجب أن تكون أكبر من 0.


* السيني: :math:`\tanh(\gamma \langle x,x'\rangle + r)`، حيث يتم تحديد :math:`r` بواسطة ``coef0``.

يتم تحديد نوى مختلفة بواسطة المعلمة `kernel`::


    >>> linear_svc = svm.SVC(kernel='linear')
    >>> linear_svc.kernel
    'linear'
    >>> rbf_svc = svm.SVC(kernel='rbf')
    >>> rbf_svc.kernel
    'rbf'

انظر أيضًا :ref:`kernel_approximation` لحل لاستخدام نوى RBF أسرع وأكثر قابلية للتطوير.



معلمات نواة RBF
----------------------------

عند تدريب SVM مع نواة *دالة الأساس الشعاعي* (RBF)، يجب مراعاة معلمتين: ``C`` و ``gamma``. المعلمة ``C``، الشائعة لجميع نوى SVM، تُوازن بين التصنيف الخاطئ لأمثلة التدريب وبساطة سطح القرار. ``C`` المنخفضة تجعل سطح القرار سلسًا، بينما تهدف ``C`` العالية إلى تصنيف جميع أمثلة التدريب بشكل صحيح. تُعرّف ``gamma`` مقدار تأثير مثال تدريب واحد. كلما زادت ``gamma``، زادت قرب الأمثلة الأخرى لتتأثر.

الاختيار المناسب لـ ``C`` و ``gamma`` أمر بالغ الأهمية لأداء SVM. يُنصح باستخدام :class:`~sklearn.model_selection.GridSearchCV` مع ``C`` و ``gamma`` متباعدتين أسيًا لاختيار قيم جيدة.


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`
* :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`


نوى مخصصة
--------------


يمكنك تعريف النوى الخاصة بك إما عن طريق إعطاء النواة كدالة بايثون أو عن طريق حساب مصفوفة غرام مُسبقًا.

تتصرف المُصنفات ذات النوى المخصصة بنفس طريقة أي مُصنفات أخرى، باستثناء:


* الحقل ``support_vectors_`` فارغ الآن، يتم تخزين مؤشرات متجهات الدعم فقط في ``support_``

* يتم تخزين مرجع (وليس نسخة) للوسيطة الأولى في طريقة ``fit()`` للرجوع إليها في المستقبل. إذا تغيرت تلك المصفوفة بين استخدام ``fit()`` و ``predict()``، فستحصل على نتائج غير متوقعة.


.. dropdown:: استخدام دوال بايثون كنوى

  يمكنك استخدام النوى المُحدّدة الخاصة بك عن طريق تمرير دالة إلى المعلمة ``kernel``.

  يجب أن تأخذ نواتك مصفوفتين ذات شكل ``(n_samples_1, n_features)`` و ``(n_samples_2, n_features)`` كوسيطات وتُعيد مصفوفة نواة ذات شكل ``(n_samples_1, n_samples_2)``.

  يُحدّد الكود التالي نواة خطية وينشئ مثيل مُصنف سيستخدم تلك النواة::


      >>> import numpy as np
      >>> from sklearn import svm
      >>> def my_kernel(X, Y):
      ...     return np.dot(X, Y.T)
      ...
      >>> clf = svm.SVC(kernel=my_kernel)



.. dropdown:: استخدام مصفوفة غرام

  يمكنك تمرير نوى محسوبة مُسبقًا باستخدام الخيار ``kernel='precomputed'``. يجب عليك بعد ذلك تمرير مصفوفة غرام بدلاً من X إلى طريقتي `fit` و `predict`. يجب توفير قيم النواة بين *جميع* متجهات التدريب ومتجهات الاختبار:


      >>> import numpy as np
      >>> from sklearn.datasets import make_classification
      >>> from sklearn.model_selection import train_test_split
      >>> from sklearn import svm
      >>> X, y = make_classification(n_samples=10, random_state=0)
      >>> X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)
      >>> clf = svm.SVC(kernel='precomputed')
      >>> # حساب النواة الخطية
      >>> gram_train = np.dot(X_train, X_train.T)
      >>> clf.fit(gram_train, y_train)
      SVC(kernel='precomputed')
      >>> # التنبؤ بأمثلة التدريب
      >>> gram_test = np.dot(X_test, X_train.T)
      >>> clf.predict(gram_test)
      array([0, 1, 0])



.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_svm_plot_custom_kernel.py`



.. _svm_mathematical_formulation:


الصيغة الرياضية
========================

تنشئ آلة متجه الدعم مستوى فائق أو مجموعة من المستويات الفائقة في فضاء ذي أبعاد عالية أو لا نهائية، والتي يمكن استخدامها للتصنيف أو الانحدار أو مهام أخرى. بشكل حدسي، يتم تحقيق فصل جيد بواسطة المستوى الفائق الذي لديه أكبر مسافة إلى أقرب نقاط بيانات التدريب لأي فئة (ما يُسمى بالهامش الوظيفي)، لأنه بشكل عام كلما زاد الهامش، قل خطأ التعميم للمُصنف. يُظهر الشكل أدناه دالة القرار لمشكلة قابلة للفصل خطيًا، مع ثلاث عينات على حدود الهامش، تُسمى "متجهات الدعم":


.. figure:: ../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_001.png
   :align: center
   :scale: 75


بشكل عام، عندما لا تكون المشكلة قابلة للفصل خطيًا، فإن متجهات الدعم هي العينات *ضمن* حدود الهامش.

نُوصي بـ [#5]_ و [#6]_ كمراجع جيدة لنظرية SVM وتطبيقاتها العملية.



SVC
---

بالنظر إلى متجهات التدريب :math:`x_i \in \mathbb{R}^p`، i=1,..., n، في فئتين، ومتجه :math:`y \in \{1, -1\}^n`، هدفنا هو إيجاد :math:`w \in \mathbb{R}^p` و :math:`b \in \mathbb{R}` بحيث يكون التنبؤ الذي قدمته :math:`\text{sign} (w^T\phi(x) + b)` صحيحًا لمعظم العينات.

SVC تحل المشكلة الأساسية التالية:


.. math::

    \min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i

    \textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
        & \zeta_i \geq 0, i=1, ..., n

بشكل حدسي، نحاول تعظيم الهامش (عن طريق تصغير :math:`||w||^2 = w^Tw`)، مع تكبد عقوبة عندما يتم تصنيف عينة بشكل خاطئ أو ضمن حدود الهامش. من الناحية المثالية، ستكون قيمة :math:`y_i (w^T \phi (x_i) + b)` هي :math:`\geq 1` لجميع العينات، مما يشير إلى تنبؤ مثالي. لكن المشاكل عادةً ما تكون غير قابلة للفصل بشكل مثالي باستخدام مستوى فائق، لذلك نسمح لبعض العينات بأن تكون على مسافة :math:`\zeta_i` من حدود الهامش الصحيحة. يتحكم مصطلح الجزاء `C` في قوة هذه العقوبة، ونتيجة لذلك، يعمل كمعلمة تنظيم عكسي (انظر الملاحظة أدناه).

المشكلة المزدوجة للمشكلة الأساسية هي


.. math::

   \min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha


   \textrm {subject to } & y^T \alpha = 0\\
   & 0 \leq \alpha_i \leq C, i=1, ..., n

حيث :math:`e` هو متجه كل الآحاد، :math:`Q` هي مصفوفة شبه موجبة :math:`n` في :math:`n`، :math:`Q_{ij} \equiv y_i y_j K(x_i, x_j)`، حيث :math:`K(x_i, x_j) = \phi (x_i)^T \phi (x_j)` هي النواة. تُسمى المصطلحات :math:`\alpha_i` المعاملات المزدوجة، وهي مُحدّدة بحد أعلى :math:`C`. يُبرز هذا التمثيل المزدوج حقيقة أن متجهات التدريب يتم تعيينها ضمنيًا في فضاء ذي أبعاد أعلى (ربما لانهائية) بواسطة الدالة :math:`\phi`: انظر `خدعة النواة <https://en.wikipedia.org/wiki/Kernel_method>`_.


بمجرد حل مشكلة التحسين، يُصبح ناتج :term:`decision_function` لعينة مُعطاة :math:`x` هو:

.. math:: \sum_{i\in SV} y_i \alpha_i K(x_i, x) + b,

وتتوافق الفئة المتوقعة مع علامتها. نحتاج فقط إلى الجمع على متجهات الدعم (أي العينات التي تقع داخل الهامش) لأن المعاملات المزدوجة :math:`\alpha_i` تساوي صفرًا للعينات الأخرى.

يمكن الوصول إلى هذه المعلمات من خلال السمات ``dual_coef_`` التي تحتوي على حاصل الضرب :math:`y_i \alpha_i`، ``support_vectors_`` التي تحتوي على متجهات الدعم، و ``intercept_`` التي تحتوي على المصطلح المستقل :math:`b`.

.. note::

    بينما تستخدم نماذج SVM المُشتقة من `libsvm`_ و `liblinear`_ ``C`` كمعلمة تنظيم، فإن معظم المقدرات الأخرى تستخدم ``alpha``. تعتمد التكافؤ الدقيق بين مقدار تنظيم نموذجين على دالة الهدف الدقيقة التي تم تحسينها بواسطة النموذج. على سبيل المثال، عندما يكون المقدّر المستخدم هو انحدار :class:`~sklearn.linear_model.Ridge`، فإن العلاقة بينهما تُعطى على أنها :math:`C = \frac{1}{alpha}`.



.. dropdown:: LinearSVC

  يمكن صياغة المشكلة الأساسية بشكل مُكافئ على النحو التالي:

  .. math::

      \min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}^{n}\max(0, 1 - y_i (w^T \phi(x_i) + b)),


  حيث نستخدم `خسارة المفصلة
  <https://en.wikipedia.org/wiki/Hinge_loss>`_. هذا هو الشكل الذي تم تحسينه مُباشرةً بواسطة :class:`LinearSVC`، ولكن على عكس الشكل المزدوج، لا يتضمن هذا الشكل حاصل الضرب الداخلي بين العينات، لذلك لا يمكن تطبيق خدعة النواة الشهيرة. هذا هو السبب في أن النواة الخطية فقط مدعومة بواسطة :class:`LinearSVC` (:math:`\phi` هي دالة الهوية).


.. _nu_svc:

.. dropdown:: NuSVC

  صيغة :math:`\nu`-SVC [#7]_ هي إعادة تحديد معلمات لـ :math:`C`-SVC، وبالتالي فهي مكافئة رياضيًا.


  نُقدّم معلمة جديدة :math:`\nu` (بدلاً من :math:`C`) التي تتحكم في عدد متجهات الدعم و *أخطاء الهامش*: :math:`\nu \in (0, 1]` هو حد أعلى لكسر أخطاء الهامش وحد أدنى لكسر متجهات الدعم. يتوافق خطأ الهامش مع عينة تقع على الجانب الخطأ من حدود هامشها: إما أنها مُصنّفة بشكل خاطئ، أو أنها مُصنّفة بشكل صحيح ولكنها لا تقع خارج الهامش.



SVR
---

بالنظر إلى متجهات التدريب :math:`x_i \in \mathbb{R}^p`، i=1,..., n، ومتجه :math:`y \in \mathbb{R}^n`، فإن :math:`\varepsilon`-SVR يحل المشكلة الأساسية التالية:


.. math::

    \min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i=1}^{n} (\zeta_i + \zeta_i^*)



    \textrm {subject to } & y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\
                          & w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\
                          & \zeta_i, \zeta_i^* \geq 0, i=1, ..., n


هنا، نُعاقب العينات التي يكون تنبؤها على الأقل :math:`\varepsilon` بعيدًا عن هدفها الحقيقي. تُعاقب هذه العينات الهدف بواسطة :math:`\zeta_i` أو :math:`\zeta_i^*`، اعتمادًا على ما إذا كانت تنبؤاتها تقع فوق أو أسفل أنبوب :math:`\varepsilon`.

المشكلة المزدوجة هي

.. math::

   \min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)


   \textrm {subject to } & e^T (\alpha - \alpha^*) = 0\\
   & 0 \leq \alpha_i, \alpha_i^* \leq C, i=1, ..., n


حيث :math:`e` هو متجه كل الآحاد، :math:`Q` هي مصفوفة شبه موجبة :math:`n` في :math:`n`، :math:`Q_{ij} \equiv K(x_i, x_j) = \phi (x_i)^T \phi (x_j)` هي النواة. هنا يتم تعيين متجهات التدريب ضمنيًا في فضاء ذي أبعاد أعلى (ربما لانهائية) بواسطة الدالة :math:`\phi`.


التنبؤ هو:


.. math:: \sum_{i \in SV}(\alpha_i - \alpha_i^*) K(x_i, x) + b

يمكن الوصول إلى هذه المعلمات من خلال السمات ``dual_coef_`` التي تحتوي على الفرق :math:`\alpha_i - \alpha_i^*`، ``support_vectors_`` التي تحتوي على متجهات الدعم، و ``intercept_`` التي تحتوي على المصطلح المستقل :math:`b`.

.. dropdown:: LinearSVR

  يمكن صياغة المشكلة الأساسية بشكل مُكافئ على النحو التالي:


  .. math::

      \min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}^{n}\max(0, |y_i - (w^T \phi(x_i) + b)| - \varepsilon),


  حيث نستخدم خسارة غير حساسة لأبسيلون، أي يتم تجاهل الأخطاء الأقل من :math:`\varepsilon`. هذا هو الشكل الذي تم تحسينه مُباشرةً بواسطة :class:`LinearSVR`.


.. _svm_implementation_details:


تفاصيل التطبيق
======================

داخليًا، نستخدم `libsvm`_ [#4]_ و `liblinear`_ [#3]_ للتعامل مع جميع الحسابات. يتم تغليف هذه المكتبات باستخدام C و Cython. للحصول على وصف للتطبيق وتفاصيل الخوارزميات المستخدمة، يرجى الرجوع إلى أوراقهما الخاصة.



.. _`libsvm`: https://www.csie.ntu.edu.tw/~cjlin/libsvm/
.. _`liblinear`: https://www.csie.ntu.edu.tw/~cjlin/liblinear/

.. rubric:: المراجع

.. [#1] Platt `"Probabilistic outputs for SVMs and comparisons to
  regularized likelihood methods"
  <https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf>`_.

.. [#2] Wu, Lin and Weng, `"Probability estimates for multi-class
  classification by pairwise coupling"
  <https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf>`_,
  JMLR 5:975-1005, 2004.

.. [#3] Fan, Rong-En, et al.,
  `"LIBLINEAR: A library for large linear classification."
  <https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf>`_,
  Journal of machine learning research 9.Aug (2008): 1871-1874.

.. [#4] Chang and Lin, `LIBSVM: A Library for Support Vector Machines
  <https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_.

.. [#5] Bishop, `Pattern recognition and machine learning
  <https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>`_,
  chapter 7 Sparse Kernel Machines

.. [#6] :doi:`"A Tutorial on Support Vector Regression"
  <10.1023/B:STCO.0000035301.49549.88>`
  Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive
  Volume 14 Issue 3, August 2004, p. 199-222.

.. [#7] Schölkopf et. al `New Support Vector Algorithms
  <https://www.stat.purdue.edu/~yuzhu/stat598m3/Papers/NewSVM.pdf>`_

.. [#8] Crammer and Singer `On the Algorithmic Implementation ofMulticlass
  Kernel-based Vector Machines
  <http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf>`_, JMLR 2001.


