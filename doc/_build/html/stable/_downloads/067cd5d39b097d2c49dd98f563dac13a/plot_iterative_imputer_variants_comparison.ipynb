{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u0627\u0633\u062a\u0646\u062a\u0627\u062c \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 \u0645\u0639 \u0645\u062a\u063a\u064a\u0631\u0627\u062a \u0645\u0646 IterativeImputer\n\n.. currentmodule:: sklearn\n\n\u0627\u0644\u0641\u0626\u0629 :class:`~impute.IterativeImputer` \u0645\u0631\u0646\u0629 \u0644\u0644\u063a\u0627\u064a\u0629 - \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647\u0627 \u0645\u0639 \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u062a\u0646\u0648\u0639\u0629 \u0645\u0646 \u0623\u062f\u0648\u0627\u062a \u0627\u0644\u062a\u0642\u062f\u064a\u0631 \u0644\u0644\u0642\u064a\u0627\u0645 \u0628\u0627\u0646\u062d\u062f\u0627\u0631 \u062f\u0627\u0626\u0631\u064a\u060c \u0645\u0639\u0627\u0645\u0644\u0629 \u0643\u0644 \u0645\u062a\u063a\u064a\u0631 \u0643\u0645\u062e\u0631\u062c \u0628\u062f\u0648\u0631\u0647.\n\n\u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0645\u062b\u0627\u0644\u060c \u0646\u0642\u0627\u0631\u0646 \u0628\u0639\u0636 \u0623\u062f\u0648\u0627\u062a \u0627\u0644\u062a\u0642\u062f\u064a\u0631 \u0644\u063a\u0631\u0636 \u0627\u0633\u062a\u0646\u062a\u0627\u062c \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 \u0645\u0639 :class:`~impute.IterativeImputer`:\n\n* :class:`~linear_model.BayesianRidge`: \u0627\u0646\u062d\u062f\u0627\u0631 \u062e\u0637\u064a \u0645\u0646\u062a\u0638\u0645\n* :class:`~ensemble.RandomForestRegressor`: \u0627\u0646\u062d\u062f\u0627\u0631 \u063a\u0627\u0628\u0627\u062a \u0627\u0644\u0623\u0634\u062c\u0627\u0631 \u0627\u0644\u0639\u0634\u0648\u0627\u0626\u064a\u0629\n* :func:`~pipeline.make_pipeline` (:class:`~kernel_approximation.Nystroem`,\n  :class:`~linear_model.Ridge`): \u062e\u0637 \u0623\u0646\u0627\u0628\u064a\u0628 \u0645\u0639 \u062a\u0648\u0633\u064a\u0639 \u0646\u0648\u0627\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u062d\u062f\u0648\u062f \u0645\u0646 \u0627\u0644\u062f\u0631\u062c\u0629 2 \u0648\u0627\u0646\u062d\u062f\u0627\u0631 \u062e\u0637\u064a \u0645\u0646\u062a\u0638\u0645\n* :class:`~neighbors.KNeighborsRegressor`: \u0645\u0634\u0627\u0628\u0647 \u0644\u0637\u0631\u0642 \u0627\u0633\u062a\u0646\u062a\u0627\u062c KNN \u0627\u0644\u0623\u062e\u0631\u0649\n\n\u0645\u0646 \u0627\u0644\u0623\u0645\u0648\u0631 \u0630\u0627\u062a \u0627\u0644\u0623\u0647\u0645\u064a\u0629 \u0627\u0644\u062e\u0627\u0635\u0629 \u0642\u062f\u0631\u0629 :class:`~impute.IterativeImputer` \u0639\u0644\u0649 \u0645\u062d\u0627\u0643\u0627\u0629 \u0633\u0644\u0648\u0643 missForest\u060c \u0648\u0647\u064a \u062d\u0632\u0645\u0629 \u0627\u0633\u062a\u0646\u062a\u0627\u062c \u0634\u0627\u0626\u0639 \u0644\u0640 R.\n\n\u0644\u0627\u062d\u0638 \u0623\u0646 :class:`~neighbors.KNeighborsRegressor` \u064a\u062e\u062a\u0644\u0641 \u0639\u0646 \u0627\u0633\u062a\u0646\u062a\u0627\u062c KNN\u060c \u0627\u0644\u0630\u064a \u064a\u062a\u0639\u0644\u0645 \u0645\u0646 \u0627\u0644\u0639\u064a\u0646\u0627\u062a \u0630\u0627\u062a \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0645\u0642\u064a\u0627\u0633 \u0645\u0633\u0627\u0641\u0629 \u064a\u0641\u0633\u0631 \u0641\u064a \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629\u060c \u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0627\u0633\u062a\u0646\u062a\u0627\u062c\u0647\u0627.\n\n\u0627\u0644\u0647\u062f\u0641 \u0647\u0648 \u0645\u0642\u0627\u0631\u0646\u0629 \u0623\u062f\u0648\u0627\u062a \u0627\u0644\u062a\u0642\u062f\u064a\u0631 \u0627\u0644\u0645\u062e\u062a\u0644\u0641\u0629 \u0644\u0645\u0639\u0631\u0641\u0629 \u0623\u064a\u0647\u0627 \u0627\u0644\u0623\u0641\u0636\u0644 \u0644\u0640 :class:`~impute.IterativeImputer` \u0639\u0646\u062f \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0623\u062f\u0627\u0629 \u062a\u0642\u062f\u064a\u0631 :class:`~linear_model.BayesianRidge` \u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u0625\u0633\u0643\u0627\u0646 \u0643\u0627\u0644\u064a\u0641\u0648\u0631\u0646\u064a\u0627 \u0645\u0639 \u0642\u064a\u0645\u0629 \u0648\u0627\u062d\u062f\u0629 \u062a\u0645 \u0625\u0632\u0627\u0644\u0647\u0627 \u0639\u0634\u0648\u0627\u0626\u064a\u064b\u0627 \u0645\u0646 \u0643\u0644 \u0635\u0641.\n\n\u0628\u0627\u0644\u0646\u0633\u0628\u0629 \u0644\u0647\u0630\u0627 \u0627\u0644\u0646\u0645\u0637 \u0627\u0644\u0645\u062d\u062f\u062f \u0645\u0646 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629\u060c \u0646\u0631\u0649 \u0623\u0646 :class:`~linear_model.BayesianRidge` \u0648 :class:`~ensemble.RandomForestRegressor` \u064a\u0639\u0637\u064a\u0627\u0646 \u0623\u0641\u0636\u0644 \u0627\u0644\u0646\u062a\u0627\u0626\u062c.\n\n\u062a\u062c\u062f\u0631 \u0627\u0644\u0625\u0634\u0627\u0631\u0629 \u0625\u0644\u0649 \u0623\u0646 \u0628\u0639\u0636 \u0623\u062f\u0648\u0627\u062a \u0627\u0644\u062a\u0642\u062f\u064a\u0631 \u0645\u062b\u0644 :class:`~ensemble.HistGradientBoostingRegressor` \u064a\u0645\u0643\u0646\u0647\u0627 \u0627\u0644\u062a\u0639\u0627\u0645\u0644 \u0628\u0634\u0643\u0644 \u0623\u0635\u0644\u064a \u0645\u0639 \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 \u0648\u063a\u0627\u0644\u0628\u064b\u0627 \u0645\u0627 \u064a\u0648\u0635\u0649 \u0628\u0647\u0627 \u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0625\u0646\u0634\u0627\u0621 \u062e\u0637\u0648\u0637 \u0623\u0646\u0627\u0628\u064a\u0628 \u0645\u0639 \u0627\u0633\u062a\u0631\u0627\u062a\u064a\u062c\u064a\u0627\u062a \u0627\u0633\u062a\u0646\u062a\u0627\u062c \u0642\u064a\u0645 \u0645\u0641\u0642\u0648\u062f\u0629 \u0645\u0639\u0642\u062f\u0629 \u0648\u0645\u0643\u0644\u0641\u0629.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\n\n# \u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0647\u0630\u0647 \u0627\u0644\u0645\u064a\u0632\u0629 \u0627\u0644\u062a\u062c\u0631\u064a\u0628\u064a\u0629\u060c \u0646\u062d\u062a\u0627\u062c \u0625\u0644\u0649 \u0637\u0644\u0628\u0647\u0627 \u0635\u0631\u0627\u062d\u0629\u064b:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import BayesianRidge, Ridge\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\n\nN_SPLITS = 5\n\nrng = np.random.RandomState(0)\n\nX_full, y_full = fetch_california_housing(return_X_y=True)\n# ~ 2k \u0639\u064a\u0646\u0629 \u0643\u0627\u0641\u064a\u0629 \u0644\u063a\u0631\u0636 \u0627\u0644\u0645\u062b\u0627\u0644.\n# \u0642\u0645 \u0628\u0625\u0632\u0627\u0644\u0629 \u0627\u0644\u0633\u0637\u0631\u064a\u0646 \u0627\u0644\u062a\u0627\u0644\u064a\u064a\u0646 \u0644\u062a\u0634\u063a\u064a\u0644 \u0623\u0628\u0637\u0623 \u0628\u0623\u0634\u0631\u0637\u0629 \u062e\u0637\u0623 \u0645\u062e\u062a\u0644\u0641\u0629.\nX_full = X_full[::10]\ny_full = y_full[::10]\nn_samples, n_features = X_full.shape\n\n# \u062a\u0642\u062f\u064a\u0631 \u0627\u0644\u0646\u062a\u064a\u062c\u0629 \u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0628\u0623\u0643\u0645\u0644\u0647\u0627\u060c \u0628\u062f\u0648\u0646 \u0642\u064a\u0645 \u0645\u0641\u0642\u0648\u062f\u0629\nbr_estimator = BayesianRidge()\nscore_full_data = pd.DataFrame(\n    cross_val_score(\n        br_estimator, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    ),\n    columns=[\"\u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0643\u0627\u0645\u0644\u0629\"],\n)\n\n# \u0625\u0636\u0627\u0641\u0629 \u0642\u064a\u0645\u0629 \u0645\u0641\u0642\u0648\u062f\u0629 \u0648\u0627\u062d\u062f\u0629 \u0644\u0643\u0644 \u0635\u0641\nX_missing = X_full.copy()\ny_missing = y_full\nmissing_samples = np.arange(n_samples)\nmissing_features = rng.choice(n_features, n_samples, replace=True)\nX_missing[missing_samples, missing_features] = np.nan\n\n# \u062a\u0642\u062f\u064a\u0631 \u0627\u0644\u0646\u062a\u064a\u062c\u0629 \u0628\u0639\u062f \u0627\u0644\u0627\u0633\u062a\u0646\u062a\u0627\u062c (\u0627\u0633\u062a\u0631\u0627\u062a\u064a\u062c\u064a\u0627\u062a \u0627\u0644\u0645\u062a\u0648\u0633\u0637 \u0648\u0627\u0644\u0648\u0633\u064a\u0637)\nscore_simple_imputer = pd.DataFrame()\nfor strategy in (\"mean\", \"median\"):\n    estimator = make_pipeline(\n        SimpleImputer(missing_values=np.nan, strategy=strategy), br_estimator\n    )\n    score_simple_imputer[strategy] = cross_val_score(\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\n# \u062a\u0642\u062f\u064a\u0631 \u0627\u0644\u0646\u062a\u064a\u062c\u0629 \u0628\u0639\u062f \u0627\u0644\u0627\u0633\u062a\u0646\u062a\u0627\u062c \u0627\u0644\u062a\u0643\u0631\u0627\u0631\u064a \u0644\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629\n# \u0645\u0639 \u0623\u062f\u0648\u0627\u062a \u062a\u0642\u062f\u064a\u0631 \u0645\u062e\u062a\u0644\u0641\u0629\nestimators = [\n    BayesianRidge(),\n    RandomForestRegressor(\n        # \u0642\u0645\u0646\u0627 \u0628\u0636\u0628\u0637 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0641\u0627\u0626\u0642\u0629 \u0644\u0640 RandomForestRegressor \u0644\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0623\u062f\u0627\u0621 \u062a\u0646\u0628\u0624\u064a \u062c\u064a\u062f \u0628\u0645\u0627 \u0641\u064a\u0647 \u0627\u0644\u0643\u0641\u0627\u064a\u0629 \u0644\u0648\u0642\u062a \u062a\u0646\u0641\u064a\u0630 \u0645\u062d\u062f\u0648\u062f.\n        n_estimators=4,\n        max_depth=10,\n        bootstrap=True,\n        max_samples=0.5,\n        n_jobs=2,\n        random_state=0,\n    ),\n    make_pipeline(\n        Nystroem(kernel=\"polynomial\", degree=2, random_state=0), Ridge(alpha=1e3)\n    ),\n    KNeighborsRegressor(n_neighbors=15),\n]\nscore_iterative_imputer = pd.DataFrame()\n# \u0623\u062f\u0627\u0629 \u0627\u0644\u0627\u0633\u062a\u0646\u062a\u0627\u062c \u0627\u0644\u062a\u0643\u0631\u0627\u0631\u064a \u062d\u0633\u0627\u0633\u0629 \u0644\u0644\u062a\u0633\u0627\u0645\u062d \u0648\n# \u062a\u0639\u062a\u0645\u062f \u0639\u0644\u0649 \u0623\u062f\u0627\u0629 \u0627\u0644\u062a\u0642\u062f\u064a\u0631 \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645\u0629 \u062f\u0627\u062e\u0644\u064a\u064b\u0627.\n# \u0642\u0645\u0646\u0627 \u0628\u0636\u0628\u0637 \u0627\u0644\u062a\u0633\u0627\u0645\u062d \u0644\u0644\u062d\u0641\u0627\u0638 \u0639\u0644\u0649 \u062a\u0634\u063a\u064a\u0644 \u0647\u0630\u0627 \u0627\u0644\u0645\u062b\u0627\u0644 \u0628\u0645\u0648\u0627\u0631\u062f \u062d\u0633\u0627\u0628\u064a\u0629 \u0645\u062d\u062f\u0648\u062f\u0629 \u0645\u0639 \u0639\u062f\u0645 \u062a\u063a\u064a\u064a\u0631 \u0627\u0644\u0646\u062a\u0627\u0626\u062c \u0643\u062b\u064a\u0631\u064b\u0627 \u0645\u0642\u0627\u0631\u0646\u0629\u064b \u0628\u0627\u0644\u062d\u0641\u0627\u0638 \u0639\u0644\u0649\n# \u0642\u064a\u0645\u0629 \u0627\u0641\u062a\u0631\u0627\u0636\u064a\u0629 \u0623\u0643\u062b\u0631 \u0635\u0631\u0627\u0645\u0629 \u0644\u0645\u0639\u0627\u0645\u0644 \u0627\u0644\u062a\u0633\u0627\u0645\u062d.\ntolerances = (1e-3, 1e-1, 1e-1, 1e-2)\nfor impute_estimator, tol in zip(estimators, tolerances):\n    estimator = make_pipeline(\n        IterativeImputer(\n            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol\n        ),\n        br_estimator,\n    )\n    score_iterative_imputer[impute_estimator.__class__.__name__] = cross_val_score(\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\nscores = pd.concat(\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\n    keys=[\"\u0627\u0644\u0623\u0635\u0644\u064a\u0629\", \"SimpleImputer\", \"IterativeImputer\"],\n    axis=1,\n)\n\n# plot california housing results\nfig, ax = plt.subplots(figsize=(13, 6))\nmeans = -scores.mean()\nerrors = scores.std()\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_title(\"\u0627\u0646\u062d\u062f\u0627\u0631 \u0625\u0633\u0643\u0627\u0646 \u0643\u0627\u0644\u064a\u0641\u0648\u0631\u0646\u064a\u0627 \u0645\u0639 \u0637\u0631\u0642 \u0627\u0633\u062a\u0646\u062a\u0627\u062c \u0645\u062e\u062a\u0644\u0641\u0629\")\nax.set_xlabel(\"MSE (\u0627\u0644\u0623\u0635\u063a\u0631 \u0647\u0648 \u0627\u0644\u0623\u0641\u0636\u0644)\")\nax.set_yticks(np.arange(means.shape[0]))\nax.set_yticklabels([\" \u0645\u0639 \".join(label) for label in means.index.tolist()])\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}