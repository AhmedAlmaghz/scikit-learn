{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u0627\u0633\u062a\u062e\u0631\u0627\u062c \u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u062a\u062d\u0644\u064a\u0644 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0627\u062a \u063a\u064a\u0631 \u0627\u0644\u0633\u0627\u0644\u0628\u0629 \u0648\u062a\u062e\u0635\u064a\u0635 \u062f\u064a\u0631\u064a\u062a\u0634\u0644\u064a\u062a \u0627\u0644\u0643\u0627\u0645\u0646\n\n\u0647\u0630\u0627 \u0645\u062b\u0627\u0644 \u0639\u0644\u0649 \u062a\u0637\u0628\u064a\u0642 :class:`~sklearn.decomposition.NMF` \u0648\n:class:`~sklearn.decomposition.LatentDirichletAllocation` \u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u0646 \u0627\u0644\u0648\u062b\u0627\u0626\u0642 \u0648\u0627\u0633\u062a\u062e\u0631\u0627\u062c \u0646\u0645\u0627\u0630\u062c \u0625\u0636\u0627\u0641\u064a\u0629 \u0644\u0647\u064a\u0643\u0644 \u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0641\u064a \u0627\u0644\u0645\u062c\u0645\u0648\u0639\u0629.  \u0627\u0644\u0645\u062e\u0631\u062c\u0627\u062a \u0639\u0628\u0627\u0631\u0629 \u0639\u0646 \u0631\u0633\u0645 \u0628\u064a\u0627\u0646\u064a \u0644\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a\u060c \u064a\u0645\u062b\u0644 \u0643\u0644 \u0645\u0646\u0647\u0627 \u0631\u0633\u0645\u064b\u0627 \u0628\u064a\u0627\u0646\u064a\u064b\u0627 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0639\u062f\u062f \u0642\u0644\u064a\u0644 \u0645\u0646 \u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0627\u0644\u0639\u0644\u064a\u0627 \u0628\u0646\u0627\u0621\u064b \u0639\u0644\u0649 \u0627\u0644\u0623\u0648\u0632\u0627\u0646.\n\n\u064a\u062a\u0645 \u062a\u0637\u0628\u064a\u0642 \u062a\u062d\u0644\u064a\u0644 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0627\u062a \u063a\u064a\u0631 \u0627\u0644\u0633\u0627\u0644\u0628\u0629 \u0645\u0639 \u062f\u0627\u0644\u062a\u064a\u0646 \u0645\u062e\u062a\u0644\u0641\u062a\u064a\u0646 \u0644\u0644\u0647\u062f\u0641: \u0645\u0639\u064a\u0627\u0631 \u0641\u0631\u0648\u0628\u064a\u0646\u064a\u0648\u0633\u060c \u0648\u0627\u0644\u062a\u0628\u0627\u0639\u062f \u0627\u0644\u0639\u0627\u0645 \u0644\u0643\u0648\u0644\u0628\u0627\u0643-\u0644\u0627\u064a\u0628\u0644\u0631.\n\u0627\u0644\u0623\u062e\u064a\u0631 \u064a\u0639\u0627\u062f\u0644 \u0627\u0644\u0641\u0647\u0631\u0633\u0629 \u0627\u0644\u062f\u0644\u0627\u0644\u064a\u0629 \u0627\u0644\u0643\u0627\u0645\u0646\u0629 \u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644\u064a\u0629.\n\n\u064a\u062c\u0628 \u0623\u0646 \u062a\u062c\u0639\u0644 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0627\u0641\u062a\u0631\u0627\u0636\u064a\u0629 (n_samples / n_features / n_components)\n\u0627\u0644\u0645\u062b\u0627\u0644 \u0642\u0627\u0628\u0644\u0627\u064b \u0644\u0644\u062a\u0634\u063a\u064a\u0644 \u0641\u064a \u0628\u0636\u0639 \u0639\u0634\u0631\u0627\u062a \u0645\u0646 \u0627\u0644\u062b\u0648\u0627\u0646\u064a. \u064a\u0645\u0643\u0646\u0643 \u0645\u062d\u0627\u0648\u0644\u0629 \u0632\u064a\u0627\u062f\u0629 \u0623\u0628\u0639\u0627\u062f \u0627\u0644\u0645\u0634\u0643\u0644\u0629\u060c \u0648\u0644\u0643\u0646 \u0643\u0646 \u0639\u0644\u0649 \u062f\u0631\u0627\u064a\u0629 \u0628\u0623\u0646 \u0627\u0644\u062a\u0639\u0642\u064a\u062f \u0627\u0644\u0632\u0645\u0646\u064a \u0647\u0648 \u0643\u062b\u064a\u0631 \u0627\u0644\u062d\u062f\u0648\u062f \u0641\u064a NMF. \u0641\u064a LDA\u060c \u0627\u0644\u062a\u0639\u0642\u064a\u062f \u0627\u0644\u0632\u0645\u0646\u064a \u064a\u062a\u0646\u0627\u0633\u0628 \u0645\u0639 (n_samples * iterations).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u0627\u0644\u0645\u0624\u0644\u0641\u0648\u0646: \u0645\u0637\u0648\u0631\u064a scikit-learn\n# \u0645\u0639\u0631\u0641 SPDX-License: BSD-3-Clause\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, MiniBatchNMF\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nn_samples = 2000\nn_features = 1000\nn_components = 10\nn_top_words = 20\nbatch_size = 128\ninit = \"nndsvda\"\n\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[-n_top_words:]\n        top_features = feature_names[top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n\n\n# \u062a\u062d\u0645\u064a\u0644 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a 20 newsgroups \u0648\u062a\u062d\u0648\u064a\u0644\u0647\u0627 \u0625\u0644\u0649 \u0646\u0627\u0642\u0644\u0627\u062a. \u0646\u0633\u062a\u062e\u062f\u0645 \u0628\u0639\u0636 \u0627\u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0627\u062a\n# \u0644\u062a\u0635\u0641\u064a\u0629 \u0627\u0644\u0645\u0635\u0637\u0644\u062d\u0627\u062a \u0639\u062f\u064a\u0645\u0629 \u0627\u0644\u0641\u0627\u0626\u062f\u0629 \u0641\u064a \u0648\u0642\u062a \u0645\u0628\u0643\u0631: \u064a\u062a\u0645 \u0625\u0632\u0627\u0644\u0629 \u0627\u0644\u0645\u0646\u0634\u0648\u0631\u0627\u062a \u0645\u0646 \u0627\u0644\u0631\u0624\u0648\u0633\u060c\n# \u0627\u0644\u0623\u0642\u062f\u0627\u0645 \u0648\u0627\u0644\u0631\u062f\u0648\u062f \u0627\u0644\u0645\u0642\u062a\u0628\u0633\u0629\u060c \u0648\u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0627\u0644\u0625\u0646\u062c\u0644\u064a\u0632\u064a\u0629 \u0627\u0644\u0634\u0627\u0626\u0639\u0629\u060c \u0648\u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0627\u0644\u062a\u064a \u062a\u062d\u062f\u062b \u0641\u064a\n# \u0648\u062b\u064a\u0642\u0629 \u0648\u0627\u062d\u062f\u0629 \u0641\u0642\u0637 \u0623\u0648 \u0641\u064a 95\u066a \u0639\u0644\u0649 \u0627\u0644\u0623\u0642\u0644 \u0645\u0646 \u0627\u0644\u0648\u062b\u0627\u0626\u0642.\n\nprint(\"\u062a\u062d\u0645\u064a\u0644 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a...\")\nt0 = time()\ndata, _ = fetch_20newsgroups(\n    shuffle=True,\n    random_state=1,\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    return_X_y=True,\n)\ndata_samples = data[:n_samples]\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\n\n# \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0645\u064a\u0632\u0627\u062a tf-idf \u0644\u0640 NMF.\nprint(\"\u0627\u0633\u062a\u062e\u0631\u0627\u062c \u0645\u064a\u0632\u0627\u062a tf-idf \u0644\u0640 NMF...\")\ntfidf_vectorizer = TfidfVectorizer(\n    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n)\nt0 = time()\ntfidf = tfidf_vectorizer.fit_transform(data_samples)\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\n\n# \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0645\u064a\u0632\u0627\u062a tf (\u0639\u062f\u062f \u0627\u0644\u0645\u0635\u0637\u0644\u062d\u0627\u062a \u0627\u0644\u062e\u0627\u0645) \u0644\u0640 LDA.\nprint(\"\u0627\u0633\u062a\u062e\u0631\u0627\u062c \u0645\u064a\u0632\u0627\u062a tf \u0644\u0640 LDA...\")\ntf_vectorizer = CountVectorizer(\n    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n)\nt0 = time()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\nprint()\n\n# \u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c NMF\nprint(\n    \"\u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c NMF (\u0645\u0639\u064a\u0627\u0631 \u0641\u0631\u0648\u0628\u064a\u0646\u064a\u0648\u0633) \u0645\u0639 \u0645\u064a\u0632\u0627\u062a tf-idf\u060c \"\n    \"n_samples=%d \u0648 n_features=%d...\" % (n_samples, n_features)\n)\nt0 = time()\nnmf = NMF(\n    n_components=n_components,\n    random_state=1,\n    init=init,\n    beta_loss=\"frobenius\",\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=1,\n).fit(tfidf)\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\n\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    nmf, tfidf_feature_names, n_top_words, \"\u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0641\u064a \u0646\u0645\u0648\u0630\u062c NMF (\u0645\u0639\u064a\u0627\u0631 \u0641\u0631\u0648\u0628\u064a\u0646\u064a\u0648\u0633)\"\n)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    nmf, tfidf_feature_names, n_top_words, \"\u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0641\u064a \u0646\u0645\u0648\u0630\u062c NMF (\u0645\u0639\u064a\u0627\u0631 \u0641\u0631\u0648\u0628\u064a\u0646\u064a\u0648\u0633)\"\n)\n\n# \u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c NMF\nprint(\n    \"\\n\" * 2,\n    \"\u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c NMF (\u0627\u0644\u062a\u0628\u0627\u0639\u062f \u0627\u0644\u0639\u0627\u0645 \u0644\u0643\u0648\u0644\u0628\u0627\u0643-\u0644\u0627\u064a\u0628\u0644\u0631) \u0645\u0639 \u0645\u064a\u0632\u0627\u062a tf-idf\u060c \"\n    \"n_samples=%d \u0648 n_features=%d...\" % (n_samples, n_features),\n)\nt0 = time()\nnmf = NMF(\n    n_components=n_components,\n    random_state=1,\n    init=init,\n    beta_loss=\"kullback-leibler\",\n    solver=\"mu\",\n    max_iter=1000,\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=0.5,\n).fit(tfidf)\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    nmf,\n    tfidf_feature_names,\n    n_top_words,\n    \"\u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0641\u064a \u0646\u0645\u0648\u0630\u062c NMF (\u0627\u0644\u062a\u0628\u0627\u0639\u062f \u0627\u0644\u0639\u0627\u0645 \u0644\u0643\u0648\u0644\u0628\u0627\u0643-\u0644\u0627\u064a\u0628\u0644\u0631)\",\n)\n\n# \u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c MiniBatchNMF\nprint(\n    \"\\n\" * 2,\n    \"\u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c MiniBatchNMF (\u0645\u0639\u064a\u0627\u0631 \u0641\u0631\u0648\u0628\u064a\u0646\u064a\u0648\u0633) \u0645\u0639 \u0645\u064a\u0632\u0627\u062a tf-idf \"\n    \"n_samples=%d \u0648 n_features=%d\u060c batch_size=%d...\"\n    % (n_samples, n_features, batch_size),\n)\nt0 = time()\nmbnmf = MiniBatchNMF(\n    n_components=n_components,\n    random_state=1,\n    batch_size=batch_size,\n    init=init,\n    beta_loss=\"frobenius\",\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=0.5,\n).fit(tfidf)\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\n\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    mbnmf,\n    tfidf_feature_names,\n    n_top_words,\n    \"\u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0641\u064a \u0646\u0645\u0648\u0630\u062c MiniBatchNMF (\u0645\u0639\u064a\u0627\u0631 \u0641\u0631\u0648\u0628\u064a\u0646\u064a\u0648\u0633)\",\n)\n\n# \u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c MiniBatchNMF\nprint(\n    \"\\n\" * 2,\n    \"\u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0648\u0630\u062c MiniBatchNMF (\u0627\u0644\u062a\u0628\u0627\u0639\u062f \u0627\u0644\u0639\u0627\u0645 \u0644\u0643\u0648\u0644\u0628\u0627\u0643-\u0644\u0627\u064a\u0628\u0644\u0631) \u0645\u0639 \u0645\u064a\u0632\u0627\u062a tf-idf\u060c \"\n    \"n_samples=%d \u0648 n_features=%d\u060c batch_size=%d...\"\n    % (n_samples, n_features, batch_size),\n)\nt0 = time()\nmbnmf = MiniBatchNMF(\n    n_components=n_components,\n    random_state=1,\n    batch_size=batch_size,\n    init=init,\n    beta_loss=\"kullback-leibler\",\n    alpha_W=0.00005,\n    alpha_H=0.00005,\n    l1_ratio=0.5,\n).fit(tfidf)\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nplot_top_words(\n    mbnmf,\n    tfidf_feature_names,\n    n_top_words,\n    \"\u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0641\u064a \u0646\u0645\u0648\u0630\u062c MiniBatchNMF (\u0627\u0644\u062a\u0628\u0627\u0639\u062f \u0627\u0644\u0639\u0627\u0645 \u0644\u0643\u0648\u0644\u0628\u0627\u0643-\u0644\u0627\u064a\u0628\u0644\u0631)\",\n)\n\nprint(\n    \"\\n\" * 2,\n    \"\u0645\u0644\u0627\u0621\u0645\u0629 \u0646\u0645\u0627\u0630\u062c LDA \u0645\u0639 \u0645\u064a\u0632\u0627\u062a tf\u060c n_samples=%d \u0648 n_features=%d...\"\n    % (n_samples, n_features),\n)\nlda = LatentDirichletAllocation(\n    n_components=n_components,\n    max_iter=5,\n    learning_method=\"online\",\n    learning_offset=50.0,\n    random_state=0,\n)\nt0 = time()\nlda.fit(tf)\nprint(\"\u062a\u0645 \u0641\u064a %0.3fs.\" % (time() - t0))\n\ntf_feature_names = tf_vectorizer.get_feature_names_out()\nplot_top_words(lda, tf_feature_names, n_top_words, \"\u0627\u0644\u0645\u0648\u0636\u0648\u0639\u0627\u062a \u0641\u064a \u0646\u0645\u0648\u0630\u062c LDA\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}