{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u062a\u0623\u062e\u064a\u0631 \u0627\u0644\u062a\u0646\u0628\u0624\n\n\u0647\u0630\u0627 \u0645\u062b\u0627\u0644 \u064a\u0648\u0636\u062d \u062a\u0623\u062e\u064a\u0631 \u0627\u0644\u062a\u0646\u0628\u0624 \u0644\u0645\u062e\u062a\u0644\u0641 \u0627\u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0627\u062a \u0641\u064a \u0645\u0643\u062a\u0628\u0629 \u0633\u0627\u064a \u0643\u064a\u062a \u0644\u064a\u0631\u0646.\n\n\u0627\u0644\u0647\u062f\u0641 \u0647\u0648 \u0642\u064a\u0627\u0633 \u0627\u0644\u062a\u0623\u062e\u064a\u0631 \u0627\u0644\u0645\u062a\u0648\u0642\u0639 \u0639\u0646\u062f \u0625\u062c\u0631\u0627\u0621 \u0627\u0644\u062a\u0646\u0628\u0624\u0627\u062a \u0625\u0645\u0627 \u0628\u0627\u0644\u062c\u0645\u0644\u0629 \u0623\u0648 \u0628\u0627\u0644\u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u0630\u0631\u064a\u0629 (\u0623\u064a \u0648\u0627\u062d\u062f\u064b\u0627 \u062a\u0644\u0648 \u0627\u0644\u0622\u062e\u0631).\n\n\u062a\u0645\u062b\u0644 \u0627\u0644\u0645\u062e\u0637\u0637\u0627\u062a \u062a\u0648\u0632\u064a\u0639 \u062a\u0623\u062e\u064a\u0631 \u0627\u0644\u062a\u0646\u0628\u0624 \u0639\u0644\u0649 \u0634\u0643\u0644 \u0645\u062e\u0637\u0637 \u0635\u0646\u062f\u0648\u0642\u064a.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u0627\u0644\u0645\u0624\u0644\u0641\u0648\u0646: \u0645\u0637\u0648\u0631\u064a \u0633\u0627\u064a \u0643\u064a\u062a \u0644\u064a\u0631\u0646\n# \u0645\u0639\u0631\u0641 \u0627\u0644\u062a\u0631\u062e\u064a\u0635: BSD-3-Clause\n\nimport gc\nimport time\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge, SGDRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.utils import shuffle\n\n\ndef _not_in_sphinx():\n    # \u062e\u062f\u0639\u0629 \u0644\u0644\u0643\u0634\u0641 \u0639\u0645\u0627 \u0625\u0630\u0627 \u0643\u0646\u0627 \u0646\u0639\u0645\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0628\u0627\u0646\u064a \u0633\u0641\u064a\u0646\u0643\u0633\n    return \"__file__\" in globals()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0648\u0638\u0627\u0626\u0641 \u0645\u0633\u0627\u0639\u062f\u0629 \u0644\u0644\u0642\u064a\u0627\u0633 \u0648\u0627\u0644\u0631\u0633\u0645\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def atomic_benchmark_estimator(estimator, X_test, verbose=False):\n    \"\"\"\u0642\u064a\u0627\u0633 \u0648\u0642\u062a \u062a\u0634\u063a\u064a\u0644 \u0627\u0644\u062a\u0646\u0628\u0624 \u0644\u0643\u0644 \u0645\u062b\u064a\u0644.\"\"\"\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_instances, dtype=float)\n    for i in range(n_instances):\n        instance = X_test[[i], :]\n        start = time.time()\n        estimator.predict(instance)\n        runtimes[i] = time.time() - start\n    if verbose:\n        print(\n            \"atomic_benchmark runtimes:\",\n            min(runtimes),\n            np.percentile(runtimes, 50),\n            max(runtimes),\n        )\n    return runtimes\n\n\ndef bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):\n    \"\"\"\u0642\u064a\u0627\u0633 \u0648\u0642\u062a \u062a\u0634\u063a\u064a\u0644 \u0627\u0644\u062a\u0646\u0628\u0624 \u0644\u0644\u0645\u062f\u062e\u0644\u0627\u062a \u0628\u0627\u0644\u0643\u0627\u0645\u0644.\"\"\"\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_bulk_repeats, dtype=float)\n    for i in range(n_bulk_repeats):\n        start = time.time()\n        estimator.predict(X_test)\n        runtimes[i] = time.time() - start\n    runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))\n    if verbose:\n        print(\n            \"bulk_benchmark runtimes:\",\n            min(runtimes),\n            np.percentile(runtimes, 50),\n            max(runtimes),\n        )\n    return runtimes\n\n\ndef benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):\n    \"\"\"\n    \u0642\u064a\u0627\u0633 \u0623\u0648\u0642\u0627\u062a \u0627\u0644\u062a\u0634\u063a\u064a\u0644 \u0644\u0644\u062a\u0646\u0628\u0624 \u0641\u064a \u0627\u0644\u0648\u0636\u0639 \u0627\u0644\u0630\u0631\u064a \u0648\u0627\u0644\u0645\u062c\u0645\u0639.\n\n    \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a\n    ----------\n    estimator : \u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0629 \u0645\u062f\u0631\u0628\u0629 \u0628\u0627\u0644\u0641\u0639\u0644 \u062a\u062f\u0639\u0645 `predict()`\n    X_test : \u0645\u062f\u062e\u0644\u0627\u062a \u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631\n    n_bulk_repeats : \u0639\u062f\u062f \u0645\u0631\u0627\u062a \u0627\u0644\u062a\u0643\u0631\u0627\u0631 \u0639\u0646\u062f \u062a\u0642\u064a\u064a\u0645 \u0627\u0644\u0648\u0636\u0639 \u0627\u0644\u0645\u062c\u0645\u0639\n\n    \u0627\u0644\u0639\u0627\u0626\u062f\u0627\u062a\n    -------\n    atomic_runtimes, bulk_runtimes : \u0632\u0648\u062c \u0645\u0646 `np.array` \u0627\u0644\u0630\u064a \u064a\u062d\u062a\u0648\u064a \u0639\u0644\u0649 \u0623\u0648\u0642\u0627\u062a \u0627\u0644\u062a\u0634\u063a\u064a\u0644 \u0628\u0627\u0644\u062b\u0648\u0627\u0646\u064a.\n\n    \"\"\"\n    atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)\n    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose)\n    return atomic_runtimes, bulk_runtimes\n\n\ndef generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):\n    \"\"\"\u062a\u0648\u0644\u064a\u062f \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u0644\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0628\u0627\u0644\u0645\u0639\u0627\u064a\u064a\u0631 \u0627\u0644\u0645\u062d\u062f\u062f\u0629.\"\"\"\n    if verbose:\n        print(\"generating dataset...\")\n\n    X, y, coef = make_regression(\n        n_samples=n_train + n_test, n_features=n_features, noise=noise, coef=True\n    )\n\n    random_seed = 13\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, train_size=n_train, test_size=n_test, random_state=random_seed\n    )\n    X_train, y_train = shuffle(X_train, y_train, random_state=random_seed)\n\n    X_scaler = StandardScaler()\n    X_train = X_scaler.fit_transform(X_train)\n    X_test = X_scaler.transform(X_test)\n\n    y_scaler = StandardScaler()\n    y_train = y_scaler.fit_transform(y_train[:, None])[:, 0]\n    y_test = y_scaler.transform(y_test[:, None])[:, 0]\n\n    gc.collect()\n    if verbose:\n        print(\"ok\")\n    return X_train, y_train, X_test, y_test\n\n\ndef boxplot_runtimes(runtimes, pred_type, configuration):\n    \"\"\"\n   \u0631\u0633\u0645 \u0645\u062e\u0637\u0637 \u062c\u062f\u064a\u062f \u0645\u0639 \u0645\u062e\u0637\u0637\u0627\u062a \u0635\u0646\u062f\u0648\u0642\u064a\u0629 \u0644\u0623\u0648\u0642\u0627\u062a \u0627\u0644\u062a\u0646\u0628\u0624.\n\n    \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a\n    ----------\n    runtimes : \u0642\u0627\u0626\u0645\u0629 \u0645\u0646 `np.array` \u0645\u0646 \u0627\u0644\u062a\u0623\u062e\u064a\u0631\u0627\u062a \u0628\u0627\u0644\u0645\u064a\u0643\u0631\u0648\u062b\u0627\u0646\u064a\u0629\n    cls_names : \u0642\u0627\u0626\u0645\u0629 \u0645\u0646 \u0623\u0633\u0645\u0627\u0621 \u0627\u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0627\u062a \u0627\u0644\u062a\u064a \u0648\u0644\u062f\u062a \u0623\u0648\u0642\u0627\u062a \u0627\u0644\u062a\u0646\u0628\u0624\n    pred_type : 'bulk' \u0623\u0648 'atomic'\n\n    \"\"\"\n\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    bp = plt.boxplot(\n        runtimes,\n    )\n\n    cls_infos = [\n        \"%s\\n(%d %s)\"\n        % (\n            estimator_conf[\"name\"],\n            estimator_conf[\"complexity_computer\"](estimator_conf[\"instance\"]),\n            estimator_conf[\"complexity_label\"],\n        )\n        for estimator_conf in configuration[\"estimators\"]\n    ]\n    plt.setp(ax1, xticklabels=cls_infos)\n    plt.setp(bp[\"boxes\"], color=\"black\")\n    plt.setp(bp[\"whiskers\"], color=\"black\")\n    plt.setp(bp[\"fliers\"], color=\"red\", marker=\"+\")\n\n    ax1.yaxis.grid(True, linestyle=\"-\", which=\"major\", color=\"lightgrey\", alpha=0.5)\n\n    ax1.set_axisbelow(True)\n    ax1.set_title(\n        \"Prediction Time per Instance - %s, %d feats.\"\n        % (pred_type.capitalize(), configuration[\"n_features\"])\n    )\n    ax1.set_ylabel(\"Prediction Time (us)\")\n\n    plt.show()\n\n\ndef benchmark(configuration):\n    \"\"\"\u062a\u0634\u063a\u064a\u0644 \u0627\u0644\u0642\u064a\u0627\u0633 \u0628\u0627\u0644\u0643\u0627\u0645\u0644.\"\"\"\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration[\"n_train\"], configuration[\"n_test\"], configuration[\"n_features\"]\n    )\n\n    stats = {}\n    for estimator_conf in configuration[\"estimators\"]:\n        print(\"Benchmarking\", estimator_conf[\"instance\"])\n        estimator_conf[\"instance\"].fit(X_train, y_train)\n        gc.collect()\n        a, b = benchmark_estimator(estimator_conf[\"instance\"], X_test)\n        stats[estimator_conf[\"name\"]] = {\"atomic\": a, \"bulk\": b}\n\n    cls_names = [\n        estimator_conf[\"name\"] for estimator_conf in configuration[\"estimators\"]\n    ]\n    runtimes = [1e6 * stats[clf_name][\"atomic\"] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, \"atomic\", configuration)\n    runtimes = [1e6 * stats[clf_name][\"bulk\"] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, \"bulk (%d)\" % configuration[\"n_test\"], configuration)\n\n\ndef n_feature_influence(estimators, n_train, n_test, n_features, percentile):\n    \"\"\"\n    \u062a\u0642\u062f\u064a\u0631 \u062a\u0623\u062b\u064a\u0631 \u0639\u062f\u062f \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0639\u0644\u0649 \u0648\u0642\u062a \u0627\u0644\u062a\u0646\u0628\u0624.\n\n    \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a\n    ----------\n\n    estimators : \u0642\u0627\u0645\u0648\u0633 \u0645\u0646 (\u0627\u0644\u0627\u0633\u0645 (str)\u060c \u0627\u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0629) \u0644\u0644\u0642\u064a\u0627\u0633\n    n_train : \u0639\u062f\u062f \u0645\u062b\u064a\u0644\u0627\u062a \u0627\u0644\u062a\u062f\u0631\u064a\u0628 (int)\n    n_test : \u0639\u062f\u062f \u0645\u062b\u064a\u0644\u0627\u062a \u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631 (int)\n    n_features : \u0642\u0627\u0626\u0645\u0629 \u0628\u0623\u0628\u0639\u0627\u062f \u0627\u0644\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0645\u0645\u064a\u0632\u0629 \u0644\u0644\u0627\u062e\u062a\u0628\u0627\u0631 (int)\n    percentile : \u0627\u0644\u0645\u0626\u0648\u064a\u0629 \u0627\u0644\u062a\u064a \u064a\u062a\u0645 \u0639\u0646\u062f\u0647\u0627 \u0642\u064a\u0627\u0633 \u0627\u0644\u0633\u0631\u0639\u0629 (int [0-100])\n\n    \u0627\u0644\u0639\u0627\u0626\u062f\u0627\u062a:\n    --------\n\n    percentiles : dict(estimator_name,\n                       dict(n_features, percentile_perf_in_us))\n\n    \"\"\"\n    percentiles = defaultdict(defaultdict)\n    for n in n_features:\n        print(\"benchmarking with %d features\" % n)\n        X_train, y_train, X_test, y_test = generate_dataset(n_train, n_test, n)\n        for cls_name, estimator in estimators.items():\n            estimator.fit(X_train, y_train)\n            gc.collect()\n            runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)\n            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes, percentile)\n    return percentiles\n\n\ndef plot_n_features_influence(percentiles, percentile):\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    colors = [\"r\", \"g\", \"b\"]\n    for i, cls_name in enumerate(percentiles.keys()):\n        x = np.array(sorted(percentiles[cls_name].keys()))\n        y = np.array([percentiles[cls_name][n] for n in x])\n        plt.plot(\n            x,\n            y,\n            color=colors[i],\n        )\n    ax1.yaxis.grid(True, linestyle=\"-\", which=\"major\", color=\"lightgrey\", alpha=0.5)\n    ax1.set_axisbelow(True)\n    ax1.set_title(\"Evolution of Prediction Time with #Features\")\n    ax1.set_xlabel(\"#Features\")\n    ax1.set_ylabel(\"Prediction Time at %d%%-ile (us)\" % percentile)\n    plt.show()\ndef benchmark_throughputs(configuration, duration_secs=0.1):\n    \"\"\"\u0642\u064a\u0627\u0633 \u0627\u0644\u0625\u0646\u062a\u0627\u062c\u064a\u0629 \u0644\u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0627\u062a \u0627\u0644\u0645\u062e\u062a\u0644\u0641\u0629.\"\"\"\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration[\"n_train\"], configuration[\"n_test\"], configuration[\"n_features\"]\n    )\n    throughputs = dict()\n    for estimator_config in configuration[\"estimators\"]:\n        estimator_config[\"instance\"].fit(X_train, y_train)\n        start_time = time.time()\n        n_predictions = 0\n        while (time.time() - start_time) < duration_secs:\n            estimator_config[\"instance\"].predict(X_test[[0]])\n            n_predictions += 1\n        throughputs[estimator_config[\"name\"]] = n_predictions / duration_secs\n    return throughputs\n\n\ndef plot_benchmark_throughput(throughputs, configuration):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors = [\"r\", \"g\", \"b\"]\n    cls_infos = [\n        \"%s\\n(%d %s)\"\n        % (\n            estimator_conf[\"name\"],\n            estimator_conf[\"complexity_computer\"](estimator_conf[\"instance\"]),\n            estimator_conf[\"complexity_label\"],\n        )\n        for estimator_conf in configuration[\"estimators\"]\n    ]\n    cls_values = [\n        throughputs[estimator_conf[\"name\"]]\n        for estimator_conf in configuration[\"estimators\"]\n    ]\n    plt.bar(range(len(throughputs)), cls_values, width=0.5, color=colors)\n    ax.set_xticks(np.linspace(0.25, len(throughputs) - 0.75, len(throughputs)))\n    ax.set_xticklabels(cls_infos, fontsize=10)\n    ymax = max(cls_values) * 1.2\n    ax.set_ylim((0, ymax))\n    ax.set_ylabel(\"Throughput (predictions/sec)\")\n    ax.set_title(\n        \"Prediction Throughput for different estimators (%d features)\"\n        % configuration[\"n_features\"]\n    )\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0642\u064a\u0627\u0633 \u0633\u0631\u0639\u0629 \u0627\u0644\u062a\u0646\u0628\u0624 \u0628\u0627\u0644\u062c\u0645\u0644\u0629/\u0627\u0644\u0630\u0631\u064a\u0629 \u0644\u0645\u062e\u062a\u0644\u0641 \u0627\u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0627\u062a\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configuration = {\n    \"n_train\": int(1e3),\n    \"n_test\": int(1e2),\n    \"n_features\": int(1e2),\n    \"estimators\": [\n        {\n            \"name\": \"Linear Model\",\n            \"instance\": SGDRegressor(\n                penalty=\"elasticnet\", alpha=0.01, l1_ratio=0.25, tol=1e-4\n            ),\n            \"complexity_label\": \"non-zero coefficients\",\n            \"complexity_computer\": lambda clf: np.count_nonzero(clf.coef_),\n        },\n        {\n            \"name\": \"RandomForest\",\n            \"instance\": RandomForestRegressor(),\n            \"complexity_label\": \"estimators\",\n            \"complexity_computer\": lambda clf: clf.n_estimators,\n        },\n        {\n            \"name\": \"SVR\",\n            \"instance\": SVR(kernel=\"rbf\"),\n            \"complexity_label\": \"support vectors\",\n            \"complexity_computer\": lambda clf: len(clf.support_vectors_),\n        },\n    ],\n}\nbenchmark(configuration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0642\u064a\u0627\u0633 \u062a\u0623\u062b\u064a\u0631 n_features \u0639\u0644\u0649 \u0633\u0631\u0639\u0629 \u0627\u0644\u062a\u0646\u0628\u0624\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "percentile = 90\npercentiles = n_feature_influence(\n    {\"ridge\": Ridge()},\n    configuration[\"n_train\"],\n    configuration[\"n_test\"],\n    [100, 250, 500],\n    percentile,\n)\nplot_n_features_influence(percentiles, percentile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0642\u064a\u0627\u0633 \u0627\u0644\u0625\u0646\u062a\u0627\u062c\u064a\u0629\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "throughputs = benchmark_throughputs(configuration)\nplot_benchmark_throughput(throughputs, configuration)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}