{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification\n\n\u0647\u0630\u0627 \u0627\u0644\u0645\u062b\u0627\u0644 \u064a\u0648\u0636\u062d \u0643\u064a\u0641 \u0623\u0646 \u0645\u0642\u062f\u0631\u0627\u062a Ledoit-Wolf \u0648Oracle Approximating\nShrinkage (OAS) \u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u062a\u0628\u0627\u064a\u0646 \u064a\u0645\u0643\u0646 \u0623\u0646 \u062a\u062d\u0633\u0646 \u0627\u0644\u062a\u0635\u0646\u064a\u0641.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u0627\u0644\u0645\u0624\u0644\u0641\u0648\u0646: \u0645\u0637\u0648\u0631\u064a scikit-learn\n# \u0645\u0639\u0631\u0641 \u0627\u0644\u062a\u0631\u062e\u064a\u0635: BSD-3-Clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.covariance import OAS\nfrom sklearn.datasets import make_blobs\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nn_train = 20  # \u0639\u062f\u062f \u0627\u0644\u0639\u064a\u0646\u0627\u062a \u0644\u0644\u062a\u062f\u0631\u064a\u0628\nn_test = 200  # \u0639\u062f\u062f \u0627\u0644\u0639\u064a\u0646\u0627\u062a \u0644\u0644\u0627\u062e\u062a\u0628\u0627\u0631\nn_averages = 50  # \u0639\u062f\u062f \u0645\u0631\u0627\u062a \u062a\u0643\u0631\u0627\u0631 \u0627\u0644\u062a\u0635\u0646\u064a\u0641\nn_features_max = 75  # \u0627\u0644\u062d\u062f \u0627\u0644\u0623\u0642\u0635\u0649 \u0644\u0639\u062f\u062f \u0627\u0644\u0645\u064a\u0632\u0627\u062a\nstep = 4  # \u062d\u062c\u0645 \u0627\u0644\u062e\u0637\u0648\u0629 \u0644\u0644\u062d\u0633\u0627\u0628\n\n\ndef generate_data(n_samples, n_features):\n    \"\"\"\u062a\u0648\u0644\u064a\u062f \u0628\u064a\u0627\u0646\u0627\u062a \u0639\u0634\u0648\u0627\u0626\u064a\u0629 \u0634\u0628\u064a\u0647\u0629 \u0628\u0627\u0644\u0643\u0631\u0627\u062a \u0645\u0639 \u0645\u064a\u0632\u0627\u062a \u0636\u0648\u0636\u0627\u0626\u064a\u0629.\n\n    \u0647\u0630\u0647 \u0627\u0644\u062f\u0627\u0644\u0629 \u062a\u0639\u064a\u062f \u0645\u0635\u0641\u0648\u0641\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0625\u062f\u062e\u0627\u0644 \u0628\u0634\u0643\u0644 `(n_samples, n_features)`\n    \u0648\u0645\u0635\u0641\u0648\u0641\u0629 \u0644\u0639\u0644\u0627\u0645\u0627\u062a \u0627\u0644\u062a\u0635\u0646\u064a\u0641 `n_samples`.\n\n    \u0645\u064a\u0632\u0629 \u0648\u0627\u062d\u062f\u0629 \u0641\u0642\u0637 \u062a\u062d\u062a\u0648\u064a \u0639\u0644\u0649 \u0645\u0639\u0644\u0648\u0645\u0627\u062a \u062a\u0645\u064a\u064a\u0632\u064a\u0629\u060c \u0648\u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u0623\u062e\u0631\u0649\n    \u062a\u062d\u062a\u0648\u064a \u0641\u0642\u0637 \u0639\u0644\u0649 \u0636\u0648\u0636\u0627\u0621.\n    \"\"\"\n    X, y = make_blobs(n_samples=n_samples, n_features=1, centers=[[-2], [2]])\n\n    # \u0625\u0636\u0627\u0641\u0629 \u0645\u064a\u0632\u0627\u062a \u063a\u064a\u0631 \u062a\u0645\u064a\u064a\u0632\u064a\u0629\n    if n_features > 1:\n        X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])\n    return X, y\n\n\nacc_clf1, acc_clf2, acc_clf3 = [], [], []\nn_features_range = range(1, n_features_max + 1, step)\nfor n_features in n_features_range:\n    score_clf1, score_clf2, score_clf3 = 0, 0, 0\n    for _ in range(n_averages):\n        X, y = generate_data(n_train, n_features)\n\n        clf1 = LinearDiscriminantAnalysis(\n            solver=\"lsqr\", shrinkage=None).fit(X, y)\n        clf2 = LinearDiscriminantAnalysis(\n            solver=\"lsqr\", shrinkage=\"auto\").fit(X, y)\n        oa = OAS(store_precision=False, assume_centered=False)\n        clf3 = LinearDiscriminantAnalysis(solver=\"lsqr\", covariance_estimator=oa).fit(\n            X, y\n        )\n\n        X, y = generate_data(n_test, n_features)\n        score_clf1 += clf1.score(X, y)\n        score_clf2 += clf2.score(X, y)\n        score_clf3 += clf3.score(X, y)\n\n    acc_clf1.append(score_clf1 / n_averages)\n    acc_clf2.append(score_clf2 / n_averages)\n    acc_clf3.append(score_clf3 / n_averages)\n\nfeatures_samples_ratio = np.array(n_features_range) / n_train\n\nplt.plot(\n    features_samples_ratio,\n    acc_clf1,\n    linewidth=2,\n    label=\"LDA\",\n    color=\"gold\",\n    linestyle=\"solid\",\n)\nplt.plot(\n    features_samples_ratio,\n    acc_clf2,\n    linewidth=2,\n    label=\"LDA with Ledoit Wolf\",\n    color=\"navy\",\n    linestyle=\"dashed\",\n)\nplt.plot(\n    features_samples_ratio,\n    acc_clf3,\n    linewidth=2,\n    label=\"LDA with OAS\",\n    color=\"red\",\n    linestyle=\"dotted\",\n)\n\nplt.xlabel(\"n_features / n_samples\")\nplt.ylabel(\"Classification accuracy\")\n\nplt.legend(loc=\"lower left\")\nplt.ylim((0.65, 1.0))\nplt.suptitle(\n    \"LDA (Linear Discriminant Analysis) vs. \"\n    + \"\\n\"\n    + \"LDA with Ledoit Wolf vs. \"\n    + \"\\n\"\n    + \"LDA with OAS (1 discriminative feature)\"\n)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}