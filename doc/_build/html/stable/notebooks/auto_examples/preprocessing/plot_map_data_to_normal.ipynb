{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0625\u0644\u0649 \u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u0627\u0644\u0637\u0628\u064a\u0639\u064a\n\n.. currentmodule:: sklearn.preprocessing\n\n\u0647\u0630\u0627 \u0627\u0644\u0645\u062b\u0627\u0644 \u064a\u0648\u0636\u062d \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u062a\u062d\u0648\u064a\u0644\u0627\u062a Box-Cox \u0648Yeo-Johnson\n\u0645\u0646 \u062e\u0644\u0627\u0644 :class:`~PowerTransformer` \u0644\u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0645\u0646 \u062a\u0648\u0632\u064a\u0639\u0627\u062a \u0645\u062e\u062a\u0644\u0641\u0629\n\u0625\u0644\u0649 \u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u0627\u0644\u0637\u0628\u064a\u0639\u064a.\n\n\u0627\u0644\u062a\u062d\u0648\u064a\u0644 \u0628\u0627\u0644\u0642\u0648\u0629 \u0645\u0641\u064a\u062f \u0643\u062a\u062d\u0648\u064a\u0644 \u0641\u064a \u0645\u0634\u0627\u0643\u0644 \u0627\u0644\u0646\u0645\u0630\u062c\u0629 \u062d\u064a\u062b\n\u064a\u062a\u0645 \u0627\u0644\u0631\u063a\u0628\u0629 \u0641\u064a \u0627\u0644\u062a\u062c\u0627\u0646\u0633 \u0648\u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u0627\u0644\u0637\u0628\u064a\u0639\u064a. \u0641\u064a\u0645\u0627 \u064a\u0644\u064a \u0623\u0645\u062b\u0644\u0629 \u0639\u0644\u0649 \u062a\u0637\u0628\u064a\u0642 Box-Cox \u0648\nYeo-Johnwon \u0639\u0644\u0649 \u0633\u062a\u0629 \u062a\u0648\u0632\u064a\u0639\u0627\u062a \u0627\u062d\u062a\u0645\u0627\u0644\u064a\u0629 \u0645\u062e\u062a\u0644\u0641\u0629: \u0627\u0644\u0644\u0648\u063a\u0627\u0631\u064a\u062a\u0645\u064a \u0627\u0644\u0637\u0628\u064a\u0639\u064a\u060c\n\u0645\u0631\u0628\u0639 \u0643\u0627\u064a\u060c \u0648\u064a\u0628\u0644\u060c \u0627\u0644\u062c\u0627\u0648\u0633\u064a\u060c \u0627\u0644\u0645\u0648\u062d\u062f\u060c \u062b\u0646\u0627\u0626\u064a \u0627\u0644\u0646\u0645\u0637.\n\n\u0645\u0644\u0627\u062d\u0638\u0629 \u0623\u0646 \u0627\u0644\u062a\u062d\u0648\u0644\u0627\u062a \u062a\u0646\u062c\u062d \u0641\u064a \u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0625\u0644\u0649 \u062a\u0648\u0632\u064a\u0639 \u0637\u0628\u064a\u0639\u064a\n\u0639\u0646\u062f \u062a\u0637\u0628\u064a\u0642\u0647\u0627 \u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0628\u064a\u0627\u0646\u0627\u062a \u0645\u0639\u064a\u0646\u0629\u060c \u0648\u0644\u0643\u0646\u0647\u0627 \u063a\u064a\u0631 \u0641\u0639\u0627\u0644\u0629 \u0645\u0639 \u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0623\u062e\u0631\u0649.\n\u0647\u0630\u0627 \u064a\u0633\u0644\u0637 \u0627\u0644\u0636\u0648\u0621 \u0639\u0644\u0649 \u0623\u0647\u0645\u064a\u0629 \u062a\u0635\u0648\u0631 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0642\u0628\u0644 \u0648\u0628\u0639\u062f\n\u0627\u0644\u062a\u062d\u0648\u064a\u0644.\n\n\u0644\u0627\u062d\u0638 \u0623\u064a\u0636\u064b\u0627 \u0623\u0646\u0647 \u0639\u0644\u0649 \u0627\u0644\u0631\u063a\u0645 \u0645\u0646 \u0623\u0646 Box-Cox \u064a\u0628\u062f\u0648 \u0623\u0646\u0647 \u064a\u0624\u062f\u064a \u0628\u0634\u0643\u0644 \u0623\u0641\u0636\u0644 \u0645\u0646 Yeo-Johnson \u0644\u0644\u062a\u0648\u0632\u064a\u0639\u0627\u062a \u0627\u0644\u0644\u0648\u063a\u0627\u0631\u064a\u062a\u0645\u064a\u0629 \u0627\u0644\u0637\u0628\u064a\u0639\u064a\u0629 \u0648\u0645\u0631\u0628\u0639\u0629 \u0643\u0627\u064a\u060c \u0625\u0644\u0627 \u0623\u0646\u0647 \u064a\u062c\u0628 \u0645\u0631\u0627\u0639\u0627\u0629 \u0623\u0646 Box-Cox \u0644\u0627\n\u064a\u062f\u0639\u0645 \u0627\u0644\u0645\u062f\u062e\u0644\u0627\u062a \u0630\u0627\u062a \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0633\u0627\u0644\u0628\u0629.\n\n\u0644\u0645\u0642\u0627\u0631\u0646\u0629\u060c \u0646\u0636\u064a\u0641 \u0623\u064a\u0636\u064b\u0627 \u0627\u0644\u0646\u0627\u062a\u062c \u0645\u0646\n:class:`~QuantileTransformer`. \u064a\u0645\u0643\u0646\u0647 \u0641\u0631\u0636 \u0623\u064a \u062a\u0648\u0632\u064a\u0639 \u062a\u0639\u0633\u0641\u064a\n\u0625\u0644\u0649 \u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u0627\u0644\u062c\u0627\u0648\u0633\u064a\u060c \u0628\u0634\u0631\u0637 \u0623\u0646 \u064a\u0643\u0648\u0646 \u0647\u0646\u0627\u0643 \u0645\u0627 \u064a\u0643\u0641\u064a \u0645\u0646 \u0639\u064a\u0646\u0627\u062a \u0627\u0644\u062a\u062f\u0631\u064a\u0628\n(\u0627\u0644\u0622\u0644\u0627\u0641). \u0644\u0623\u0646\u0647 \u0637\u0631\u064a\u0642\u0629 \u063a\u064a\u0631 \u0645\u0639\u0644\u0645\u064a\u0629\u060c \u0645\u0646 \u0627\u0644\u0635\u0639\u0628 \u062a\u0641\u0633\u064a\u0631\u0647\u0627\n\u0623\u0643\u062b\u0631 \u0645\u0646 \u0627\u0644\u0637\u0631\u0642 \u0627\u0644\u0645\u0639\u0644\u0645\u064a\u0629 (Box-Cox \u0648Yeo-Johnson).\n\n\u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \"\u0627\u0644\u0635\u063a\u064a\u0631\u0629\" (\u0623\u0642\u0644 \u0645\u0646 \u0628\u0636\u0639 \u0645\u0626\u0627\u062a \u0645\u0646 \u0627\u0644\u0646\u0642\u0627\u0637)\u060c \u0645\u062d\u0648\u0644 \u0627\u0644\u0643\u0648\u0627\u0646\u062a\u064a\u0644\n\u0639\u0631\u0636\u0629 \u0644\u062e\u0637\u0631 \u0627\u0644\u0625\u0641\u0631\u0627\u0637 \u0641\u064a \u0627\u0644\u062a\u062d\u062f\u064a\u062f. \u064a\u0648\u0635\u0649 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u062a\u062d\u0648\u064a\u0644 \u0628\u0627\u0644\u0642\u0648\u0629.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u0627\u0644\u0645\u0624\u0644\u0641\u0648\u0646: \u0645\u0637\u0648\u0631\u064a scikit-learn\n# \u0645\u0639\u0631\u0641 \u0627\u0644\u062a\u0631\u062e\u064a\u0635: BSD-3-Clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer\n\nN_SAMPLES = 1000\nFONT_SIZE = 6\nBINS = 30\n\n\nrng = np.random.RandomState(304)\nbc = PowerTransformer(method=\"box-cox\")\nyj = PowerTransformer(method=\"yeo-johnson\")\n# n_quantiles is set to the training set size rather than the default value\n# to avoid a warning being raised by this example\nqt = QuantileTransformer(\n    n_quantiles=500, output_distribution=\"normal\", random_state=rng\n)\nsize = (N_SAMPLES, 1)\n\n\n# \u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u0627\u0644\u0644\u0648\u063a\u0627\u0631\u064a\u062a\u0645\u064a \u0627\u0644\u0637\u0628\u064a\u0639\u064a\nX_lognormal = rng.lognormal(size=size)\n\n# \u062a\u0648\u0632\u064a\u0639 \u0645\u0631\u0628\u0639 \u0643\u0627\u064a\ndf = 3\nX_chisq = rng.chisquare(df=df, size=size)\n\n# \u062a\u0648\u0632\u064a\u0639 \u0648\u064a\u0628\u0644\na = 50\nX_weibull = rng.weibull(a=a, size=size)\n\n# \u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u0627\u0644\u062c\u0627\u0648\u0633\u064a\nloc = 100\nX_gaussian = rng.normal(loc=loc, size=size)\n\n# \u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u0627\u0644\u0645\u0648\u062d\u062f\nX_uniform = rng.uniform(low=0, high=1, size=size)\n\n# \u0627\u0644\u062a\u0648\u0632\u064a\u0639 \u062b\u0646\u0627\u0626\u064a \u0627\u0644\u0646\u0645\u0637\nloc_a, loc_b = 100, 105\nX_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\nX_bimodal = np.concatenate([X_a, X_b], axis=0)\n\n\n# \u0625\u0646\u0634\u0627\u0621 \u0627\u0644\u0631\u0633\u0648\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a\u0629\ndistributions = [\n    (\"Lognormal\", X_lognormal),\n    (\"Chi-squared\", X_chisq),\n    (\"Weibull\", X_weibull),\n    (\"Gaussian\", X_gaussian),\n    (\"Uniform\", X_uniform),\n    (\"Bimodal\", X_bimodal),\n]\n\ncolors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n\nfig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(2))\naxes = axes.flatten()\naxes_idxs = [\n    (0, 3, 6, 9),\n    (1, 4, 7, 10),\n    (2, 5, 8, 11),\n    (12, 15, 18, 21),\n    (13, 16, 19, 22),\n    (14, 17, 20, 23),\n]\naxes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\n\n\nfor distribution, color, axes in zip(distributions, colors, axes_list):\n    name, X = distribution\n    X_train, X_test = train_test_split(X, test_size=0.5)\n\n    # \u0625\u062c\u0631\u0627\u0621 \u062a\u062d\u0648\u064a\u0644\u0627\u062a \u0627\u0644\u0642\u0648\u0629 \u0648\u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0643\u0648\u0627\u0646\u062a\u064a\u0644\n    X_trans_bc = bc.fit(X_train).transform(X_test)\n    lmbda_bc = round(bc.lambdas_[0], 2)\n    X_trans_yj = yj.fit(X_train).transform(X_test)\n    lmbda_yj = round(yj.lambdas_[0], 2)\n    X_trans_qt = qt.fit(X_train).transform(X_test)\n\n    ax_original, ax_bc, ax_yj, ax_qt = axes\n\n    ax_original.hist(X_train, color=color, bins=BINS)\n    ax_original.set_title(name, fontsize=FONT_SIZE)\n    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n\n    for ax, X_trans, meth_name, lmbda in zip(\n        (ax_bc, ax_yj, ax_qt),\n        (X_trans_bc, X_trans_yj, X_trans_qt),\n        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n        (lmbda_bc, lmbda_yj, None),\n    ):\n        ax.hist(X_trans, color=color, bins=BINS)\n        title = \"After {}\".format(meth_name)\n        if lmbda is not None:\n            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n        ax.set_title(title, fontsize=FONT_SIZE)\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n        ax.set_xlim([-3.5, 3.5])\n\n\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}