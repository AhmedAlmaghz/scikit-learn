
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/ensemble/plot_gradient_boosting_quantile.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_ensemble_plot_gradient_boosting_quantile.py>`
        to download the full example code. or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py:


=====================================================
فترات التنبؤ لانحدار التعزيز المتدرج
=====================================================

يوضح هذا المثال كيفية استخدام انحدار الكميات لإنشاء فترات تنبؤ. انظر :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`
لمثال يعرض بعض الميزات الأخرى لـ
:class:`~ensemble.HistGradientBoostingRegressor`.

.. GENERATED FROM PYTHON SOURCE LINES 11-15

.. code-block:: Python


    # Authors: The scikit-learn developers
    # SPDX-License-Identifier: BSD-3-Clause








.. GENERATED FROM PYTHON SOURCE LINES 16-18

إنشاء بعض البيانات لمشكلة انحدار اصطناعية عن طريق تطبيق
الدالة f على مدخلات عشوائية موزعة بشكل منتظم.

.. GENERATED FROM PYTHON SOURCE LINES 18-40

.. code-block:: Python

    from sklearn.base import clone
    from pprint import pprint
    from sklearn.metrics import make_scorer
    from sklearn.model_selection import HalvingRandomSearchCV
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.metrics import mean_pinball_loss, mean_squared_error
    from sklearn.ensemble import GradientBoostingRegressor
    import numpy as np

    from sklearn.model_selection import train_test_split


    def f(x):
        """الدالة المراد التنبؤ بها."""
        return x * np.sin(x)


    rng = np.random.RandomState(42)
    X = np.atleast_2d(rng.uniform(0, 10.0, size=1000)).T
    expected_y = f(X).ravel()








.. GENERATED FROM PYTHON SOURCE LINES 41-50

لجعل المشكلة مثيرة للاهتمام، نقوم بإنشاء ملاحظات للهدف y على أنها
مجموع حد حتمي محسوب بواسطة الدالة f وحد ضوضاء عشوائي
يتبع `توزيع لوغاريتمي عادي
<https://en.wikipedia.org/wiki/Log-normal_distribution>`_ متمركز. لجعل هذا أكثر
إثارة للاهتمام، نأخذ في الاعتبار الحالة التي يعتمد فيها اتساع الضوضاء
على المتغير المدخل x (ضوضاء غير متجانسة).

التوزيع اللوغاريتمي العادي غير متماثل وذو ذيل طويل: من المحتمل ملاحظة قيم متطرفة كبيرة
ولكن من المستحيل ملاحظة قيم متطرفة صغيرة.

.. GENERATED FROM PYTHON SOURCE LINES 50-54

.. code-block:: Python

    sigma = 0.5 + X.ravel() / 10
    noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)
    y = expected_y + noise








.. GENERATED FROM PYTHON SOURCE LINES 55-56

تقسيم البيانات إلى مجموعات تدريب واختبار:

.. GENERATED FROM PYTHON SOURCE LINES 56-58

.. code-block:: Python

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)








.. GENERATED FROM PYTHON SOURCE LINES 59-71

ملاءمة منحنيات انحدار كمية وغير خطية للمربعات الصغرى
--------------------------------------------------------

ملاءمة نماذج التعزيز المتدرج المدربة مع خسارة الكمية
و alpha=0.05، 0.5، 0.95.

النماذج التي تم الحصول عليها لـ alpha=0.05 و alpha=0.95 تنتج فاصل ثقة 90%
(95% - 5% = 90%).

النموذج المدرب بـ alpha=0.5 ينتج انحدارًا للوسيط: في
المتوسط، يجب أن يكون هناك نفس عدد ملاحظات الهدف فوق وتحت
القيم المتوقعة.

.. GENERATED FROM PYTHON SOURCE LINES 71-85

.. code-block:: Python


    all_models = {}
    common_params = dict(
        learning_rate=0.05,
        n_estimators=200,
        max_depth=2,
        min_samples_leaf=9,
        min_samples_split=9,
    )
    for alpha in [0.05, 0.5, 0.95]:
        gbr = GradientBoostingRegressor(
            loss="quantile", alpha=alpha, **common_params)
        all_models["q %1.2f" % alpha] = gbr.fit(X_train, y_train)








.. GENERATED FROM PYTHON SOURCE LINES 86-93

لاحظ أن :class:`~sklearn.ensemble.HistGradientBoostingRegressor` أسرع بكثير من
:class:`~sklearn.ensemble.GradientBoostingRegressor` بدءًا من
مجموعات البيانات المتوسطة (`n_samples >= 10_000`)، وهي ليست حالة
المثال الحالي.

من أجل المقارنة، نقوم أيضًا بملاءمة نموذج أساسي مدرب باستخدام
متوسط ​​مربع الخطأ (MSE) المعتاد.

.. GENERATED FROM PYTHON SOURCE LINES 93-96

.. code-block:: Python

    gbr_ls = GradientBoostingRegressor(loss="squared_error", **common_params)
    all_models["mse"] = gbr_ls.fit(X_train, y_train)








.. GENERATED FROM PYTHON SOURCE LINES 97-98

إنشاء مجموعة تقييم متباعدة بشكل متساوٍ من قيم الإدخال التي تغطي النطاق [0، 10].

.. GENERATED FROM PYTHON SOURCE LINES 98-100

.. code-block:: Python

    xx = np.atleast_2d(np.linspace(0, 10, 1000)).T








.. GENERATED FROM PYTHON SOURCE LINES 101-104

ارسم دالة المتوسط ​​الشرطي الحقيقي f، تنبؤات المتوسط
الشرطي (الخسارة تساوي مربع الخطأ)، الوسيط الشرطي وفترة 90% الشرطية
(من المئين الشرطي الخامس إلى 95).

.. GENERATED FROM PYTHON SOURCE LINES 104-127

.. code-block:: Python


    y_pred = all_models["mse"].predict(xx)
    y_lower = all_models["q 0.05"].predict(xx)
    y_upper = all_models["q 0.95"].predict(xx)
    y_med = all_models["q 0.50"].predict(xx)

    fig = plt.figure(figsize=(10, 10))
    plt.plot(xx, f(xx), "g:", linewidth=3, label=r"$f(x) = x\,\sin(x)$")
    plt.plot(X_test, y_test, "b.", markersize=10, label="ملاحظات الاختبار")
    plt.plot(xx, y_med, "r-", label="الوسيط المتوقع")
    plt.plot(xx, y_pred, "r-", label="المتوسط المتوقع")
    plt.plot(xx, y_upper, "k-")
    plt.plot(xx, y_lower, "k-")
    plt.fill_between(
        xx.ravel(), y_lower, y_upper, alpha=0.4, label="الفترة المتوقعة 90%"
    )
    plt.xlabel("$x$")
    plt.ylabel("$f(x)$")
    plt.ylim(-10, 25)
    plt.legend(loc="upper left")
    plt.show()





.. image-sg:: /auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_quantile_001.png
   :alt: plot gradient boosting quantile
   :srcset: /auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_quantile_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 128-143

بمقارنة الوسيط المتوقع بالمتوسط ​​المتوقع، نلاحظ أن الوسيط
أقل من المتوسط ​​في المتوسط ​​لأن الضوضاء منحرفة نحو القيم
العالية (القيم المتطرفة الكبيرة). يبدو أيضًا أن تقدير الوسيط أكثر سلاسة
نظرًا لمتانته الطبيعية للقيم المتطرفة.

لاحظ أيضًا أن التحيز الاستقرائي لأشجار التعزيز المتدرج
يمنع للأسف كمية 0.05 الخاصة بنا من التقاط الشكل الجيبي
للإشارة بشكل كامل، خاصة حول x = 8. يمكن لضبط المعلمات الفائقة
تقليل هذا التأثير كما هو موضح في الجزء الأخير من هذا الدفتر.

تحليل مقاييس الخطأ
-----------------------------

قياس النماذج باستخدام :func:`~sklearn.metrics.mean_squared_error`
و :func:`~sklearn.metrics.mean_pinball_loss` على بيانات التدريب.

.. GENERATED FROM PYTHON SOURCE LINES 143-162

.. code-block:: Python



    def highlight_min(x):
        x_min = x.min()
        return ["font-weight: bold" if v == x_min else "" for v in x]


    results = []
    for name, gbr in sorted(all_models.items()):
        metrics = {"model": name}
        y_pred = gbr.predict(X_train)
        for alpha in [0.05, 0.5, 0.95]:
            metrics["pbl=%1.2f" % alpha] = mean_pinball_loss(
                y_train, y_pred, alpha=alpha)
        metrics["MSE"] = mean_squared_error(y_train, y_pred)
        results.append(metrics)

    pd.DataFrame(results).set_index("model").style.apply(highlight_min)






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    #T_8d7e7_row0_col3, #T_8d7e7_row1_col0, #T_8d7e7_row2_col1, #T_8d7e7_row3_col2 {
      font-weight: bold;
    }
    </style>
    <table id="T_8d7e7">
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th id="T_8d7e7_level0_col0" class="col_heading level0 col0" >pbl=0.05</th>
          <th id="T_8d7e7_level0_col1" class="col_heading level0 col1" >pbl=0.50</th>
          <th id="T_8d7e7_level0_col2" class="col_heading level0 col2" >pbl=0.95</th>
          <th id="T_8d7e7_level0_col3" class="col_heading level0 col3" >MSE</th>
        </tr>
        <tr>
          <th class="index_name level0" >model</th>
          <th class="blank col0" >&nbsp;</th>
          <th class="blank col1" >&nbsp;</th>
          <th class="blank col2" >&nbsp;</th>
          <th class="blank col3" >&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_8d7e7_level0_row0" class="row_heading level0 row0" >mse</th>
          <td id="T_8d7e7_row0_col0" class="data row0 col0" >0.715413</td>
          <td id="T_8d7e7_row0_col1" class="data row0 col1" >0.715413</td>
          <td id="T_8d7e7_row0_col2" class="data row0 col2" >0.715413</td>
          <td id="T_8d7e7_row0_col3" class="data row0 col3" >7.750348</td>
        </tr>
        <tr>
          <th id="T_8d7e7_level0_row1" class="row_heading level0 row1" >q 0.05</th>
          <td id="T_8d7e7_row1_col0" class="data row1 col0" >0.127128</td>
          <td id="T_8d7e7_row1_col1" class="data row1 col1" >1.253445</td>
          <td id="T_8d7e7_row1_col2" class="data row1 col2" >2.379763</td>
          <td id="T_8d7e7_row1_col3" class="data row1 col3" >18.933253</td>
        </tr>
        <tr>
          <th id="T_8d7e7_level0_row2" class="row_heading level0 row2" >q 0.50</th>
          <td id="T_8d7e7_row2_col0" class="data row2 col0" >0.305438</td>
          <td id="T_8d7e7_row2_col1" class="data row2 col1" >0.622811</td>
          <td id="T_8d7e7_row2_col2" class="data row2 col2" >0.940184</td>
          <td id="T_8d7e7_row2_col3" class="data row2 col3" >9.827917</td>
        </tr>
        <tr>
          <th id="T_8d7e7_level0_row3" class="row_heading level0 row3" >q 0.95</th>
          <td id="T_8d7e7_row3_col0" class="data row3 col0" >3.909909</td>
          <td id="T_8d7e7_row3_col1" class="data row3 col1" >2.145957</td>
          <td id="T_8d7e7_row3_col2" class="data row3 col2" >0.382005</td>
          <td id="T_8d7e7_row3_col3" class="data row3 col3" >28.667219</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 163-177

يعرض عمود واحد جميع النماذج التي تم تقييمها بواسطة نفس المقياس. يجب الحصول على الحد الأدنى للعدد
في عمود عندما يتم تدريب النموذج وقياسه
بنفس المقياس. يجب أن يكون هذا هو الحال دائمًا في مجموعة التدريب إذا تقارب
التدريب.

لاحظ أنه نظرًا لأن توزيع الهدف غير متماثل، فإن المتوسط ​​الشرطي
المتوقع والوسيط الشرطي يختلفان اختلافًا كبيرًا، وبالتالي لا يمكن للمرء استخدام نموذج مربع الخطأ للحصول على تقدير جيد
للوسيط الشرطي أو العكس.

إذا كان توزيع الهدف متماثلًا ولم يكن به قيم متطرفة (على سبيل المثال مع
ضوضاء غاوسية)، فإن مقدر الوسيط ومقدر المربعات الصغرى
سينتجان تنبؤات متشابهة.

ثم نفعل الشيء نفسه في مجموعة الاختبار.

.. GENERATED FROM PYTHON SOURCE LINES 177-190

.. code-block:: Python

    results = []
    for name, gbr in sorted(all_models.items()):
        metrics = {"model": name}
        y_pred = gbr.predict(X_test)
        for alpha in [0.05, 0.5, 0.95]:
            metrics["pbl=%1.2f" % alpha] = mean_pinball_loss(
                y_test, y_pred, alpha=alpha)
        metrics["MSE"] = mean_squared_error(y_test, y_pred)
        results.append(metrics)

    pd.DataFrame(results).set_index("model").style.apply(highlight_min)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    #T_17d8f_row1_col0, #T_17d8f_row2_col1, #T_17d8f_row2_col3, #T_17d8f_row3_col2 {
      font-weight: bold;
    }
    </style>
    <table id="T_17d8f">
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th id="T_17d8f_level0_col0" class="col_heading level0 col0" >pbl=0.05</th>
          <th id="T_17d8f_level0_col1" class="col_heading level0 col1" >pbl=0.50</th>
          <th id="T_17d8f_level0_col2" class="col_heading level0 col2" >pbl=0.95</th>
          <th id="T_17d8f_level0_col3" class="col_heading level0 col3" >MSE</th>
        </tr>
        <tr>
          <th class="index_name level0" >model</th>
          <th class="blank col0" >&nbsp;</th>
          <th class="blank col1" >&nbsp;</th>
          <th class="blank col2" >&nbsp;</th>
          <th class="blank col3" >&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_17d8f_level0_row0" class="row_heading level0 row0" >mse</th>
          <td id="T_17d8f_row0_col0" class="data row0 col0" >0.917281</td>
          <td id="T_17d8f_row0_col1" class="data row0 col1" >0.767498</td>
          <td id="T_17d8f_row0_col2" class="data row0 col2" >0.617715</td>
          <td id="T_17d8f_row0_col3" class="data row0 col3" >6.692901</td>
        </tr>
        <tr>
          <th id="T_17d8f_level0_row1" class="row_heading level0 row1" >q 0.05</th>
          <td id="T_17d8f_row1_col0" class="data row1 col0" >0.144204</td>
          <td id="T_17d8f_row1_col1" class="data row1 col1" >1.245961</td>
          <td id="T_17d8f_row1_col2" class="data row1 col2" >2.347717</td>
          <td id="T_17d8f_row1_col3" class="data row1 col3" >15.648026</td>
        </tr>
        <tr>
          <th id="T_17d8f_level0_row2" class="row_heading level0 row2" >q 0.50</th>
          <td id="T_17d8f_row2_col0" class="data row2 col0" >0.412021</td>
          <td id="T_17d8f_row2_col1" class="data row2 col1" >0.607752</td>
          <td id="T_17d8f_row2_col2" class="data row2 col2" >0.803483</td>
          <td id="T_17d8f_row2_col3" class="data row2 col3" >5.874771</td>
        </tr>
        <tr>
          <th id="T_17d8f_level0_row3" class="row_heading level0 row3" >q 0.95</th>
          <td id="T_17d8f_row3_col0" class="data row3 col0" >4.354394</td>
          <td id="T_17d8f_row3_col1" class="data row3 col1" >2.355445</td>
          <td id="T_17d8f_row3_col2" class="data row3 col2" >0.356497</td>
          <td id="T_17d8f_row3_col3" class="data row3 col3" >34.852774</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 191-213

الأخطاء أعلى مما يعني أن النماذج قد تجاوزت البيانات قليلاً. لا يزال
يُظهر أن أفضل مقياس اختبار يتم الحصول عليه عندما يتم تدريب النموذج عن طريق
تقليل نفس المقياس.

لاحظ أن مقدر الوسيط الشرطي يتنافس مع مقدر مربع
الخطأ من حيث MSE في مجموعة الاختبار: يمكن تفسير ذلك من خلال
حقيقة أن مقدر مربع الخطأ حساس للغاية للقيم المتطرفة الكبيرة
والتي يمكن أن تتسبب في تجاوز كبير. يمكن ملاحظة ذلك على الجانب الأيمن
من الرسم البياني السابق. مقدر الوسيط الشرطي متحيز
(تقليل التقدير لهذه الضوضاء غير المتماثلة) ولكنه أيضًا قوي بشكل طبيعي
للقيم المتطرفة ولا يتجاوزها.

.. _calibration-section:

معايرة فاصل الثقة
--------------------------------------

يمكننا أيضًا تقييم قدرة مقدري الكميات المتطرفين على
إنتاج فاصل ثقة شرطي معاير جيدًا بنسبة 90%.

للقيام بذلك، يمكننا حساب جزء الملاحظات التي تقع بين
التنبؤات:

.. GENERATED FROM PYTHON SOURCE LINES 213-223

.. code-block:: Python

    def coverage_fraction(y, y_low, y_high):
        return np.mean(np.logical_and(y >= y_low, y <= y_high))


    coverage_fraction(
        y_train,
        all_models["q 0.05"].predict(X_train),
        all_models["q 0.95"].predict(X_train),
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    np.float64(0.9)



.. GENERATED FROM PYTHON SOURCE LINES 224-226

في مجموعة التدريب، تكون المعايرة قريبة جدًا من قيمة التغطية
المتوقعة لفاصل ثقة 90%.

.. GENERATED FROM PYTHON SOURCE LINES 226-232

.. code-block:: Python

    coverage_fraction(
        y_test, all_models["q 0.05"].predict(
            X_test), all_models["q 0.95"].predict(X_test)
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    np.float64(0.868)



.. GENERATED FROM PYTHON SOURCE LINES 233-250

في مجموعة الاختبار، يكون فاصل الثقة المقدر ضيقًا جدًا.
لاحظ، مع ذلك، أننا سنحتاج إلى تضمين هذه المقاييس في حلقة تحقق متقاطع
لتقييم تقلبها في ظل إعادة أخذ عينات البيانات.

ضبط المعلمات الفائقة لمنحنيات انحدار الكمية
------------------------------------------------------

في الرسم البياني أعلاه، لاحظنا أن منحنى انحدار المئين الخامس يبدو أنه
غير مناسب ولا يمكنه التكيف مع الشكل الجيبي للإشارة.

تم ضبط المعلمات الفائقة للنموذج يدويًا تقريبًا لمنحنى انحدار
الوسيط، وليس هناك سبب يدعو إلى أن تكون المعلمات الفائقة نفسها
مناسبة لمنحنى انحدار المئين الخامس.

لتأكيد هذه الفرضية، نقوم بضبط المعلمات الفائقة لمنحنى انحدار جديد
للمئين الخامس عن طريق تحديد أفضل معلمات النموذج عن طريق
التحقق المتقاطع من خسارة الكرة والدبوس مع alpha = 0.05:

.. GENERATED FROM PYTHON SOURCE LINES 252-279

.. code-block:: Python

    from sklearn.experimental import enable_halving_search_cv  # noqa

    param_grid = dict(
        learning_rate=[0.05, 0.1, 0.2],
        max_depth=[2, 5, 10],
        min_samples_leaf=[1, 5, 10, 20],
        min_samples_split=[5, 10, 20, 30, 50],
    )
    alpha = 0.05
    neg_mean_pinball_loss_05p_scorer = make_scorer(
        mean_pinball_loss,
        alpha=alpha,
        greater_is_better=False,  # تعظيم الخسارة السلبية
    )
    gbr = GradientBoostingRegressor(loss="quantile", alpha=alpha, random_state=0)
    search_05p = HalvingRandomSearchCV(
        gbr,
        param_grid,
        resource="n_estimators",
        max_resources=250,
        min_resources=50,
        scoring=neg_mean_pinball_loss_05p_scorer,
        n_jobs=2,
        random_state=0,
    ).fit(X_train, y_train)
    pprint(search_05p.best_params_)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'learning_rate': 0.2,
     'max_depth': 2,
     'min_samples_leaf': 20,
     'min_samples_split': 10,
     'n_estimators': 150}




.. GENERATED FROM PYTHON SOURCE LINES 280-287

نلاحظ أن المعلمات الفائقة التي تم ضبطها يدويًا لمنحنى انحدار
الوسيط تقع في نفس نطاق المعلمات الفائقة المناسبة لمنحنى انحدار
المئين الخامس.

لنقم الآن بضبط المعلمات الفائقة لمنحنى انحدار المئين 95. نحتاج إلى إعادة تعريف
مقياس `scoring` المستخدم لتحديد أفضل نموذج، جنبًا إلى جنب مع ضبط معلمة alpha لمقدر التعزيز المتدرج الداخلي
نفسه:

.. GENERATED FROM PYTHON SOURCE LINES 287-301

.. code-block:: Python


    alpha = 0.95
    neg_mean_pinball_loss_95p_scorer = make_scorer(
        mean_pinball_loss,
        alpha=alpha,
        greater_is_better=False,  # تعظيم الخسارة السلبية
    )
    search_95p = clone(search_05p).set_params(
        estimator__alpha=alpha,
        scoring=neg_mean_pinball_loss_95p_scorer,
    )
    search_95p.fit(X_train, y_train)
    pprint(search_95p.best_params_)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'learning_rate': 0.05,
     'max_depth': 2,
     'min_samples_leaf': 5,
     'min_samples_split': 20,
     'n_estimators': 150}




.. GENERATED FROM PYTHON SOURCE LINES 302-310

تُظهر النتيجة أن المعلمات الفائقة لمنحنى انحدار المئين 95
التي حددها إجراء البحث تقع تقريبًا في نفس نطاق المعلمات الفائقة
التي تم ضبطها يدويًا لمنحنى انحدار الوسيط والمعلمات الفائقة
التي حددها إجراء البحث لمنحنى انحدار المئين الخامس. ومع ذلك،
أدت عمليات البحث عن المعلمات الفائقة إلى تحسين فاصل ثقة 90%
الذي يتكون من تنبؤات هذين المنحنيين الكميين المضبوطين.
لاحظ أن تنبؤ المئين 95 العلوي له شكل أكثر خشونة
من تنبؤ المئين الخامس السفلي بسبب القيم المتطرفة:

.. GENERATED FROM PYTHON SOURCE LINES 310-328

.. code-block:: Python

    y_lower = search_05p.predict(xx)
    y_upper = search_95p.predict(xx)

    fig = plt.figure(figsize=(10, 10))
    plt.plot(xx, f(xx), "g:", linewidth=3, label=r"$f(x) = x\,\sin(x)$")
    plt.plot(X_test, y_test, "b.", markersize=10, label="ملاحظات الاختبار")
    plt.plot(xx, y_upper, "k-")
    plt.plot(xx, y_lower, "k-")
    plt.fill_between(
        xx.ravel(), y_lower, y_upper, alpha=0.4, label="الفترة المتوقعة 90%"
    )
    plt.xlabel("$x$")
    plt.ylabel("$f(x)$")
    plt.ylim(-10, 25)
    plt.legend(loc="upper left")
    plt.title("التنبؤ بمعلمات فائقة مضبوطة")
    plt.show()




.. image-sg:: /auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_quantile_002.png
   :alt: التنبؤ بمعلمات فائقة مضبوطة
   :srcset: /auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_quantile_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 329-334

يبدو الرسم البياني نوعياً أفضل من النماذج غير المضبوطة، خاصة
بالنسبة لشكل الكمية الأقل.

نقوم الآن بتقييم المعايرة المشتركة لزوج المقدرات
كمياً:

.. GENERATED FROM PYTHON SOURCE LINES 334-336

.. code-block:: Python

    coverage_fraction(y_train, search_05p.predict(
        X_train), search_95p.predict(X_train))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    np.float64(0.9026666666666666)



.. GENERATED FROM PYTHON SOURCE LINES 337-339

.. code-block:: Python

    coverage_fraction(y_test, search_05p.predict(
        X_test), search_95p.predict(X_test))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    np.float64(0.796)



.. GENERATED FROM PYTHON SOURCE LINES 340-345

معايرة الزوج المضبوط للأسف ليست أفضل في مجموعة الاختبار:
لا يزال عرض فاصل الثقة المقدر ضيقًا جدًا.

مرة أخرى، سنحتاج إلى تضمين هذه الدراسة في حلقة تحقق متقاطع
لتقييم تقلب هذه التقديرات بشكل أفضل.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.069 seconds)


.. _sphx_glr_download_auto_examples_ensemble_plot_gradient_boosting_quantile.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/main?urlpath=lab/tree/notebooks/auto_examples/ensemble/plot_gradient_boosting_quantile.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../../lite/lab/index.html?path=auto_examples/ensemble/plot_gradient_boosting_quantile.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_gradient_boosting_quantile.ipynb <plot_gradient_boosting_quantile.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_gradient_boosting_quantile.py <plot_gradient_boosting_quantile.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_gradient_boosting_quantile.zip <plot_gradient_boosting_quantile.zip>`


.. include:: plot_gradient_boosting_quantile.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
