
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/covariance/plot_covariance_estimation.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_covariance_plot_covariance_estimation.py>`
        to download the full example code. or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_covariance_plot_covariance_estimation.py:


======================================================
تأثير تحويل الأهداف في نموذج الانحدار
======================================================

في هذا المثال، نقدم نظرة عامة على
:class:`~sklearn.compose.TransformedTargetRegressor`. نستخدم مثالين
لتوضيح فائدة تحويل الأهداف قبل تعلم نموذج انحدار خطي. يستخدم المثال الأول بيانات تركيبية بينما يعتمد المثال الثاني على مجموعة بيانات منازل Ames.

.. GENERATED FROM PYTHON SOURCE LINES 11-32

.. code-block:: Python


    # Authors: The scikit-learn developers
    # SPDX-License-Identifier: BSD-3-Clause

    from sklearn.model_selection import GridSearchCV
    from sklearn.covariance import OAS, LedoitWolf
    from sklearn.covariance import ShrunkCovariance, empirical_covariance, log_likelihood
    from scipy import linalg
    from sklearn.preprocessing import QuantileTransformer
    from sklearn.preprocessing import quantile_transform
    from sklearn.datasets import fetch_openml
    from sklearn.metrics import PredictionErrorDisplay
    from sklearn.linear_model import RidgeCV
    from sklearn.compose import TransformedTargetRegressor
    from sklearn.metrics import median_absolute_error, r2_score
    from sklearn.model_selection import train_test_split
    import matplotlib.pyplot as plt
    from sklearn.datasets import make_regression
    import numpy as np
    print(__doc__)








.. GENERATED FROM PYTHON SOURCE LINES 33-45

مثال تركيبي
#################

يتم إنشاء مجموعة بيانات انحدار عشوائية تركيبية. يتم تعديل الأهداف ``y`` بواسطة:

1. ترجمة جميع الأهداف بحيث تكون جميع الإدخالات
   غير سالبة (عن طريق إضافة القيمة المطلقة لأدنى ``y``) و
2. تطبيق دالة أسية للحصول على أهداف غير خطية
   لا يمكن ملاءمتها باستخدام نموذج خطي بسيط.

لذلك، سيتم استخدام دالة لوغاريتمية (`np.log1p`) ودالة أسية
(`np.expm1`) لتحويل الأهداف قبل تدريب نموذج انحدار خطي واستخدامه للتنبؤ.

.. GENERATED FROM PYTHON SOURCE LINES 45-51

.. code-block:: Python



    X, y = make_regression(n_samples=10_000, noise=100, random_state=0)
    y = np.expm1((y + abs(y.min())) / 200)
    y_trans = np.log1p(y)








.. GENERATED FROM PYTHON SOURCE LINES 52-54

أدناه نرسم دوال كثافة الاحتمال للهدف
قبل وبعد تطبيق الدوال اللوغاريتمية.

.. GENERATED FROM PYTHON SOURCE LINES 54-74

.. code-block:: Python



    f, (ax0, ax1) = plt.subplots(1, 2)

    ax0.hist(y, bins=100, density=True)
    ax0.set_xlim([0, 2000])
    ax0.set_ylabel("الاحتمالية")
    ax0.set_xlabel("الهدف")
    ax0.set_title("توزيع الهدف")

    ax1.hist(y_trans, bins=100, density=True)
    ax1.set_ylabel("الاحتمالية")
    ax1.set_xlabel("الهدف")
    ax1.set_title("توزيع الهدف المحول")

    f.suptitle("البيانات التركيبية", y=1.05)
    plt.tight_layout()

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)




.. image-sg:: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_001.png
   :alt: البيانات التركيبية, توزيع الهدف, توزيع الهدف المحول
   :srcset: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 75-78

في البداية، سيتم تطبيق نموذج خطي على الأهداف الأصلية. نظرًا لـ
اللاخطية، لن يكون النموذج المدرب دقيقًا أثناء
التنبؤ. بعد ذلك، يتم استخدام دالة لوغاريتمية لجعل الأهداف خطية، مما يسمح بتنبؤ أفضل حتى مع نموذج خطي مشابه كما هو موضح بواسطة متوسط الخطأ المطلق (MedAE).

.. GENERATED FROM PYTHON SOURCE LINES 78-87

.. code-block:: Python



    def compute_score(y_true, y_pred):
        return {
            "R2": f"{r2_score(y_true, y_pred):.3f}",
            "MedAE": f"{median_absolute_error(y_true, y_pred):.3f}",
        }









.. GENERATED FROM PYTHON SOURCE LINES 88-125

.. code-block:: Python


    f, (ax0, ax1) = plt.subplots(1, 2, sharey=True)

    ridge_cv = RidgeCV().fit(X_train, y_train)
    y_pred_ridge = ridge_cv.predict(X_test)

    ridge_cv_with_trans_target = TransformedTargetRegressor(
        regressor=RidgeCV(), func=np.log1p, inverse_func=np.expm1
    ).fit(X_train, y_train)
    y_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)

    PredictionErrorDisplay.from_predictions(
        y_test,
        y_pred_ridge,
        kind="actual_vs_predicted",
        ax=ax0,
        scatter_kwargs={"alpha": 0.5},
    )
    PredictionErrorDisplay.from_predictions(
        y_test,
        y_pred_ridge_with_trans_target,
        kind="actual_vs_predicted",
        ax=ax1,
        scatter_kwargs={"alpha": 0.5},
    )

    # إضافة الدرجة في وسيلة الإيضاح لكل محور
    for ax, y_pred in zip([ax0, ax1], [y_pred_ridge, y_pred_ridge_with_trans_target]):
        for name, score in compute_score(y_test, y_pred).items():
            ax.plot([], [], " ", label=f"{name}={score}")
        ax.legend(loc="upper left")

    ax0.set_title("انحدار ريدج \n بدون تحويل الهدف")
    ax1.set_title("انحدار ريدج \n مع تحويل الهدف")
    f.suptitle("البيانات التركيبية", y=1.05)
    plt.tight_layout()




.. image-sg:: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_002.png
   :alt: البيانات التركيبية, انحدار ريدج   بدون تحويل الهدف, انحدار ريدج   مع تحويل الهدف
   :srcset: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 126-132

مجموعة بيانات من العالم الحقيقي
###################

بطريقة مماثلة، يتم استخدام مجموعة بيانات منازل Ames لإظهار تأثير
تحويل الأهداف قبل تعلم نموذج. في هذا المثال،
الهدف المراد التنبؤ به هو سعر بيع كل منزل.

.. GENERATED FROM PYTHON SOURCE LINES 132-144

.. code-block:: Python


    ames = fetch_openml(name="house_prices", as_frame=True)
    # احتفظ بالأعمدة الرقمية فقط
    X = ames.data.select_dtypes(np.number)
    # إزالة الأعمدة ذات القيم NaN أو Inf
    X = X.drop(columns=["LotFrontage", "GarageYrBlt", "MasVnrArea"])
    # اجعل السعر بالآلاف من الدولارات
    y = ames.target / 1000
    y_trans = quantile_transform(
        y.to_frame(), n_quantiles=900, output_distribution="normal", copy=True
    ).squeeze()








.. GENERATED FROM PYTHON SOURCE LINES 145-148

يتم استخدام :class:`~sklearn.preprocessing.QuantileTransformer` لتطبيع
توزيع الهدف قبل تطبيق نموذج
:class:`~sklearn.linear_model.RidgeCV`.

.. GENERATED FROM PYTHON SOURCE LINES 148-163

.. code-block:: Python

    f, (ax0, ax1) = plt.subplots(1, 2)

    ax0.hist(y, bins=100, density=True)
    ax0.set_ylabel("الاحتمالية")
    ax0.set_xlabel("الهدف")
    ax0.set_title("توزيع الهدف")

    ax1.hist(y_trans, bins=100, density=True)
    ax1.set_ylabel("الاحتمالية")
    ax1.set_xlabel("الهدف")
    ax1.set_title("توزيع الهدف المحول")

    f.suptitle("بيانات منازل Ames: سعر البيع", y=1.05)
    plt.tight_layout()




.. image-sg:: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_003.png
   :alt: بيانات منازل Ames: سعر البيع, توزيع الهدف, توزيع الهدف المحول
   :srcset: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 164-166

.. code-block:: Python

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)








.. GENERATED FROM PYTHON SOURCE LINES 167-170

تأثير المحول أضعف من البيانات التركيبية. ومع ذلك،
يؤدي التحويل إلى زيادة في :math:`R^2` وانخفاض كبير
في MedAE. يأخذ مخطط المتبقيات (الهدف المتوقع - الهدف الحقيقي مقابل الهدف المتوقع) بدون تحويل الهدف شكلًا منحنًا يشبه "الابتسامة العكسية" بسبب القيم المتبقية التي تختلف اعتمادًا على قيمة الهدف المتوقع. مع تحويل الهدف، يكون الشكل أكثر خطية مما يشير إلى ملاءمة أفضل للنموذج.

.. GENERATED FROM PYTHON SOURCE LINES 170-247

.. code-block:: Python


    f, (ax0, ax1) = plt.subplots(2, 2, sharey="row", figsize=(6.5, 8))

    ridge_cv = RidgeCV().fit(X_train, y_train)
    y_pred_ridge = ridge_cv.predict(X_test)

    ridge_cv_with_trans_target = TransformedTargetRegressor(
        regressor=RidgeCV(),
        transformer=QuantileTransformer(
            n_quantiles=900, output_distribution="normal"),
    ).fit(X_train, y_train)
    y_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)

    # رسم القيم الفعلية مقابل القيم المتوقعة
    PredictionErrorDisplay.from_predictions(
        y_test,
        y_pred_ridge,
        kind="actual_vs_predicted",
        ax=ax0[0],
        scatter_kwargs={"alpha": 0.5},
    )
    PredictionErrorDisplay.from_predictions(
        y_test,
        y_pred_ridge_with_trans_target,
        kind="actual_vs_predicted",
        ax=ax0[1],
        scatter_kwargs={"alpha": 0.5},
    )

    # إضافة الدرجة في وسيلة الإيضاح لكل محور
    for ax, y_pred in zip([ax0[0], ax0[1]], [y_pred_ridge, y_pred_ridge_with_trans_target]):
        for name, score in compute_score(y_test, y_pred).items():
            ax.plot([], [], " ", label=f"{name}={score}")
        ax.legend(loc="upper left")

    ax0[0].set_title("انحدار ريدج \n بدون تحويل الهدف")
    ax0[1].set_title("انحدار ريدج \n مع تحويل الهدف")


    # رسم المتبقيات مقابل القيم المتوقعة
    PredictionErrorDisplay.from_predictions(
        y_test,
        y_pred_ridge,
        kind="residual_vs_predicted",
        ax=ax1[0],
        scatter_kwargs={"alpha": 0.5},
    )
    PredictionErrorDisplay.from_predictions(
        y_test,
        y_pred_ridge_with_trans_target,
        kind="residual_vs_predicted",
        ax=ax1[1],
        scatter_kwargs={"alpha": 0.5},
    )
    ax1[0].set_title("انحدار ريدج \n بدون تحويل الهدف")
    ax1[1].set_title("انحدار ريدج \n مع تحويل الهدف")

    f.suptitle("بيانات منازل Ames: سعر البيع", y=1.05)
    plt.tight_layout()
    plt.show()
    """
    =======================================================================
    تقدير انكماش التغاير: LedoitWolf مقابل OAS وأقصى احتمال
    =======================================================================

    عند العمل مع تقدير التغاير، فإن النهج المعتاد هو استخدام
    مقدر أقصى احتمال، مثل
    :class:`~sklearn.covariance.EmpiricalCovariance`. إنه غير متحيز، أي
    يتقارب مع التغاير الحقيقي (السكاني) عند إعطاء العديد من
    الملاحظات. ومع ذلك، قد يكون من المفيد أيضًا تنظيمه، من أجل تقليل تباينه؛ وهذا بدوره يؤدي إلى بعض التحيز. يوضح هذا المثال التنظيم البسيط المستخدم في
    مقدرات :ref:`shrunk_covariance`. على وجه الخصوص، يركز على كيفية
    تعيين مقدار التنظيم، أي كيفية اختيار المفاضلة بين التحيز والتباين.
    """

    # Authors: The scikit-learn developers
    # SPDX-License-Identifier: BSD-3-Clause




.. image-sg:: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_004.png
   :alt: بيانات منازل Ames: سعر البيع, انحدار ريدج   بدون تحويل الهدف, انحدار ريدج   مع تحويل الهدف, انحدار ريدج   بدون تحويل الهدف, انحدار ريدج   مع تحويل الهدف
   :srcset: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    '\n=======================================================================\nتقدير انكماش التغاير: LedoitWolf مقابل OAS وأقصى احتمال\n=======================================================================\n\nعند العمل مع تقدير التغاير، فإن النهج المعتاد هو استخدام\nمقدر أقصى احتمال، مثل\n:class:`~sklearn.covariance.EmpiricalCovariance`. إنه غير متحيز، أي\nيتقارب مع التغاير الحقيقي (السكاني) عند إعطاء العديد من\nالملاحظات. ومع ذلك، قد يكون من المفيد أيضًا تنظيمه، من أجل تقليل تباينه؛ وهذا بدوره يؤدي إلى بعض التحيز. يوضح هذا المثال التنظيم البسيط المستخدم في\nمقدرات :ref:`shrunk_covariance`. على وجه الخصوص، يركز على كيفية\nتعيين مقدار التنظيم، أي كيفية اختيار المفاضلة بين التحيز والتباين.\n'



.. GENERATED FROM PYTHON SOURCE LINES 248-250

إنشاء بيانات عينة
--------------------

.. GENERATED FROM PYTHON SOURCE LINES 250-263

.. code-block:: Python



    n_features, n_samples = 40, 20
    np.random.seed(42)
    base_X_train = np.random.normal(size=(n_samples, n_features))
    base_X_test = np.random.normal(size=(n_samples, n_features))

    # تلوين العينات
    coloring_matrix = np.random.normal(size=(n_features, n_features))
    X_train = np.dot(base_X_train, coloring_matrix)
    X_test = np.dot(base_X_test, coloring_matrix)









.. GENERATED FROM PYTHON SOURCE LINES 264-266

حساب الاحتمال على بيانات الاختبار
-----------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 266-280

.. code-block:: Python



    # تغطية نطاق من قيم معامل الانكماش المحتملة
    shrinkages = np.logspace(-2, 0, 30)
    negative_logliks = [
        -ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test) for s in shrinkages
    ]

    # تحت نموذج الحقيقة الأساسية، الذي لن نتمكن من الوصول إليه في الإعدادات الحقيقية
    real_cov = np.dot(coloring_matrix.T, coloring_matrix)
    emp_cov = empirical_covariance(X_train)
    loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))









.. GENERATED FROM PYTHON SOURCE LINES 281-297

مقارنة طرق مختلفة لتعيين معلمة التنظيم
--------------------------------------------------------------------

هنا نقارن 3 طرق:

* تعيين المعلمة عن طريق التحقق المتبادل للاحتمال على ثلاث طيات
  وفقًا لشبكة من معلمات الانكماش المحتملة.

* صيغة مقفلة مقترحة من قبل Ledoit و Wolf لحساب
  معلمة التنظيم المثلى بشكل مقارب (تقليل معيار MSE
  )، مما ينتج عنه تقدير التغاير :class:`~sklearn.covariance.LedoitWolf`.

* تحسين لانكماش Ledoit-Wolf،
  :class:`~sklearn.covariance.OAS`، الذي اقترحه Chen et al.
  تقاربه أفضل بكثير بافتراض أن البيانات
  غاوسية، خاصة للعينات الصغيرة.

.. GENERATED FROM PYTHON SOURCE LINES 297-312

.. code-block:: Python



    # GridSearch لمعامل انكماش مثالي
    tuned_parameters = [{"shrinkage": shrinkages}]
    cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)
    cv.fit(X_train)

    # تقدير معامل الانكماش الأمثل لـ Ledoit-Wolf
    lw = LedoitWolf()
    loglik_lw = lw.fit(X_train).score(X_test)

    # تقدير معامل OAS
    oa = OAS()
    loglik_oa = oa.fit(X_train).score(X_test)








.. GENERATED FROM PYTHON SOURCE LINES 313-320

رسم النتائج
------------


لتحديد خطأ التقدير كميًا، نرسم احتمال البيانات غير المرئية لـ
قيم مختلفة لمعلمة الانكماش. نعرض أيضًا الخيارات بواسطة
التحقق المتبادل، أو مع تقديرات LedoitWolf و OAS.

.. GENERATED FROM PYTHON SOURCE LINES 320-367

.. code-block:: Python



    fig = plt.figure()
    plt.title("التغاير المنظم: الاحتمال ومعامل الانكماش")
    plt.xlabel("معلمة التنظيم: معامل الانكماش")
    plt.ylabel("الخطأ: لوغاريتم الاحتمال السالب على بيانات الاختبار")
    # نطاق منحنى الانكماش
    plt.loglog(shrinkages, negative_logliks, label="لوغاريتم الاحتمال السالب")

    plt.plot(plt.xlim(), 2 * [loglik_real], "--r", label="احتمال التغاير الحقيقي")

    # ضبط العرض
    lik_max = np.amax(negative_logliks)
    lik_min = np.amin(negative_logliks)
    ymin = lik_min - 6.0 * np.log((plt.ylim()[1] - plt.ylim()[0]))
    ymax = lik_max + 10.0 * np.log(lik_max - lik_min)
    xmin = shrinkages[0]
    xmax = shrinkages[-1]
    # احتمال LW
    plt.vlines(
        lw.shrinkage_,
        ymin,
        -loglik_lw,
        color="magenta",
        linewidth=3,
        label="تقدير Ledoit-Wolf",
    )
    # احتمال OAS
    plt.vlines(
        oa.shrinkage_, ymin, -loglik_oa, color="purple", linewidth=3, label="تقدير OAS"
    )
    # أفضل احتمال لمقدر CV
    plt.vlines(
        cv.best_estimator_.shrinkage,
        ymin,
        -cv.best_estimator_.score(X_test),
        color="cyan",
        linewidth=3,
        label="أفضل تقدير للتحقق المتبادل",
    )

    plt.ylim(ymin, ymax)
    plt.xlim(xmin, xmax)
    plt.legend()

    plt.show()




.. image-sg:: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_005.png
   :alt: التغاير المنظم: الاحتمال ومعامل الانكماش
   :srcset: /auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 368-375

.. note::

   يتوافق تقدير أقصى احتمال مع عدم الانكماش،
   وبالتالي يكون أداؤه ضعيفًا. يعمل تقدير Ledoit-Wolf بشكل جيد حقًا،
   حيث إنه قريب من الأمثل وليس مكلفًا من الناحية الحسابية. في هذا
   المثال، يكون تقدير OAS بعيدًا بعض الشيء. ومن المثير للاهتمام، أن كلا
   النهجين يتفوقان على التحقق المتبادل، وهو الأكثر تكلفة من الناحية الحسابية.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 3.029 seconds)


.. _sphx_glr_download_auto_examples_covariance_plot_covariance_estimation.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/main?urlpath=lab/tree/notebooks/auto_examples/covariance/plot_covariance_estimation.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../../lite/lab/index.html?path=auto_examples/covariance/plot_covariance_estimation.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_covariance_estimation.ipynb <plot_covariance_estimation.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_covariance_estimation.py <plot_covariance_estimation.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_covariance_estimation.zip <plot_covariance_estimation.zip>`


.. include:: plot_covariance_estimation.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
