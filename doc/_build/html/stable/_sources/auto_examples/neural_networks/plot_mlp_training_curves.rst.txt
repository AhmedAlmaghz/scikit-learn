
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/neural_networks/plot_mlp_training_curves.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_neural_networks_plot_mlp_training_curves.py>`
        to download the full example code. or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py:


========================================================
مقارنة استراتيجيات التعلم العشوائي لتصنيف الشبكة العصبية متعددة الطبقات
========================================================

هذا المثال يوضح بعض منحنيات الخسارة التدريبية لاستراتيجيات التعلم العشوائي المختلفة، بما في ذلك SGD و Adam. بسبب قيود الوقت، نستخدم عدة مجموعات بيانات صغيرة، والتي قد تكون مناسبة أكثر لخوارزمية L-BFGS. ومع ذلك، يبدو أن الاتجاه العام الموضح في هذه الأمثلة ينطبق أيضًا على مجموعات البيانات الأكبر.

ملاحظة: يمكن أن تعتمد هذه النتائج بشكل كبير على قيمة "learning_rate_init".

.. GENERATED FROM PYTHON SOURCE LINES 11-138



.. image-sg:: /auto_examples/neural_networks/images/sphx_glr_plot_mlp_training_curves_001.png
   :alt: iris, digits, circles, moons
   :srcset: /auto_examples/neural_networks/images/sphx_glr_plot_mlp_training_curves_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    التعلم على مجموعة البيانات iris
    التدريب: معدل التعلم الثابت
    درجة مجموعة التدريب: 0.980000
    خسارة مجموعة التدريب: 0.096950
    التدريب: الثابت مع الزخم
    درجة مجموعة التدريب: 0.980000
    خسارة مجموعة التدريب: 0.049530
    التدريب: الثابت مع زخم نيستروف
    درجة مجموعة التدريب: 0.980000
    خسارة مجموعة التدريب: 0.049540
    التدريب: معدل التعلم مع التوسيع العكسي
    درجة مجموعة التدريب: 0.360000
    خسارة مجموعة التدريب: 0.978444
    التدريب: التوسيع العكسي مع الزخم
    درجة مجموعة التدريب: 0.860000
    خسارة مجموعة التدريب: 0.504185
    التدريب: التوسيع العكسي مع زخم نيستروف
    درجة مجموعة التدريب: 0.860000
    خسارة مجموعة التدريب: 0.503452
    التدريب: آدم
    درجة مجموعة التدريب: 0.980000
    خسارة مجموعة التدريب: 0.045311

    التعلم على مجموعة البيانات digits
    التدريب: معدل التعلم الثابت
    درجة مجموعة التدريب: 0.956038
    خسارة مجموعة التدريب: 0.243802
    التدريب: الثابت مع الزخم
    درجة مجموعة التدريب: 0.992766
    خسارة مجموعة التدريب: 0.041297
    التدريب: الثابت مع زخم نيستروف
    درجة مجموعة التدريب: 0.993879
    خسارة مجموعة التدريب: 0.042898
    التدريب: معدل التعلم مع التوسيع العكسي
    درجة مجموعة التدريب: 0.638843
    خسارة مجموعة التدريب: 1.855465
    التدريب: التوسيع العكسي مع الزخم
    درجة مجموعة التدريب: 0.909293
    خسارة مجموعة التدريب: 0.318387
    التدريب: التوسيع العكسي مع زخم نيستروف
    درجة مجموعة التدريب: 0.912632
    خسارة مجموعة التدريب: 0.290584
    التدريب: آدم
    درجة مجموعة التدريب: 0.991653
    خسارة مجموعة التدريب: 0.045934

    التعلم على مجموعة البيانات circles
    التدريب: معدل التعلم الثابت
    درجة مجموعة التدريب: 0.840000
    خسارة مجموعة التدريب: 0.601052
    التدريب: الثابت مع الزخم
    درجة مجموعة التدريب: 0.940000
    خسارة مجموعة التدريب: 0.157334
    التدريب: الثابت مع زخم نيستروف
    درجة مجموعة التدريب: 0.940000
    خسارة مجموعة التدريب: 0.154453
    التدريب: معدل التعلم مع التوسيع العكسي
    درجة مجموعة التدريب: 0.500000
    خسارة مجموعة التدريب: 0.692470
    التدريب: التوسيع العكسي مع الزخم
    درجة مجموعة التدريب: 0.500000
    خسارة مجموعة التدريب: 0.689751
    التدريب: التوسيع العكسي مع زخم نيستروف
    درجة مجموعة التدريب: 0.500000
    خسارة مجموعة التدريب: 0.689143
    التدريب: آدم
    درجة مجموعة التدريب: 0.940000
    خسارة مجموعة التدريب: 0.150527

    التعلم على مجموعة البيانات moons
    التدريب: معدل التعلم الثابت
    درجة مجموعة التدريب: 0.850000
    خسارة مجموعة التدريب: 0.341523
    التدريب: الثابت مع الزخم
    درجة مجموعة التدريب: 0.850000
    خسارة مجموعة التدريب: 0.336188
    التدريب: الثابت مع زخم نيستروف
    درجة مجموعة التدريب: 0.850000
    خسارة مجموعة التدريب: 0.335919
    التدريب: معدل التعلم مع التوسيع العكسي
    درجة مجموعة التدريب: 0.500000
    خسارة مجموعة التدريب: 0.689015
    التدريب: التوسيع العكسي مع الزخم
    درجة مجموعة التدريب: 0.830000
    خسارة مجموعة التدريب: 0.513034
    التدريب: التوسيع العكسي مع زخم نيستروف
    درجة مجموعة التدريب: 0.830000
    خسارة مجموعة التدريب: 0.512595
    التدريب: آدم
    درجة مجموعة التدريب: 0.930000
    خسارة مجموعة التدريب: 0.170087






|

.. code-block:: Python


    # المؤلفون: مطوري مكتبة ساي كيت ليرن
    # معرف الترخيص: BSD-3-Clause

    import warnings

    import matplotlib.pyplot as plt

    from sklearn import datasets
    from sklearn.exceptions import ConvergenceWarning
    from sklearn.neural_network import MLPClassifier
    from sklearn.preprocessing import MinMaxScaler

    # معدلات تعلم مختلفة وجداول زمنية ومعاملات الزخم
    params = [
        {
            "solver": "sgd",
            "learning_rate": "constant",
            "momentum": 0,
            "learning_rate_init": 0.2,
        },
        {
            "solver": "sgd",
            "learning_rate": "constant",
            "momentum": 0.9,
            "nesterovs_momentum": False,
            "learning_rate_init": 0.2,
        },
        {
            "solver": "sgd",
            "learning_rate": "constant",
            "momentum": 0.9,
            "nesterovs_momentum": True,
            "learning_rate_init": 0.2,
        },
        {
            "solver": "sgd",
            "learning_rate": "invscaling",
            "momentum": 0,
            "learning_rate_init": 0.2,
        },
        {
            "solver": "sgd",
            "learning_rate": "invscaling",
            "momentum": 0.9,
            "nesterovs_momentum": False,
            "learning_rate_init": 0.2,
        },
        {
            "solver": "sgd",
            "learning_rate": "invscaling",
            "momentum": 0.9,
            "nesterovs_momentum": True,
            "learning_rate_init": 0.2,
        },
        {"solver": "adam", "learning_rate_init": 0.01},
    ]

    labels = [
        "معدل التعلم الثابت",
        "الثابت مع الزخم",
        "الثابت مع زخم نيستروف",
        "معدل التعلم مع التوسيع العكسي",
        "التوسيع العكسي مع الزخم",
        "التوسيع العكسي مع زخم نيستروف",
        "آدم",
    ]

    plot_args = [
        {"c": "red", "linestyle": "-"},
        {"c": "green", "linestyle": "-"},
        {"c": "blue", "linestyle": "-"},
        {"c": "red", "linestyle": "--"},
        {"c": "green", "linestyle": "--"},
        {"c": "blue", "linestyle": "--"},
        {"c": "black", "linestyle": "-"},
    ]


    def plot_on_dataset(X, y, ax, name):
        # لكل مجموعة بيانات، قم برسم منحنى التعلم لكل استراتيجية تعلم
        print("\nالتعلم على مجموعة البيانات %s" % name)
        ax.set_title(name)

        X = MinMaxScaler().fit_transform(X)
        mlps = []
        if name == "digits":
            # مجموعة digits أكبر ولكن تتقارب بشكل سريع نسبيًا
            max_iter = 15
        else:
            max_iter = 400

        for label, param in zip(labels, params):
            print("التدريب: %s" % label)
            mlp = MLPClassifier(random_state=0, max_iter=max_iter, **param)

            # بعض تركيبات المعاملات لن تتقارب كما هو موضح في الرسوم البيانية، لذلك يتم تجاهلها هنا
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    "ignore", category=ConvergenceWarning, module="sklearn"
                )
                mlp.fit(X, y)

            mlps.append(mlp)
            print("درجة مجموعة التدريب: %f" % mlp.score(X, y))
            print("خسارة مجموعة التدريب: %f" % mlp.loss_)
        for mlp, label, args in zip(mlps, labels, plot_args):
            ax.plot(mlp.loss_curve_, label=label, **args)


    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    # تحميل / توليد بعض مجموعات البيانات التجريبية
    iris = datasets.load_iris()
    X_digits, y_digits = datasets.load_digits(return_X_y=True)
    data_sets = [
        (iris.data, iris.target),
        (X_digits, y_digits),
        datasets.make_circles(noise=0.2, factor=0.5, random_state=1),
        datasets.make_moons(noise=0.3, random_state=0),
    ]

    for ax, data, name in zip(
        axes.ravel(), data_sets, ["iris", "digits", "circles", "moons"]
    ):
        plot_on_dataset(*data, ax=ax, name=name)

    fig.legend(ax.get_lines(), labels, ncol=3, loc="upper center")
    plt.show()

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 3.636 seconds)


.. _sphx_glr_download_auto_examples_neural_networks_plot_mlp_training_curves.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/main?urlpath=lab/tree/notebooks/auto_examples/neural_networks/plot_mlp_training_curves.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../../lite/lab/index.html?path=auto_examples/neural_networks/plot_mlp_training_curves.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_mlp_training_curves.ipynb <plot_mlp_training_curves.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_mlp_training_curves.py <plot_mlp_training_curves.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_mlp_training_curves.zip <plot_mlp_training_curves.zip>`


.. include:: plot_mlp_training_curves.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
