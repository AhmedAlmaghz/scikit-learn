
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/applications/plot_time_series_lagged_features.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_applications_plot_time_series_lagged_features.py>`
        to download the full example code. or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_applications_plot_time_series_lagged_features.py:


===========================================
الميزات المتأخرة للتنبؤ بالسلاسل الزمنية
===========================================

هذا المثال يوضح كيفية استخدام الميزات المتأخرة التي تم تصميمها بواسطة Polars في التنبؤ بالسلاسل الزمنية باستخدام :class:`~sklearn.ensemble.HistGradientBoostingRegressor` على مجموعة بيانات طلب مشاركة الدراجات.

راجع المثال على
:ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`
لاستكشاف بعض البيانات حول هذه المجموعة والاطلاع على عرض توضيحي حول الهندسة الميزات الدورية.

.. GENERATED FROM PYTHON SOURCE LINES 12-15

.. code-block:: Python

    # المؤلفون: مطوري scikit-learn
    # معرف SPDX-License: BSD-3-Clause








.. GENERATED FROM PYTHON SOURCE LINES 16-29

تحليل مجموعة بيانات طلب مشاركة الدراجات
-----------------------------------------

نبدأ بتحميل البيانات من مستودع OpenML كملف Parquet خام
لتوضيح كيفية العمل مع ملف Parquet عشوائي بدلاً من إخفاء هذه
الخطوة في أداة ملائمة مثل `sklearn.datasets.fetch_openml`.

يمكن العثور على عنوان URL لملف Parquet في الوصف JSON لمجموعة بيانات
طلب مشاركة الدراجات مع معرف 44063 على openml.org
(https://openml.org/search?type=data&status=active&id=44063).

يتم توفير هاش `sha256` للملف أيضًا لضمان سلامة الملف
الذي تم تنزيله.

.. GENERATED FROM PYTHON SOURCE LINES 29-57

.. code-block:: Python

    from sklearn.model_selection import cross_validate
    from sklearn.metrics import (
        make_scorer,
        mean_absolute_error,
        mean_pinball_loss,
        root_mean_squared_error,
    )
    from collections import defaultdict
    from sklearn.model_selection import cross_val_score
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import HistGradientBoostingRegressor
    import matplotlib.pyplot as plt
    import polars.selectors as cs
    import numpy as np
    import polars as pl

    from sklearn.datasets import fetch_file

    pl.Config.set_fmt_str_lengths(20)

    bike_sharing_data_file = fetch_file(
        "https://openml1.win.tue.nl/datasets/0004/44063/dataset_44063.pq",
        sha256="d120af76829af0d256338dc6dd4be5df4fd1f35bf3a283cab66a51c1c6abd06a",
    )
    bike_sharing_data_file





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    PosixPath('/root/scikit_learn_data/openml1.win.tue.nl/datasets_0004_44063/dataset_44063.pq')



.. GENERATED FROM PYTHON SOURCE LINES 58-62

نقوم بتحميل ملف Parquet باستخدام Polars للهندسة الميزات. يقوم Polars
تلقائيًا بتخزين التعبيرات الفرعية الشائعة التي يتم إعادة استخدامها في تعبيرات متعددة
(مثل `pl.col("count").shift(1)` أدناه). راجع
https://docs.pola.rs/user-guide/lazy/optimizations/ لمزيد من المعلومات.

.. GENERATED FROM PYTHON SOURCE LINES 62-65

.. code-block:: Python


    df = pl.read_parquet(bike_sharing_data_file)








.. GENERATED FROM PYTHON SOURCE LINES 66-68

بعد ذلك، نلقي نظرة على الملخص الإحصائي لمجموعة البيانات
حتى نتمكن من فهم البيانات التي نعمل عليها بشكل أفضل.

.. GENERATED FROM PYTHON SOURCE LINES 68-73

.. code-block:: Python


    summary = df.select(cs.numeric()).describe()
    summary







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><style>
    .dataframe > thead > tr,
    .dataframe > tbody > tr {
      text-align: right;
      white-space: pre-wrap;
    }
    </style>
    <small>shape: (9, 8)</small><table border="1" class="dataframe"><thead><tr><th>statistic</th><th>month</th><th>hour</th><th>temp</th><th>feel_temp</th><th>humidity</th><th>windspeed</th><th>count</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>17379.0</td><td>17379.0</td><td>17379.0</td><td>17379.0</td><td>17379.0</td><td>17379.0</td><td>17379.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>6.537775</td><td>11.546752</td><td>20.376474</td><td>23.788755</td><td>0.627229</td><td>12.73654</td><td>189.463088</td></tr><tr><td>&quot;std&quot;</td><td>3.438776</td><td>6.914405</td><td>7.894801</td><td>8.592511</td><td>0.19293</td><td>8.196795</td><td>181.387599</td></tr><tr><td>&quot;min&quot;</td><td>1.0</td><td>0.0</td><td>0.82</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td></tr><tr><td>&quot;25%&quot;</td><td>4.0</td><td>6.0</td><td>13.94</td><td>16.665</td><td>0.48</td><td>7.0015</td><td>40.0</td></tr><tr><td>&quot;50%&quot;</td><td>7.0</td><td>12.0</td><td>20.5</td><td>24.24</td><td>0.63</td><td>12.998</td><td>142.0</td></tr><tr><td>&quot;75%&quot;</td><td>10.0</td><td>18.0</td><td>27.06</td><td>31.06</td><td>0.78</td><td>16.9979</td><td>281.0</td></tr><tr><td>&quot;max&quot;</td><td>12.0</td><td>23.0</td><td>41.0</td><td>50.0</td><td>1.0</td><td>56.9969</td><td>977.0</td></tr></tbody></table></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 74-76

دعنا نلقي نظرة على عدد المواسم `"fall"`، `"spring"`، `"summer"`
و `"winter"` الموجودة في مجموعة البيانات للتأكد من أنها متوازنة.

.. GENERATED FROM PYTHON SOURCE LINES 76-81

.. code-block:: Python



    df["season"].value_counts()







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><style>
    .dataframe > thead > tr,
    .dataframe > tbody > tr {
      text-align: right;
      white-space: pre-wrap;
    }
    </style>
    <small>shape: (4, 2)</small><table border="1" class="dataframe"><thead><tr><th>season</th><th>count</th></tr><tr><td>cat</td><td>u32</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>4242</td></tr><tr><td>&quot;0&quot;</td><td>4496</td></tr><tr><td>&quot;2&quot;</td><td>4409</td></tr><tr><td>&quot;3&quot;</td><td>4232</td></tr></tbody></table></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 82-89

توليد الميزات المتأخرة المصممة بواسطة Polars
--------------------------------------------
دعنا نأخذ في الاعتبار مشكلة التنبؤ بالطلب في
الساعة التالية بناءً على الطلبات السابقة. نظرًا لأن الطلب هو متغير مستمر،
يمكن للمرء أن يستخدم بشكل حدسي أي نموذج انحدار. ومع ذلك، لا نملك
مجموعة البيانات المعتادة `(X_train, y_train)`. بدلاً من ذلك، لدينا فقط
بيانات الطلب `y_train` منظمة تسلسليًا حسب الوقت.

.. GENERATED FROM PYTHON SOURCE LINES 89-105

.. code-block:: Python

    lagged_df = df.select(
        "count",
        *[pl.col("count").shift(i).alias(f"lagged_count_{i}h") for i in [1, 2, 3]],
        lagged_count_1d=pl.col("count").shift(24),
        lagged_count_1d_1h=pl.col("count").shift(24 + 1),
        lagged_count_7d=pl.col("count").shift(7 * 24),
        lagged_count_7d_1h=pl.col("count").shift(7 * 24 + 1),
        lagged_mean_24h=pl.col("count").shift(1).rolling_mean(24),
        lagged_max_24h=pl.col("count").shift(1).rolling_max(24),
        lagged_min_24h=pl.col("count").shift(1).rolling_min(24),
        lagged_mean_7d=pl.col("count").shift(1).rolling_mean(7 * 24),
        lagged_max_7d=pl.col("count").shift(1).rolling_max(7 * 24),
        lagged_min_7d=pl.col("count").shift(1).rolling_min(7 * 24),
    )
    lagged_df.tail(10)






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><style>
    .dataframe > thead > tr,
    .dataframe > tbody > tr {
      text-align: right;
      white-space: pre-wrap;
    }
    </style>
    <small>shape: (10, 14)</small><table border="1" class="dataframe"><thead><tr><th>count</th><th>lagged_count_1h</th><th>lagged_count_2h</th><th>lagged_count_3h</th><th>lagged_count_1d</th><th>lagged_count_1d_1h</th><th>lagged_count_7d</th><th>lagged_count_7d_1h</th><th>lagged_mean_24h</th><th>lagged_max_24h</th><th>lagged_min_24h</th><th>lagged_mean_7d</th><th>lagged_max_7d</th><th>lagged_min_7d</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>247</td><td>203</td><td>224</td><td>157</td><td>160</td><td>169</td><td>70</td><td>135</td><td>93.5</td><td>224</td><td>1</td><td>67.732143</td><td>271</td><td>1</td></tr><tr><td>315</td><td>247</td><td>203</td><td>224</td><td>138</td><td>160</td><td>46</td><td>70</td><td>97.125</td><td>247</td><td>1</td><td>68.785714</td><td>271</td><td>1</td></tr><tr><td>214</td><td>315</td><td>247</td><td>203</td><td>133</td><td>138</td><td>33</td><td>46</td><td>104.5</td><td>315</td><td>1</td><td>70.386905</td><td>315</td><td>1</td></tr><tr><td>164</td><td>214</td><td>315</td><td>247</td><td>123</td><td>133</td><td>33</td><td>33</td><td>107.875</td><td>315</td><td>1</td><td>71.464286</td><td>315</td><td>1</td></tr><tr><td>122</td><td>164</td><td>214</td><td>315</td><td>125</td><td>123</td><td>26</td><td>33</td><td>109.583333</td><td>315</td><td>1</td><td>72.244048</td><td>315</td><td>1</td></tr><tr><td>119</td><td>122</td><td>164</td><td>214</td><td>102</td><td>125</td><td>26</td><td>26</td><td>109.458333</td><td>315</td><td>1</td><td>72.815476</td><td>315</td><td>1</td></tr><tr><td>89</td><td>119</td><td>122</td><td>164</td><td>72</td><td>102</td><td>18</td><td>26</td><td>110.166667</td><td>315</td><td>1</td><td>73.369048</td><td>315</td><td>1</td></tr><tr><td>90</td><td>89</td><td>119</td><td>122</td><td>47</td><td>72</td><td>23</td><td>18</td><td>110.875</td><td>315</td><td>1</td><td>73.791667</td><td>315</td><td>1</td></tr><tr><td>61</td><td>90</td><td>89</td><td>119</td><td>36</td><td>47</td><td>22</td><td>23</td><td>112.666667</td><td>315</td><td>1</td><td>74.190476</td><td>315</td><td>1</td></tr><tr><td>49</td><td>61</td><td>90</td><td>89</td><td>49</td><td>36</td><td>12</td><td>22</td><td>113.708333</td><td>315</td><td>1</td><td>74.422619</td><td>315</td><td>1</td></tr></tbody></table></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 106-107

ولكن انتبه، فإن الأسطر الأولى لها قيم غير محددة لأن ماضيها غير معروف. يعتمد هذا على مقدار التأخير الذي استخدمناه:

.. GENERATED FROM PYTHON SOURCE LINES 107-109

.. code-block:: Python

    lagged_df.head(10)






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><style>
    .dataframe > thead > tr,
    .dataframe > tbody > tr {
      text-align: right;
      white-space: pre-wrap;
    }
    </style>
    <small>shape: (10, 14)</small><table border="1" class="dataframe"><thead><tr><th>count</th><th>lagged_count_1h</th><th>lagged_count_2h</th><th>lagged_count_3h</th><th>lagged_count_1d</th><th>lagged_count_1d_1h</th><th>lagged_count_7d</th><th>lagged_count_7d_1h</th><th>lagged_mean_24h</th><th>lagged_max_24h</th><th>lagged_min_24h</th><th>lagged_mean_7d</th><th>lagged_max_7d</th><th>lagged_min_7d</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>16</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>40</td><td>16</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>32</td><td>40</td><td>16</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>13</td><td>32</td><td>40</td><td>16</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>13</td><td>32</td><td>40</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>13</td><td>32</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>1</td><td>1</td><td>13</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>2</td><td>1</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>8</td><td>3</td><td>2</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>14</td><td>8</td><td>3</td><td>2</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 110-112

يمكننا الآن فصل الميزات المتأخرة في مصفوفة `X` ومتغير الهدف
(العددات التي يتعين التنبؤ بها) في مصفوفة من نفس البعد الأول `y`.

.. GENERATED FROM PYTHON SOURCE LINES 112-116

.. code-block:: Python

    lagged_df = lagged_df.drop_nulls()
    X = lagged_df.drop("count")
    y = lagged_df["count"]
    print("X shape: {}\ny shape: {}".format(X.shape, y.shape))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    X shape: (17210, 13)
    y shape: (17210,)




.. GENERATED FROM PYTHON SOURCE LINES 117-124

تقييم ساذج للتنبؤ بالطلب على الدراجات في الساعة التالية
--------------------------------------------------------
دعنا نقسم مجموعتنا المجدولة بشكل عشوائي لتدريب نموذج شجرة الانحدار المعزز
(GBRT) وتقييمه باستخدام متوسط خطأ النسبة المئوية (MAPE). إذا كان نموذجنا يهدف إلى التنبؤ
(أي التنبؤ ببيانات المستقبل من بيانات الماضي)، فيجب علينا عدم استخدام بيانات التدريب
التي تكون لاحقة لبيانات الاختبار. في تعلم الآلة للسلاسل الزمنية
لا يصح افتراض "i.i.d" (مستقل ومتطابق التوزيع) لأن نقاط البيانات ليست مستقلة ولها علاقة زمنية.

.. GENERATED FROM PYTHON SOURCE LINES 124-131

.. code-block:: Python


    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    model = HistGradientBoostingRegressor().fit(X_train, y_train)








.. GENERATED FROM PYTHON SOURCE LINES 132-133

إلقاء نظرة على أداء النموذج.

.. GENERATED FROM PYTHON SOURCE LINES 133-137

.. code-block:: Python


    y_pred = model.predict(X_test)
    mean_absolute_percentage_error(y_test, y_pred)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    0.3928958540837012



.. GENERATED FROM PYTHON SOURCE LINES 138-144

تقييم التنبؤ الصحيح للساعة التالية
---------------------------------------
دعنا نستخدم استراتيجيات تقسيم التقييم الصحيحة التي تأخذ في الاعتبار
البنية الزمنية لمجموعة البيانات لتقييم قدرة النموذج على
التنبؤ بنقاط البيانات في المستقبل (لتجنب الغش عن طريق قراءة القيم من
الميزات المتأخرة في مجموعة التدريب).

.. GENERATED FROM PYTHON SOURCE LINES 144-153

.. code-block:: Python


    ts_cv = TimeSeriesSplit(
        n_splits=3,  # للحفاظ على سرعة الكمبيوتر المحمول بما يكفي على أجهزة الكمبيوتر المحمولة الشائعة
        gap=48,  # فجوة بيانات لمدة يومين بين التدريب والاختبار
        max_train_size=10000,  # الحفاظ على مجموعات التدريب بأحجام قابلة للمقارنة
        test_size=3000,  # للحصول على 2 أو 3 أرقام من الدقة في الدرجات
    )
    all_splits = list(ts_cv.split(X, y))








.. GENERATED FROM PYTHON SOURCE LINES 154-155

تدريب النموذج وتقييم أدائه بناءً على MAPE.

.. GENERATED FROM PYTHON SOURCE LINES 155-163

.. code-block:: Python

    train_idx, test_idx = all_splits[0]
    X_train, X_test = X[train_idx, :], X[test_idx, :]
    y_train, y_test = y[train_idx], y[test_idx]

    model = HistGradientBoostingRegressor().fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mean_absolute_percentage_error(y_test, y_pred)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    0.44300751539296973



.. GENERATED FROM PYTHON SOURCE LINES 164-169

خطأ التعميم المقاس عبر تقسيم الاختبار المدرب العشوائي
هو متفائل للغاية. من المرجح أن يكون التعميم عبر تقسيم زمني
أكثر تمثيلاً للأداء الحقيقي لنموذج الانحدار.
دعنا نقيم هذه التباين في تقييم الخطأ لدينا مع
تقسيم الصحيح:

.. GENERATED FROM PYTHON SOURCE LINES 169-175

.. code-block:: Python


    cv_mape_scores = -cross_val_score(
        model, X, y, cv=ts_cv, scoring="neg_mean_absolute_percentage_error"
    )
    cv_mape_scores





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([0.44300752, 0.27772182, 0.3697178 ])



.. GENERATED FROM PYTHON SOURCE LINES 176-179

التباين عبر التقسيمات كبير جدًا! في إعداد الحياة الواقعية
يُنصح باستخدام المزيد من التقسيمات لتقييم التباين بشكل أفضل.
دعنا نبلغ عن متوسط درجات CV وانحرافها المعياري من الآن فصاعدًا.

.. GENERATED FROM PYTHON SOURCE LINES 179-181

.. code-block:: Python

    print(f"CV MAPE: {cv_mape_scores.mean():.3f} ± {cv_mape_scores.std():.3f}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    CV MAPE: 0.363 ± 0.068




.. GENERATED FROM PYTHON SOURCE LINES 182-184

يمكننا حساب العديد من مجموعات مقاييس التقييم ووظائف الخسارة،
والتي يتم الإبلاغ عنها أدناه بقليل.

.. GENERATED FROM PYTHON SOURCE LINES 184-225

.. code-block:: Python



    def consolidate_scores(cv_results, scores, metric):
        if metric == "MAPE":
            scores[metric].append(f"{value.mean():.2f} ± {value.std():.2f}")
        else:
            scores[metric].append(f"{value.mean():.1f} ± {value.std():.1f}")

        return scores


    scoring = {
        "MAPE": make_scorer(mean_absolute_percentage_error),
        "RMSE": make_scorer(root_mean_squared_error),
        "MAE": make_scorer(mean_absolute_error),
        "pinball_loss_05": make_scorer(mean_pinball_loss, alpha=0.05),
        "pinball_loss_50": make_scorer(mean_pinball_loss, alpha=0.50),
        "pinball_loss_95": make_scorer(mean_pinball_loss, alpha=0.95),
    }
    loss_functions = ["squared_error", "poisson", "absolute_error"]
    scores = defaultdict(list)
    for loss_func in loss_functions:
        model = HistGradientBoostingRegressor(loss=loss_func)
        cv_results = cross_validate(
            model,
            X,
            y,
            cv=ts_cv,
            scoring=scoring,
            n_jobs=2,
        )
        time = cv_results["fit_time"]
        scores["loss"].append(loss_func)
        scores["fit_time"].append(f"{time.mean():.2f} ± {time.std():.2f} s")

        for key, value in cv_results.items():
            if key.startswith("test_"):
                metric = key.split("test_")[1]
                scores = consolidate_scores(cv_results, scores, metric)









.. GENERATED FROM PYTHON SOURCE LINES 226-241

نمذجة عدم اليقين التنبؤي عبر الانحدار الكمي
-------------------------------------------------------
بدلاً من نمذجة القيمة المتوقعة لتوزيع
:math:`Y|X` مثلما تفعل خسائر المربعات الصغرى و Poisson، يمكن للمرء أن يحاول
تقدير الكميات لتوزيع الشرطي.

:math:`Y|X=x_i` من المتوقع أن تكون متغيرًا عشوائيًا لنقطة بيانات معينة
:math:`x_i` لأننا نتوقع أن عدد الإيجارات لا يمكن التنبؤ به بدقة 100%
من الميزات. يمكن أن يتأثر بعوامل أخرى لا يتم التقاطها بشكل صحيح بواسطة
الميزات المتأخرة الموجودة. على سبيل المثال، ما إذا كان سيمطر في الساعة التالية
لا يمكن التنبؤ به بالكامل من بيانات إيجار الدراجات في الساعات السابقة. هذا ما نسميه
عدم اليقين العشوائي.

يجعل الانحدار الكمي من الممكن إعطاء وصف أدق لهذا
التوزيع دون افتراضات قوية حول شكله.

.. GENERATED FROM PYTHON SOURCE LINES 241-266

.. code-block:: Python

    quantile_list = [0.05, 0.5, 0.95]

    for quantile in quantile_list:
        model = HistGradientBoostingRegressor(loss="quantile", quantile=quantile)
        cv_results = cross_validate(
            model,
            X,
            y,
            cv=ts_cv,
            scoring=scoring,
            n_jobs=2,
        )
        time = cv_results["fit_time"]
        scores["fit_time"].append(f"{time.mean():.2f} ± {time.std():.2f} s")

        scores["loss"].append(f"quantile {int(quantile*100)}")
        for key, value in cv_results.items():
            if key.startswith("test_"):
                metric = key.split("test_")[1]
                scores = consolidate_scores(cv_results, scores, metric)

    scores_df = pl.DataFrame(scores)
    scores_df







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><style>
    .dataframe > thead > tr,
    .dataframe > tbody > tr {
      text-align: right;
      white-space: pre-wrap;
    }
    </style>
    <small>shape: (6, 8)</small><table border="1" class="dataframe"><thead><tr><th>loss</th><th>fit_time</th><th>MAPE</th><th>RMSE</th><th>MAE</th><th>pinball_loss_05</th><th>pinball_loss_50</th><th>pinball_loss_95</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;squared_error&quot;</td><td>&quot;0.33 ± 0.01 s&quot;</td><td>&quot;0.36 ± 0.07&quot;</td><td>&quot;62.3 ± 3.5&quot;</td><td>&quot;39.1 ± 2.3&quot;</td><td>&quot;17.7 ± 1.3&quot;</td><td>&quot;19.5 ± 1.1&quot;</td><td>&quot;21.4 ± 2.4&quot;</td></tr><tr><td>&quot;poisson&quot;</td><td>&quot;0.35 ± 0.01 s&quot;</td><td>&quot;0.32 ± 0.07&quot;</td><td>&quot;64.2 ± 4.0&quot;</td><td>&quot;39.3 ± 2.8&quot;</td><td>&quot;16.7 ± 1.5&quot;</td><td>&quot;19.7 ± 1.4&quot;</td><td>&quot;22.6 ± 3.0&quot;</td></tr><tr><td>&quot;absolute_error&quot;</td><td>&quot;0.49 ± 0.03 s&quot;</td><td>&quot;0.32 ± 0.06&quot;</td><td>&quot;64.6 ± 3.8&quot;</td><td>&quot;39.9 ± 3.2&quot;</td><td>&quot;17.1 ± 1.1&quot;</td><td>&quot;19.9 ± 1.6&quot;</td><td>&quot;22.7 ± 3.1&quot;</td></tr><tr><td>&quot;quantile 5&quot;</td><td>&quot;0.64 ± 0.02 s&quot;</td><td>&quot;0.41 ± 0.01&quot;</td><td>&quot;145.6 ± 20.9&quot;</td><td>&quot;92.5 ± 16.2&quot;</td><td>&quot;5.9 ± 0.9&quot;</td><td>&quot;46.2 ± 8.1&quot;</td><td>&quot;86.6 ± 15.3&quot;</td></tr><tr><td>&quot;quantile 50&quot;</td><td>&quot;0.68 ± 0.01 s&quot;</td><td>&quot;0.32 ± 0.06&quot;</td><td>&quot;64.6 ± 3.8&quot;</td><td>&quot;39.9 ± 3.2&quot;</td><td>&quot;17.1 ± 1.1&quot;</td><td>&quot;19.9 ± 1.6&quot;</td><td>&quot;22.7 ± 3.1&quot;</td></tr><tr><td>&quot;quantile 95&quot;</td><td>&quot;0.86 ± 0.10 s&quot;</td><td>&quot;1.07 ± 0.27&quot;</td><td>&quot;99.6 ± 8.7&quot;</td><td>&quot;72.0 ± 6.1&quot;</td><td>&quot;62.9 ± 7.4&quot;</td><td>&quot;36.0 ± 3.1&quot;</td><td>&quot;9.1 ± 1.3&quot;</td></tr></tbody></table></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 267-268

دعنا نلقي نظرة على الخسائر التي تقلل من كل مقياس.

.. GENERATED FROM PYTHON SOURCE LINES 268-282

.. code-block:: Python

    def min_arg(col):
        col_split = pl.col(col).str.split(" ")
        return pl.arg_sort_by(
            col_split.list.get(0).cast(pl.Float64),
            col_split.list.get(2).cast(pl.Float64),
        ).first()


    scores_df.select(
        pl.col("loss").get(min_arg(col_name)).alias(col_name)
        for col_name in scores_df.columns
        if col_name != "loss"
    )






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><style>
    .dataframe > thead > tr,
    .dataframe > tbody > tr {
      text-align: right;
      white-space: pre-wrap;
    }
    </style>
    <small>shape: (1, 7)</small><table border="1" class="dataframe"><thead><tr><th>fit_time</th><th>MAPE</th><th>RMSE</th><th>MAE</th><th>pinball_loss_05</th><th>pinball_loss_50</th><th>pinball_loss_95</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;squared_error&quot;</td><td>&quot;absolute_error&quot;</td><td>&quot;squared_error&quot;</td><td>&quot;squared_error&quot;</td><td>&quot;quantile 5&quot;</td><td>&quot;squared_error&quot;</td><td>&quot;quantile 95&quot;</td></tr></tbody></table></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 283-294

حتى إذا كانت توزيعات الدرجات تتداخل بسبب التباين في مجموعة البيانات،
فمن الصحيح أن متوسط RMSE أقل عندما `loss="squared_error"`، في حين أن
متوسط MAPE أقل عندما `loss="absolute_error"` كما هو متوقع. هذا هو
أيضًا الحال بالنسبة لمتوسط Pinball Loss مع الكميات 5 و95. الدرجات
المقابلة لخسارة الكمية 50 تتداخل مع الدرجات التي تم الحصول عليها
عن طريق تقليل وظائف الخسارة الأخرى، وهو أيضًا الحال بالنسبة لـ MAE.

نظرة نوعية على التنبؤات
-------------------------------------
يمكننا الآن تصور أداء النموذج فيما يتعلق
بالخمسة بالمائة، والوسيط، والـ 95 بالمائة:

.. GENERATED FROM PYTHON SOURCE LINES 294-317

.. code-block:: Python

    all_splits = list(ts_cv.split(X, y))
    train_idx, test_idx = all_splits[0]

    X_train, X_test = X[train_idx, :], X[test_idx, :]
    y_train, y_test = y[train_idx], y[test_idx]
    X_train, X_test = X[train_idx, :], X[test_idx, :]
    y_train, y_test = y[train_idx], y[test_idx]

    max_iter = 50
    gbrt_mean_poisson = HistGradientBoostingRegressor(
        loss="poisson", max_iter=max_iter)
    gbrt_mean_poisson.fit(X_train, y_train)
    mean_predictions = gbrt_mean_poisson.predict(X_test)

    gbrt_median = HistGradientBoostingRegressor(
        loss="quantile", quantile=0.5, max_iter=max_iter
    )
    gbrt_median.fit(X_train, y_train)
    median_predictions = gbrt_median.predict(X_test)

    gbrt_percentile_5 = HistGradientBoostingRegressor(
        loss="quantile", quantile=0.05, max_iter=max_iter
    )








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.648 seconds)


.. _sphx_glr_download_auto_examples_applications_plot_time_series_lagged_features.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/main?urlpath=lab/tree/notebooks/auto_examples/applications/plot_time_series_lagged_features.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../../lite/lab/index.html?path=auto_examples/applications/plot_time_series_lagged_features.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_time_series_lagged_features.ipynb <plot_time_series_lagged_features.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_time_series_lagged_features.py <plot_time_series_lagged_features.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_time_series_lagged_features.zip <plot_time_series_lagged_features.zip>`


.. include:: plot_time_series_lagged_features.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
