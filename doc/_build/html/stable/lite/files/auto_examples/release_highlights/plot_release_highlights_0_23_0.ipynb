{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the scikit-learn examples in JupyterLite is experimental and you may encounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import sklearn` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/scikit-learn/scikit-learn/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u0623\u0628\u0631\u0632 \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u062c\u062f\u064a\u062f\u0629 \u0641\u064a \u0625\u0635\u062f\u0627\u0631 scikit-learn 0.23\n\n.. currentmodule:: sklearn\n\n\u064a\u0633\u0639\u062f\u0646\u0627 \u0627\u0644\u0625\u0639\u0644\u0627\u0646 \u0639\u0646 \u0625\u0635\u062f\u0627\u0631 scikit-learn 0.23! \u062a\u0645 \u0625\u0635\u0644\u0627\u062d \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0623\u062e\u0637\u0627\u0621\n\u0648\u0625\u0636\u0627\u0641\u0629 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u062a\u062d\u0633\u064a\u0646\u0627\u062a\u060c \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0628\u0639\u0636 \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u0631\u0626\u064a\u0633\u064a\u0629 \u0627\u0644\u062c\u062f\u064a\u062f\u0629. \u0646\u0633\u062a\u0639\u0631\u0636\n\u0623\u062f\u0646\u0627\u0647 \u0628\u0639\u0636 \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u0631\u0626\u064a\u0633\u064a\u0629 \u0644\u0647\u0630\u0627 \u0627\u0644\u0625\u0635\u062f\u0627\u0631. **\u0644\u0644\u0627\u0637\u0644\u0627\u0639 \u0639\u0644\u0649 \u0642\u0627\u0626\u0645\u0629 \u0634\u0627\u0645\u0644\u0629 \u0628\u062c\u0645\u064a\u0639\n\u0627\u0644\u062a\u063a\u064a\u064a\u0631\u0627\u062a**\u060c \u064a\u0631\u062c\u0649 \u0627\u0644\u0631\u062c\u0648\u0639 \u0625\u0644\u0649 `\u0645\u0644\u0627\u062d\u0638\u0627\u062a \u0627\u0644\u0625\u0635\u062f\u0627\u0631 <release_notes_0_23>`.\n\n\u0644\u062a\u062b\u0628\u064a\u062a \u0623\u062d\u062f\u062b \u0625\u0635\u062f\u0627\u0631 (\u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 pip)::\n\n    pip install --upgrade scikit-learn\n\n\u0623\u0648 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 conda::\n\n    conda install -c conda-forge scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0627\u0644\u0646\u0645\u0627\u0630\u062c \u0627\u0644\u062e\u0637\u064a\u0629 \u0627\u0644\u0639\u0627\u0645\u0629\u060c \u0648\u062e\u0633\u0627\u0631\u0629 \u0628\u0648\u0627\u0633\u0648\u0646 \u0644\u0644\u062a\u0639\u0632\u064a\u0632 \u0627\u0644\u062a\u062f\u0631\u064a\u062c\u064a\n\u0627\u0644\u0646\u0645\u0627\u0630\u062c \u0627\u0644\u062e\u0637\u064a\u0629 \u0627\u0644\u0639\u0627\u0645\u0629 \u0627\u0644\u0645\u0646\u062a\u0638\u0631\u0629 \u0645\u0646\u0630 \u0641\u062a\u0631\u0629 \u0637\u0648\u064a\u0644\u0629 \u0645\u0639 \u062f\u0627\u0644\u0627\u062a \u062e\u0633\u0627\u0631\u0629 \u063a\u064a\u0631 \u0637\u0628\u064a\u0639\u064a\u0629 \u0623\u0635\u0628\u062d\u062a\n\u0645\u062a\u0648\u0641\u0631\u0629 \u0627\u0644\u0622\u0646. \u0639\u0644\u0649 \u0648\u062c\u0647 \u0627\u0644\u062e\u0635\u0648\u0635\u060c \u062a\u0645 \u062a\u0646\u0641\u064a\u0630 \u062b\u0644\u0627\u062b\u0629 \u0645\u0646\u0638\u0645\u064a\u0646 \u062c\u062f\u062f:\n:class:`~sklearn.linear_model.PoissonRegressor`\u060c\n:class:`~sklearn.linear_model.GammaRegressor`\u060c \u0648\n:class:`~sklearn.linear_model.TweedieRegressor`. \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0645\u0646\u0638\u0645 \u0628\u0648\u0627\u0633\u0648\u0646\n\u0644\u0646\u0645\u0630\u062c\u0629 \u0627\u0644\u0639\u062f \u0627\u0644\u0625\u064a\u062c\u0627\u0628\u064a \u0644\u0644\u0645\u062a\u063a\u064a\u0631\u0627\u062a \u0627\u0644\u0635\u062d\u064a\u062d\u0629\u060c \u0623\u0648 \u0627\u0644\u062a\u0631\u062f\u062f\u0627\u062a \u0627\u0644\u0646\u0633\u0628\u064a\u0629. \u0627\u0642\u0631\u0623 \u0627\u0644\u0645\u0632\u064a\u062f \u0641\u064a\n`\u062f\u0644\u064a\u0644 \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645 <Generalized_linear_regression>`. \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0630\u0644\u0643\u060c\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor` \u064a\u062f\u0639\u0645 \u0627\u0644\u0622\u0646\n'poisson' \u0643\u062e\u0633\u0627\u0631\u0629 \u0623\u064a\u0636\u064b\u0627.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nn_samples, n_features = 1000, 20\nrng = np.random.RandomState(0)\nX = rng.randn(n_samples, n_features)\n# \u0627\u0644\u0647\u062f\u0641 \u0627\u0644\u0625\u064a\u062c\u0627\u0628\u064a \u0644\u0644\u0645\u062a\u063a\u064a\u0631\u0627\u062a \u0627\u0644\u0635\u062d\u064a\u062d\u0629 \u0645\u0631\u062a\u0628\u0637 \u0628\u0640 X[:, 5] \u0645\u0639 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0623\u0635\u0641\u0627\u0631:\ny = rng.poisson(lam=np.exp(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\nglm = PoissonRegressor()\ngbdt = HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.01)\nglm.fit(X_train, y_train)\ngbdt.fit(X_train, y_train)\nprint(glm.score(X_test, y_test))\nprint(gbdt.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u062a\u0645\u062b\u064a\u0644 \u0645\u0631\u0626\u064a \u063a\u0646\u064a \u0644\u0644\u0645\u0642\u062f\u0631\u0627\u062a\n\u064a\u0645\u0643\u0646 \u0627\u0644\u0622\u0646 \u062a\u0635\u0648\u0631 \u0627\u0644\u0645\u0642\u062f\u0631\u0627\u062a \u0641\u064a \u0627\u0644\u062f\u0641\u0627\u062a\u0631 \u0645\u0646 \u062e\u0644\u0627\u0644 \u062a\u0645\u0643\u064a\u0646 \u062e\u064a\u0627\u0631\n`display='diagram'` . \u0647\u0630\u0627 \u0645\u0641\u064a\u062f \u0628\u0634\u0643\u0644 \u062e\u0627\u0635 \u0644\u062a\u0644\u062e\u064a\u0635 \u0628\u0646\u064a\u0629 \u0627\u0644\u0623\u0646\u0627\u0628\u064a\u0628 \u0648\u0627\u0644\u0645\u0642\u062f\u0631\u0627\u062a\n\u0627\u0644\u0645\u0631\u0643\u0628\u0629 \u0627\u0644\u0623\u062e\u0631\u0649\u060c \u0645\u0639 \u0627\u0644\u062a\u0641\u0627\u0639\u0644 \u0644\u062a\u0648\u0641\u064a\u0631 \u0627\u0644\u062a\u0641\u0627\u0635\u064a\u0644. \u0627\u0646\u0642\u0631 \u0639\u0644\u0649 \u0627\u0644\u0635\u0648\u0631\u0629 \u0627\u0644\u062a\u0648\u0636\u064a\u062d\u064a\u0629\n\u0623\u062f\u0646\u0627\u0647 \u0644\u062a\u0648\u0633\u064a\u0639 \u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u0623\u0646\u0627\u0628\u064a\u0628. \u0631\u0627\u062c\u0639 `visualizing_composite_estimators`\n\u0644\u0645\u0639\u0631\u0641\u0629 \u0643\u064a\u0641\u064a\u0629 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0647\u0630\u0647 \u0627\u0644\u0645\u064a\u0632\u0629.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.linear_model import LogisticRegression\n\nset_config(display=\"diagram\")\n\nnum_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\ncat_proc = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(handle_unknown=\"ignore\"),\n)\n\npreprocessor = make_column_transformer(\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = make_pipeline(preprocessor, LogisticRegression())\nclf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u062a\u062d\u0633\u064a\u0646\u0627\u062a \u0641\u064a \u0642\u0627\u0628\u0644\u064a\u0629 \u0627\u0644\u062a\u0648\u0633\u0639 \u0648\u0627\u0644\u0627\u0633\u062a\u0642\u0631\u0627\u0631 \u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0629 KMeans\n\u062a\u0645 \u0625\u0639\u0627\u062f\u0629 \u062a\u0635\u0645\u064a\u0645 \u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0629 :class:`~sklearn.cluster.KMeans` \u0628\u0627\u0644\u0643\u0627\u0645\u0644\u060c \u0648\u0647\u064a \u0627\u0644\u0622\u0646\n\u0623\u0633\u0631\u0639 \u0648\u0623\u0643\u062b\u0631 \u0627\u0633\u062a\u0642\u0631\u0627\u0631\u064b\u0627 \u0628\u0634\u0643\u0644 \u0645\u0644\u062d\u0648\u0638. \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0630\u0644\u0643\u060c \u0623\u0635\u0628\u062d\u062a \u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0629 Elkan\n\u0645\u062a\u0648\u0627\u0641\u0642\u0629 \u0645\u0639 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0627\u062a \u0627\u0644\u0645\u062a\u0646\u0627\u062b\u0631\u0629. \u064a\u0633\u062a\u062e\u062f\u0645 \u0627\u0644\u0645\u0642\u062f\u0631 \u0645\u0648\u0627\u0632\u0627\u0629 \u0642\u0627\u0626\u0645\u0629 \u0639\u0644\u0649 OpenMP\n\u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0627\u0644\u0627\u0639\u062a\u0645\u0627\u062f \u0639\u0644\u0649 joblib\u060c \u0644\u0630\u0644\u0643 \u0644\u0645 \u064a\u0639\u062f \u0644\u062e\u064a\u0627\u0631 `n_jobs` \u0623\u064a \u062a\u0623\u062b\u064a\u0631. \u0644\u0644\u062d\u0635\u0648\u0644\n\u0639\u0644\u0649 \u0645\u0632\u064a\u062f \u0645\u0646 \u0627\u0644\u062a\u0641\u0627\u0635\u064a\u0644 \u062d\u0648\u0644 \u0643\u064a\u0641\u064a\u0629 \u0627\u0644\u062a\u062d\u0643\u0645 \u0641\u064a \u0639\u062f\u062f \u0627\u0644\u062e\u064a\u0648\u0637\u060c \u064a\u0631\u062c\u0649 \u0627\u0644\u0631\u062c\u0648\u0639 \u0625\u0644\u0649\n\u0645\u0644\u0627\u062d\u0638\u0627\u062a\u0646\u0627 \u062d\u0648\u0644 `\u0627\u0644\u0645\u0648\u0627\u0632\u0627\u0629 <parallelism>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scipy\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import completeness_score\n\nrng = np.random.RandomState(0)\nX, y = make_blobs(random_state=rng)\nX = scipy.sparse.csr_matrix(X)\nX_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\nkmeans = KMeans(n_init=\"auto\").fit(X_train)\nprint(completeness_score(kmeans.predict(X_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u062a\u062d\u0633\u064a\u0646\u0627\u062a \u0639\u0644\u0649 \u0627\u0644\u0645\u0642\u062f\u0631\u0627\u062a \u0627\u0644\u0642\u0627\u0626\u0645\u0629 \u0639\u0644\u0649 \u0627\u0644\u062a\u062f\u0631\u062c \u0627\u0644\u062a\u062f\u0631\u064a\u062c\u064a \u0644\u0644\u062a\u0639\u0632\u064a\u0632 \u0627\u0644\u062a\u062f\u0631\u064a\u062c\u064a\n\u062a\u0645 \u0625\u062c\u0631\u0627\u0621 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u062a\u062d\u0633\u064a\u0646\u0627\u062a \u0639\u0644\u0649\n:class:`~sklearn.ensemble.HistGradientBoostingClassifier` \u0648\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor`. \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649\n\u062e\u0633\u0627\u0631\u0629 \u0628\u0648\u0627\u0633\u0648\u0646 \u0627\u0644\u0645\u0630\u0643\u0648\u0631\u0629 \u0623\u0639\u0644\u0627\u0647\u060c \u062a\u062f\u0639\u0645 \u0647\u0630\u0647 \u0627\u0644\u0645\u0642\u062f\u0631\u0627\u062a \u0627\u0644\u0622\u0646 `\u0623\u0648\u0632\u0627\u0646 \u0627\u0644\u0639\u064a\u0646\u0627\u062a\n<sw_hgbdt>`. \u0623\u064a\u0636\u064b\u0627\u060c \u062a\u0645 \u0625\u0636\u0627\u0641\u0629 \u0645\u0639\u064a\u0627\u0631 \u0625\u064a\u0642\u0627\u0641 \u0645\u0628\u0643\u0631 \u062a\u0644\u0642\u0627\u0626\u064a: \u064a\u062a\u0645 \u062a\u0645\u0643\u064a\u0646 \u0627\u0644\u0625\u064a\u0642\u0627\u0641 \u0627\u0644\u0645\u0628\u0643\u0631\n\u0628\u0634\u0643\u0644 \u0627\u0641\u062a\u0631\u0627\u0636\u064a \u0639\u0646\u062f\u0645\u0627 \u064a\u062a\u062c\u0627\u0648\u0632 \u0639\u062f\u062f \u0627\u0644\u0639\u064a\u0646\u0627\u062a 10 \u0622\u0644\u0627\u0641. \u0623\u062e\u064a\u0631\u064b\u0627\u060c \u064a\u0645\u0643\u0646 \u0644\u0644\u0645\u0633\u062a\u062e\u062f\u0645\u064a\u0646 \u0627\u0644\u0622\u0646\n\u062a\u062d\u062f\u064a\u062f `\u0642\u064a\u0648\u062f \u0623\u062d\u0627\u062f\u064a\u0629 \u0627\u0644\u0627\u062a\u062c\u0627\u0647 <monotonic_cst_gbdt>` \u0644\u062a\u0642\u064a\u064a\u062f \u0627\u0644\u062a\u0648\u0642\u0639\u0627\u062a \u0628\u0646\u0627\u0621\u064b\n\u0639\u0644\u0649 \u062a\u063a\u064a\u0631\u0627\u062a \u0645\u064a\u0632\u0627\u062a \u0645\u062d\u062f\u062f\u0629. \u0641\u064a \u0627\u0644\u0645\u062b\u0627\u0644 \u0627\u0644\u062a\u0627\u0644\u064a\u060c \u0646\u0642\u0648\u0645 \u0628\u0628\u0646\u0627\u0621 \u0647\u062f\u0641 \u0645\u0631\u062a\u0628\u0637 \u0628\u0634\u0643\u0644 \u0639\u0627\u0645\n\u0628\u0627\u0644\u0645\u062a\u063a\u064a\u0631 \u0627\u0644\u0623\u0648\u0644\u060c \u0645\u0639 \u0628\u0639\u0636 \u0627\u0644\u0636\u062c\u064a\u062c. \u064a\u0633\u0645\u062d \u062a\u0637\u0628\u064a\u0642 \u0627\u0644\u0642\u064a\u0648\u062f \u0627\u0644\u0623\u062d\u0627\u062f\u064a\u0629 \u0627\u0644\u0627\u062a\u062c\u0627\u0647\n\u0628\u0627\u0644\u062a\u0648\u0642\u0639\u0627\u062a \u0628\u0627\u0644\u062a\u0642\u0627\u0637 \u0627\u0644\u062a\u0623\u062b\u064a\u0631 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0645\u062a\u063a\u064a\u0631 \u0627\u0644\u0623\u0648\u0644\u060c \u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0645\u0644\u0627\u0621\u0645\u0629 \u0627\u0644\u0636\u062c\u064a\u062c.\n\u0644\u0645\u062b\u0627\u0644 \u0639\u0644\u0649 \u0627\u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645\u060c \u0631\u0627\u062c\u0639 `sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# from sklearn.inspection import plot_partial_dependence\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nn_samples = 500\nrng = np.random.RandomState(0)\nX = rng.randn(n_samples, 2)\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\ny = 5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise\n\ngbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\ngbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# disp = plot_partial_dependence(\ndisp = PartialDependenceDisplay.from_estimator(\n    gbdt_no_cst,\n    X,\n    features=[0],\n    feature_names=[\"feature 0\"],\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n)\n# plot_partial_dependence(\nPartialDependenceDisplay.from_estimator(\n    gbdt_cst,\n    X,\n    features=[0],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\ndisp.axes_[0, 0].plot(\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n)\ndisp.axes_[0, 0].set_ylim(-3, 3)\ndisp.axes_[0, 0].set_xlim(-1, 1)\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u062f\u0639\u0645 \u0623\u0648\u0632\u0627\u0646 \u0627\u0644\u0639\u064a\u0646\u0627\u062a \u0644\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u062a\u064a Lasso \u0648 ElasticNet\n\u062e\u0648\u0627\u0631\u0632\u0645\u064a\u062a\u0627 \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0627\u0644\u062e\u0637\u064a :class:`~sklearn.linear_model.Lasso` \u0648\n:class:`~sklearn.linear_model.ElasticNet` \u062a\u062f\u0639\u0645\u0627\u0646 \u0627\u0644\u0622\u0646 \u0623\u0648\u0632\u0627\u0646 \u0627\u0644\u0639\u064a\u0646\u0627\u062a.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n    X, y, sample_weight, random_state=rng\n)\nreg = Lasso()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}