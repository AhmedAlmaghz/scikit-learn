{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the scikit-learn examples in JupyterLite is experimental and you may encounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import sklearn` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/scikit-learn/scikit-learn/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u0645\u0642\u0627\u0631\u0646\u0629 \u0628\u064a\u0646 FeatureHasher \u0648 DictVectorizer\n\n\u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0645\u062b\u0627\u0644\u060c \u0646\u0648\u0636\u062d \u0639\u0645\u0644\u064a\u0629 \u062a\u0645\u062b\u064a\u0644 \u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0646\u0635\u0648\u0635\u060c \u0648\u0647\u064a \u0639\u0645\u0644\u064a\u0629 \u062a\u0645\u062b\u064a\u0644 \u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0625\u062f\u062e\u0627\u0644 \u063a\u064a\u0631 \u0627\u0644\u0631\u0642\u0645\u064a\u0629 (\u0645\u062b\u0644 \u0627\u0644\u0642\u0648\u0627\u0645\u064a\u0633 \u0623\u0648 \u0648\u062b\u0627\u0626\u0642 \u0627\u0644\u0646\u0635\u0648\u0635) \u0643\u0645\u062a\u062c\u0647\u0627\u062a \u0645\u0646 \u0627\u0644\u0623\u0639\u062f\u0627\u062f \u0627\u0644\u062d\u0642\u064a\u0642\u064a\u0629.\n\n\u0646\u0642\u0627\u0631\u0646 \u0623\u0648\u0644\u0627\u064b \u0628\u064a\u0646 :func:`~sklearn.feature_extraction.FeatureHasher` \u0648\n:func:`~sklearn.feature_extraction.DictVectorizer` \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0643\u0644\u062a\u0627 \u0627\u0644\u0637\u0631\u064a\u0642\u062a\u064a\u0646 \u0644\u062a\u0645\u062b\u064a\u0644 \u0646\u0627\u0642\u0644\u0627\u062a \u0648\u062b\u0627\u0626\u0642 \u0627\u0644\u0646\u0635\u0648\u0635 \u0627\u0644\u062a\u064a \u062a\u062a\u0645 \u0645\u0639\u0627\u0644\u062c\u062a\u0647\u0627 \u0645\u0633\u0628\u0642\u064b\u0627 (\u062a\u062c\u0632\u064a\u0626\u0647\u0627) \u0628\u0645\u0633\u0627\u0639\u062f\u0629 \u062f\u0627\u0644\u0629 \u0628\u0627\u064a\u062b\u0648\u0646 \u0645\u062e\u0635\u0635\u0629.\n\n\u0641\u064a\u0645\u0627 \u0628\u0639\u062f\u060c \u0646\u0642\u062f\u0645 \u0648\u0646\u062d\u0644\u0644 \u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0646\u0635\u0648\u0635 \u0627\u0644\u0645\u062e\u0635\u0635\u0629:\n:func:`~sklearn.feature_extraction.text.HashingVectorizer`\u060c\n:func:`~sklearn.feature_extraction.text.CountVectorizer` \u0648\n:func:`~sklearn.feature_extraction.text.TfidfVectorizer` \u0627\u0644\u062a\u064a \u062a\u062a\u0639\u0627\u0645\u0644 \u0645\u0639 \u0643\u0644 \u0645\u0646 \u062a\u062c\u0632\u064a\u0621 \u0627\u0644\u0646\u0635\u0648\u0635 \u0648\u062a\u062c\u0645\u064a\u0639 \u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u062e\u0635\u0627\u0626\u0635 \u0636\u0645\u0646 \u0641\u0626\u0629 \u0648\u0627\u062d\u062f\u0629.\n\n\u0647\u062f\u0641 \u0627\u0644\u0645\u062b\u0627\u0644 \u0647\u0648 \u062a\u0648\u0636\u064a\u062d \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0648\u0627\u062c\u0647\u0629 \u0628\u0631\u0645\u062c\u0629 \u0627\u0644\u062a\u0637\u0628\u064a\u0642\u0627\u062a (API) \u0644\u062a\u0645\u062b\u064a\u0644 \u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0646\u0635\u0648\u0635 \u0648\u0645\u0642\u0627\u0631\u0646\u0629 \u0623\u0648\u0642\u0627\u062a \u0645\u0639\u0627\u0644\u062c\u062a\u0647\u0627. \u0631\u0627\u062c\u0639 \u0646\u0635\u0648\u0635 \u0627\u0644\u0623\u0645\u062b\u0644\u0629\n`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n\u0648 `sphx_glr_auto_examples_text_plot_document_clustering.py` \u0644\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0641\u0639\u0644\u064a \u0639\u0644\u0649 \u0648\u062b\u0627\u0626\u0642 \u0627\u0644\u0646\u0635\u0648\u0635.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u0627\u0644\u0645\u0624\u0644\u0641\u0648\u0646: \u0645\u0637\u0648\u0631\u064a \u0633\u0643\u0627\u064a\u0644\u0631\u0646\n# \u0645\u0639\u0631\u0641 \u0627\u0644\u062a\u0631\u062e\u064a\u0635: BSD-3-Clause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u062a\u062d\u0645\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\n\n\u0646\u0642\u0648\u0645 \u0628\u062a\u062d\u0645\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0645\u0646 `20newsgroups_dataset`\u060c \u0648\u0627\u0644\u062a\u064a \u062a\u062a\u0636\u0645\u0646 \u062d\u0648\u0627\u0644\u064a\n18000 \u0645\u0646\u0634\u0648\u0631\u0627\u062a \u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0627\u0644\u0623\u062e\u0628\u0627\u0631 \u062d\u0648\u0644 20 \u0645\u0648\u0636\u0648\u0639\u064b\u0627 \u0645\u0642\u0633\u0645\u0629 \u0625\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u062a\u064a\u0646: \u0648\u0627\u062d\u062f\u0629 \u0644\u0644\u062a\u062f\u0631\u064a\u0628\n\u0648\u0627\u0644\u0623\u062e\u0631\u0649 \u0644\u0644\u0627\u062e\u062a\u0628\u0627\u0631. \u0645\u0646 \u0623\u062c\u0644 \u0627\u0644\u0628\u0633\u0627\u0637\u0629 \u0648\u062a\u0642\u0644\u064a\u0644 \u0627\u0644\u062a\u0643\u0644\u0641\u0629 \u0627\u0644\u062d\u0633\u0627\u0628\u064a\u0629\u060c \u0646\u062e\u062a\u0627\u0631 \u0645\u062c\u0645\u0648\u0639\u0629 \u0641\u0631\u0639\u064a\u0629 \u0645\u0646 7 \u0645\u0648\u0627\u0636\u064a\u0639 \u0648\u0646\u0633\u062a\u062e\u062f\u0645 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628 \u0641\u0642\u0637.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n\ncategories = [\n    \"alt.atheism\",\n    \"comp.graphics\",\n    \"comp.sys.ibm.pc.hardware\",\n    \"misc.forsale\",\n    \"rec.autos\",\n    \"sci.space\",\n    \"talk.religion.misc\",\n]\n\nprint(\"\u062a\u062d\u0645\u064a\u0644 \u0628\u064a\u0627\u0646\u0627\u062a \u062a\u062f\u0631\u064a\u0628 \u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0627\u0644\u0623\u062e\u0628\u0627\u0631 20\")\nraw_data, _ = fetch_20newsgroups(subset=\"train\", categories=categories, return_X_y=True)\ndata_size_mb = sum(len(s.encode(\"utf-8\")) for s in raw_data) / 1e6\nprint(f\"{len(raw_data)} \u0648\u062b\u0627\u0626\u0642 - {data_size_mb:.3f}MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u062a\u062d\u062f\u064a\u062f \u062f\u0627\u0644\u0627\u062a \u0645\u0627 \u0642\u0628\u0644 \u0627\u0644\u0645\u0639\u0627\u0644\u062c\u0629\n\n\u0642\u062f \u064a\u0643\u0648\u0646 \u0627\u0644\u0631\u0645\u0632 \u0639\u0628\u0627\u0631\u0629 \u0639\u0646 \u0643\u0644\u0645\u0629\u060c \u0623\u0648 \u062c\u0632\u0621 \u0645\u0646 \u0643\u0644\u0645\u0629\u060c \u0623\u0648 \u0623\u064a \u0634\u064a\u0621 \u064a\u0642\u0639 \u0628\u064a\u0646 \u0627\u0644\u0645\u0633\u0627\u0641\u0627\u062a \u0623\u0648\n\u0627\u0644\u0631\u0645\u0648\u0632 \u0641\u064a \u0633\u0644\u0633\u0644\u0629. \u0647\u0646\u0627\u060c \u0646\u062d\u062f\u062f \u062f\u0627\u0644\u0629 \u062a\u0633\u062a\u062e\u0631\u062c \u0627\u0644\u0631\u0645\u0648\u0632 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645\n\u062a\u0639\u0628\u064a\u0631 \u0639\u0627\u062f\u064a (regex) \u0628\u0633\u064a\u0637 \u064a\u0637\u0627\u0628\u0642 \u062d\u0631\u0648\u0641 \u0627\u0644\u0643\u0644\u0645\u0627\u062a. \u0648\u0647\u0630\u0627 \u064a\u0634\u0645\u0644 \u0645\u0639\u0638\u0645 \u0627\u0644\u0631\u0645\u0648\u0632 \u0627\u0644\u062a\u064a \u064a\u0645\u0643\u0646 \u0623\u0646 \u062a\u0643\u0648\u0646 \u062c\u0632\u0621\u064b\u0627 \u0645\u0646 \u0643\u0644\u0645\u0629 \u0641\u064a \u0623\u064a \u0644\u063a\u0629\u060c\n\u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0627\u0644\u0623\u0631\u0642\u0627\u0645 \u0648\u062e\u0637 \u0627\u0644\u062a\u062d\u062a\u064a\u0629:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import re\n\n\ndef tokenize(doc):\n    \"\"\"\u0627\u0633\u062a\u062e\u0631\u0627\u062c \u0627\u0644\u0631\u0645\u0648\u0632 \u0645\u0646 doc.\n\n    \u064a\u0633\u062a\u062e\u062f\u0645 \u0647\u0630\u0627 \u062a\u0639\u0628\u064a\u0631\u064b\u0627 \u0639\u0627\u062f\u064a\u064b\u0627 \u0628\u0633\u064a\u0637\u064b\u0627 \u064a\u0637\u0627\u0628\u0642 \u062d\u0631\u0648\u0641 \u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0644\u062a\u0642\u0633\u064a\u0645 \u0627\u0644\u0633\u0644\u0627\u0633\u0644\n    \u0625\u0644\u0649 \u0631\u0645\u0648\u0632. \u0644\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0646\u0647\u062c \u0623\u0643\u062b\u0631 \u0645\u0628\u062f\u0623\u064a\u0629\u060c \u0631\u0627\u062c\u0639 CountVectorizer \u0623\u0648\n    TfidfVectorizer.\n    \"\"\"\n    return (tok.lower() for tok in re.findall(r\"\\w+\", doc))\n\n\nlist(tokenize(\"This is a simple example, isn't it?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0646\u062d\u062f\u062f \u062f\u0627\u0644\u0629 \u0625\u0636\u0627\u0641\u064a\u0629 \u062a\u062d\u0633\u0628 (\u062a\u0643\u0631\u0627\u0631) \u062d\u062f\u0648\u062b\n\u0643\u0644 \u0631\u0645\u0632 \u0641\u064a \u0648\u062b\u064a\u0642\u0629 \u0645\u0639\u064a\u0646\u0629. \u062a\u0639\u064a\u062f \u062f\u0627\u0644\u0629 \u0642\u0627\u0645\u0648\u0633 \u0627\u0644\u062a\u0631\u062f\u062f\u0627\u062a \u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647\u0627\n\u0628\u0648\u0627\u0633\u0637\u0629 \u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0646\u0635\u0648\u0635.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\n\ndef token_freqs(doc):\n    \"\"\"\u0627\u0633\u062a\u062e\u0631\u0627\u062c \u0642\u0627\u0645\u0648\u0633 \u064a\u064f\u0645\u064e\u062b\u0650\u0644 \u062e\u0631\u064a\u0637\u0629 \u0645\u0646 \u0627\u0644\u0631\u0645\u0648\u0632 \u0641\u064a doc \u0625\u0644\u0649 \u062a\u0643\u0631\u0627\u0631\u0627\u062a\u0647\u0627.\"\"\"\n\n    freq = defaultdict(int)\n    for tok in tokenize(doc):\n        freq[tok] += 1\n    return freq\n\n\ntoken_freqs(\"That is one example, but this is another one\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0644\u0627\u062d\u0638 \u0639\u0644\u0649 \u0648\u062c\u0647 \u0627\u0644\u062e\u0635\u0648\u0635 \u0623\u0646 \u0627\u0644\u0631\u0645\u0632 \u0627\u0644\u0645\u062a\u0643\u0631\u0631 `\"is\"[ \u064a\u062a\u0645 \u062d\u0633\u0627\u0628\u0647 \u0645\u0631\u062a\u064a\u0646 \u0639\u0644\u0649 \u0633\u0628\u064a\u0644\n\u0627\u0644\u0645\u062b\u0627\u0644.\n\n\u0625\u0646 \u062a\u0642\u0633\u064a\u0645 \u0648\u062b\u064a\u0642\u0629 \u0646\u0635\u064a\u0629 \u0625\u0644\u0649 \u0631\u0645\u0648\u0632 \u0643\u0644\u0645\u0627\u062a\u060c \u0645\u0639 \u0641\u0642\u062f\u0627\u0646 \u0645\u062d\u062a\u0645\u0644 \u0644\u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0627\u0644\u062a\u0631\u062a\u064a\u0628\n\u0628\u064a\u0646 \u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0641\u064a \u062c\u0645\u0644\u0629\u060c \u064a\u064f\u0637\u0644\u0642 \u0639\u0644\u064a\u0647 \u063a\u0627\u0644\u0628\u064b\u0627 \u062a\u0645\u062b\u064a\u0644 \"\u062d\u0642\u064a\u0628\u0629 \u0627\u0644\u0643\u0644\u0645\u0627\u062a](https://en.wikipedia.org/wiki/Bag-of-words_model)\".\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DictVectorizer\n\n\u0623\u0648\u0644\u0627\u064b\u060c \u0646\u0642\u0648\u0645 \u0628\u0627\u062e\u062a\u0628\u0627\u0631 \u0623\u062f\u0627\u0621 :func:`~sklearn.feature_extraction.DictVectorizer`\u060c\n\u062b\u0645 \u0646\u0642\u0627\u0631\u0646\u0647 \u0628\u0640 :func:`~sklearn.feature_extraction.FeatureHasher` \u062d\u064a\u062b \u0623\u0646 \u0643\u0644\u0627\u0647\u0645\u0627\n\u064a\u0633\u062a\u0642\u0628\u0644 \u0627\u0644\u0642\u0648\u0627\u0645\u064a\u0633 \u0643\u0645\u062f\u062e\u0644\u0627\u062a.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\n\nfrom sklearn.feature_extraction import DictVectorizer\n\ndict_count_vectorizers = defaultdict(list)\n\nt0 = time()\nvectorizer = DictVectorizer()\nvectorizer.fit_transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\ndict_count_vectorizers[\"vectorizer\"].append(\n    vectorizer.__class__.__name__ + \"\\non freq dicts\"\n)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u064a\u062a\u0645 \u062a\u062e\u0632\u064a\u0646 \u0627\u0644\u062e\u0631\u064a\u0637\u0629 \u0627\u0644\u0641\u0639\u0644\u064a\u0629 \u0645\u0646 \u0631\u0645\u0648\u0632 \u0627\u0644\u0646\u0635 \u0625\u0644\u0649 \u0641\u0647\u0631\u0633 \u0627\u0644\u0639\u0645\u0648\u062f \u0628\u0634\u0643\u0644 \u0635\u0631\u064a\u062d \u0641\u064a\n\u0633\u0645\u0629 `.vocabulary_` \u0648\u0627\u0644\u062a\u064a \u0647\u064a \u0639\u0628\u0627\u0631\u0629 \u0639\u0646 \u0642\u0627\u0645\u0648\u0633 \u0628\u0627\u064a\u062b\u0648\u0646 \u0643\u0628\u064a\u0631 \u0645\u062d\u062a\u0645\u0644:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "type(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "len(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.vocabulary_[\"example\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FeatureHasher\n\n\u062a\u0623\u062e\u0630 \u0627\u0644\u0642\u0648\u0627\u0645\u064a\u0633 \u0645\u0633\u0627\u062d\u0629 \u062a\u062e\u0632\u064a\u0646 \u0643\u0628\u064a\u0631\u0629 \u0648\u062a\u0646\u0645\u0648 \u0641\u064a \u0627\u0644\u062d\u062c\u0645 \u0645\u0639 \u0646\u0645\u0648 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628. \u0628\u062f\u0644\u0627\u064b \u0645\u0646\n\u0632\u064a\u0627\u062f\u0629 \u062d\u062c\u0645 \u0627\u0644\u0645\u062a\u062c\u0647\u0627\u062a \u0645\u0639 \u0627\u0644\u0642\u0627\u0645\u0648\u0633\u060c \u064a\u0642\u0648\u0645 \u062a\u0645\u062b\u064a\u0644 \u0627\u0644\u062e\u0635\u0627\u0626\u0635 \u0627\u0644\u0647\u0627\u0634\u0629 \u0628\u0628\u0646\u0627\u0621 \u0645\u062a\u062c\u0647 \u0645\u0646\n\u0637\u0648\u0644 \u0645\u062d\u062f\u062f \u0645\u0633\u0628\u0642\u064b\u0627 \u0639\u0646 \u0637\u0631\u064a\u0642 \u062a\u0637\u0628\u064a\u0642 \u062f\u0627\u0644\u0629 \u0647\u0627\u0634 `h` \u0639\u0644\u0649 \u0627\u0644\u062e\u0635\u0627\u0626\u0635 (\u0645\u062b\u0644 \u0627\u0644\u0631\u0645\u0648\u0632)\u060c \u062b\u0645\n\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0642\u064a\u0645 \u0627\u0644\u0647\u0627\u0634 \u0645\u0628\u0627\u0634\u0631\u0629\u064b \u0643\u0641\u0647\u0631\u0633 \u0644\u0644\u062e\u0635\u0627\u0626\u0635 \u0648\u062a\u062d\u062f\u064a\u062b \u0627\u0644\u0645\u062a\u062c\u0647 \u0627\u0644\u0646\u0627\u062a\u062c \u0639\u0646\u062f \u062a\u0644\u0643\n\u0627\u0644\u0641\u0647\u0627\u0631\u0633. \u0639\u0646\u062f\u0645\u0627 \u0644\u0627 \u062a\u0643\u0648\u0646 \u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u062e\u0635\u0627\u0626\u0635 \u0643\u0628\u064a\u0631\u0629 \u0628\u0645\u0627 \u064a\u0643\u0641\u064a\u060c \u062a\u0645\u064a\u0644 \u062f\u0627\u0644\u0627\u062a \u0627\u0644\u0647\u0627\u0634 \u0625\u0644\u0649\n\u062a\u0639\u064a\u064a\u0646 \u0642\u064a\u0645 \u0645\u062a\u0645\u064a\u0632\u0629 \u0625\u0644\u0649 \u0646\u0641\u0633 \u0643\u0648\u062f \u0627\u0644\u0647\u0627\u0634 (\u0627\u0635\u0637\u062f\u0627\u0645\u0627\u062a \u0627\u0644\u0647\u0627\u0634). \u0648\u0646\u062a\u064a\u062c\u0629 \u0644\u0630\u0644\u0643\u060c \u0645\u0646\n\u0627\u0644\u0645\u0633\u062a\u062d\u064a\u0644 \u062a\u062d\u062f\u064a\u062f \u0627\u0644\u0643\u0627\u0626\u0646 \u0627\u0644\u0630\u064a \u0623\u0646\u062a\u062c \u0623\u064a \u0643\u0648\u062f \u0647\u0627\u0634 \u0645\u0639\u064a\u0646.\n\n\u0628\u0633\u0628\u0628 \u0645\u0627 \u0633\u0628\u0642\u060c \u0645\u0646 \u0627\u0644\u0645\u0633\u062a\u062d\u064a\u0644 \u0627\u0633\u062a\u0639\u0627\u062f\u0629 \u0627\u0644\u0631\u0645\u0648\u0632 \u0627\u0644\u0623\u0635\u0644\u064a\u0629 \u0645\u0646 \u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u062e\u0635\u0627\u0626\u0635\u060c \u0648\u0623\u0641\u0636\u0644\n\u0646\u0647\u062c \u0644\u062a\u0642\u062f\u064a\u0631 \u0639\u062f\u062f \u0627\u0644\u0645\u0635\u0637\u0644\u062d\u0627\u062a \u0627\u0644\u0641\u0631\u064a\u062f\u0629 \u0641\u064a \u0627\u0644\u0642\u0627\u0645\u0648\u0633 \u0627\u0644\u0623\u0635\u0644\u064a \u0647\u0648 \u062d\u0633\u0627\u0628 \u0639\u062f\u062f \u0627\u0644\u0623\u0639\u0645\u062f\u0629\n\u0627\u0644\u0646\u0634\u0637\u0629 \u0641\u064a \u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u062e\u0635\u0627\u0626\u0635 \u0627\u0644\u0645\u0634\u0641\u0631\u0629. \u0644\u0647\u0630\u0627 \u0627\u0644\u063a\u0631\u0636\u060c \u0646\u062d\u062f\u062f \u0627\u0644\u062f\u0627\u0644\u0629 \u0627\u0644\u062a\u0627\u0644\u064a\u0629:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n\ndef n_nonzero_columns(X):\n    \"\"\"\u0639\u062f\u062f \u0627\u0644\u0623\u0639\u0645\u062f\u0629 \u0627\u0644\u062a\u064a \u062a\u062d\u062a\u0648\u064a \u0639\u0644\u0649 \u0642\u064a\u0645\u0629 \u063a\u064a\u0631 \u0635\u0641\u0631\u064a\u0629 \u0648\u0627\u062d\u062f\u0629 \u0639\u0644\u0649 \u0627\u0644\u0623\u0642\u0644 \u0641\u064a \u0645\u0635\u0641\u0648\u0641\u0629 CSR.\n\n    \u0647\u0630\u0627 \u0645\u0641\u064a\u062f \u0644\u062d\u0633\u0627\u0628 \u0639\u062f\u062f \u0623\u0639\u0645\u062f\u0629 \u0627\u0644\u062e\u0635\u0627\u0626\u0635 \u0627\u0644\u062a\u064a \u062a\u0643\u0648\u0646 \u0646\u0634\u0637\u0629 \u0628\u0634\u0643\u0644 \u0641\u0639\u0627\u0644 \u0639\u0646\u062f \u0627\u0633\u062a\u062e\u062f\u0627\u0645\n    FeatureHasher.\n    \"\"\"\n    return len(np.unique(X.nonzero()[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0627\u0644\u0639\u062f\u062f \u0627\u0644\u0627\u0641\u062a\u0631\u0627\u0636\u064a \u0644\u0644\u062e\u0635\u0627\u0626\u0635 \u0644\u0640\n:func:`~sklearn.feature_extraction.FeatureHasher` \u0647\u0648 2**20. \u0647\u0646\u0627 \u0646\u062d\u062f\u062f\n`n_features = 2**18` \u0644\u062a\u0648\u0636\u064a\u062d \u0627\u0635\u0637\u062f\u0627\u0645\u0627\u062a \u0627\u0644\u0647\u0627\u0634.\n\n**FeatureHasher \u0639\u0644\u0649 \u0642\u0648\u0627\u0645\u064a\u0633 \u0627\u0644\u062a\u0631\u062f\u062f\u0627\u062a**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction import FeatureHasher\n\nt0 = time()\nhasher = FeatureHasher(n_features=2**18)\nX = hasher.transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\ndict_count_vectorizers[\"vectorizer\"].append(\n    hasher.__class__.__name__ + \"\\non freq dicts\"\n)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0639\u062f\u062f \u0627\u0644\u0631\u0645\u0648\u0632 \u0627\u0644\u0641\u0631\u064a\u062f\u0629 \u0639\u0646\u062f \u0627\u0633\u062a\u062e\u062f\u0627\u0645\n:func:`~sklearn.feature_extraction.FeatureHasher` \u0623\u0642\u0644 \u0645\u0646 \u062a\u0644\u0643 \u0627\u0644\u062a\u064a \u062a\u0645 \u0627\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u064a\u0647\u0627\n\u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 :func:`~sklearn.feature_extraction.DictVectorizer`. \u0648\u064a\u0631\u062c\u0639 \u0630\u0644\u0643 \u0625\u0644\u0649\n\u0627\u0635\u0637\u062f\u0627\u0645\u0627\u062a \u0627\u0644\u0647\u0627\u0634.\n\n\u064a\u0645\u0643\u0646 \u062a\u0642\u0644\u064a\u0644 \u0639\u062f\u062f \u0627\u0644\u0627\u0635\u0637\u062f\u0627\u0645\u0627\u062a \u0639\u0646 \u0637\u0631\u064a\u0642 \u0632\u064a\u0627\u062f\u0629 \u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u062e\u0635\u0627\u0626\u0635. \u0644\u0627\u062d\u0638 \u0623\u0646 \u0633\u0631\u0639\u0629\n\u0646\u0627\u0642\u0644\u0629 \u0627\u0644\u062e\u0635\u0627\u0626\u0635 \u0644\u0627 \u062a\u062a\u063a\u064a\u0631 \u0628\u0634\u0643\u0644 \u0643\u0628\u064a\u0631 \u0639\u0646\u062f \u062a\u062d\u062f\u064a\u062f \u0639\u062f\u062f \u0643\u0628\u064a\u0631 \u0645\u0646 \u0627\u0644\u062e\u0635\u0627\u0626\u0635\u060c \u0639\u0644\u0649 \u0627\u0644\u0631\u063a\u0645\n\u0645\u0646 \u0623\u0646\u0647\u0627 \u062a\u0633\u0628\u0628 \u0623\u0628\u0639\u0627\u062f \u0645\u0639\u0627\u0645\u0644\u0627\u062a \u0623\u0643\u0628\u0631 \u0648\u062a\u062a\u0637\u0644\u0628 \u0628\u0627\u0644\u062a\u0627\u0644\u064a \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0630\u0627\u0643\u0631\u0629 \u0623\u0643\u0628\u0631 \u0644\u062a\u062e\u0632\u064a\u0646\u0647\u0627\u060c\n\u062d\u062a\u0649 \u0644\u0648 \u0643\u0627\u0646\u062a \u0623\u063a\u0644\u0628\u064a\u062a\u0647\u0627 \u063a\u064a\u0631 \u0646\u0634\u0637\u0629.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = time()\nhasher = FeatureHasher(n_features=2**22)\nX = hasher.transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\n\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0646\u0624\u0643\u062f \u0623\u0646 \u0639\u062f\u062f \u0627\u0644\u0631\u0645\u0648\u0632 \u0627\u0644\u0641\u0631\u064a\u062f\u0629 \u064a\u0642\u062a\u0631\u0628 \u0645\u0646 \u0639\u062f\u062f \u0627\u0644\u0645\u0635\u0637\u0644\u062d\u0627\u062a \u0627\u0644\u0641\u0631\u064a\u062f\u0629 \u0627\u0644\u062a\u064a \u0648\u062c\u062f\u0647\u0627\n:func:`~sklearn.feature_extraction.DictVectorizer`.\n\n**FeatureHasher \u0639\u0644\u0649 \u0627\u0644\u0631\u0645\u0648\u0632 \u0627\u0644\u062e\u0627\u0645**\n\n\u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0630\u0644\u0643\u060c \u064a\u0645\u0643\u0646\u0643 \u062a\u062d\u062f\u064a\u062f `input_type=\"string\"` \u0641\u064a\n:func:`~sklearn.feature_extraction.FeatureHasher` \u0644\u062a\u0645\u062b\u064a\u0644 \u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0633\u0644\u0627\u0633\u0644\n\u0627\u0644\u0646\u0627\u062a\u062c\u0629 \u0645\u0628\u0627\u0634\u0631\u0629 \u0645\u0646 \u062f\u0627\u0644\u0629 `tokenize` \u0627\u0644\u0645\u062e\u0635\u0635\u0629. \u0648\u0647\u0630\u0627 \u064a\u0639\u0627\u062f\u0644 \u062a\u0645\u0631\u064a\u0631 \u0642\u0627\u0645\u0648\u0633 \u0645\u0639\n\u062a\u0643\u0631\u0627\u0631 \u0636\u0645\u0646\u064a \u064a\u0633\u0627\u0648\u064a 1 \u0644\u0643\u0644 \u0627\u0633\u0645 \u062e\u0627\u0635\u064a\u0629.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = time()\nhasher = FeatureHasher(n_features=2**18, input_type=\"string\")\nX = hasher.transform(tokenize(d) for d in raw_data)\nduration = time() - t0\ndict_count_vectorizers[\"vectorizer\"].append(\n    hasher.__class__.__name__ + \"\\non raw tokens\"\n)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {n_nonzero_columns(X)} unique tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0646\u062d\u0646 \u0627\u0644\u0622\u0646 \u0646\u0631\u0633\u0645 \u0633\u0631\u0639\u0629 \u0627\u0644\u0637\u0631\u0642 \u0627\u0644\u0645\u0630\u0643\u0648\u0631\u0629 \u0623\u0639\u0644\u0627\u0647 \u0644\u062a\u0645\u062b\u064a\u0644 \u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0646\u0635\u0648\u0635.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\ny_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\nax.set_yticks(y_pos)\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\nax.invert_yaxis()\n_ = ax.set_xlabel(\"\u0627\u0644\u0633\u0631\u0639\u0629 (MB/s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0641\u064a \u0643\u0644\u062a\u0627 \u0627\u0644\u062d\u0627\u0644\u062a\u064a\u0646\u060c :func:`~sklearn.feature_extraction.FeatureHasher`\n\u0623\u0633\u0631\u0639 \u0645\u0631\u062a\u064a\u0646 \u062a\u0642\u0631\u064a\u0628\u064b\u0627 \u0645\u0646\n:func:`~sklearn.feature_extraction.DictVectorizer`. \u0648\u0647\u0630\u0627 \u0645\u0641\u064a\u062f \u0639\u0646\u062f \u0627\u0644\u062a\u0639\u0627\u0645\u0644\n\u0645\u0639 \u0643\u0645\u064a\u0627\u062a \u0643\u0628\u064a\u0631\u0629 \u0645\u0646 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\u060c \u0645\u0639 \u0627\u0644\u062c\u0627\u0646\u0628 \u0627\u0644\u0633\u0644\u0628\u064a \u0627\u0644\u0645\u062a\u0645\u062b\u0644 \u0641\u064a \u0641\u0642\u062f\u0627\u0646 \u0625\u0645\u0643\u0627\u0646\u064a\u0629\n\u0639\u0643\u0633 \u0627\u0644\u062a\u062d\u0648\u064a\u0644\u060c \u0645\u0645\u0627 \u064a\u062c\u0639\u0644 \u062a\u0641\u0633\u064a\u0631 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0645\u0647\u0645\u0629 \u0623\u0643\u062b\u0631 \u062a\u0639\u0642\u064a\u062f\u064b\u0627.\n\n\u0625\u0646 `FeatureHeasher` \u0645\u0639 `input_type=\"string\"` \u0623\u0633\u0631\u0639 \u0642\u0644\u064a\u0644\u0627\u064b \u0645\u0646 \u0627\u0644\u0645\u062a\u063a\u064a\u0631 \u0627\u0644\u0630\u064a\n\u064a\u0639\u0645\u0644 \u0639\u0644\u0649 \u0642\u0627\u0645\u0648\u0633 \u0627\u0644\u062a\u0631\u062f\u062f\u0627\u062a \u0644\u0623\u0646\u0647 \u0644\u0627 \u064a\u062d\u0633\u0628 \u0627\u0644\u0631\u0645\u0648\u0632 \u0627\u0644\u0645\u062a\u0643\u0631\u0631\u0629: \u064a\u062a\u0645 \u062d\u0633\u0627\u0628 \u0643\u0644 \u0631\u0645\u0632\n\u0636\u0645\u0646\u064a\u064b\u0627 \u0645\u0631\u0629 \u0648\u0627\u062d\u062f\u0629\u060c \u062d\u062a\u0649 \u0644\u0648 \u062a\u0643\u0631\u0631. \u0648\u0647\u0630\u0627 \u064a\u0639\u062a\u0645\u062f \u0639\u0644\u0649 \u0645\u0647\u0645\u0629 \u0627\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0622\u0644\u064a\n\u0627\u0644\u0646\u0647\u0627\u0626\u064a\u0629\u060c \u0641\u0642\u062f \u064a\u0643\u0648\u0646 \u0647\u0630\u0627 \u0639\u064a\u0628\u064b\u0627 \u0623\u0648 \u0644\u0627.\n\n## \u0627\u0644\u0645\u0642\u0627\u0631\u0646\u0629 \u0645\u0639 \u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0646\u0635\u0648\u0635 \u0627\u0644\u0645\u062e\u0635\u0635\u0629\n\n:func:`~sklearn.feature_extraction.text.CountVectorizer` \u064a\u0642\u0628\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u062e\u0627\u0645\n\u062d\u064a\u062b \u0623\u0646\u0647 \u064a\u0646\u0641\u0630 \u062a\u062c\u0632\u064a\u0621 \u0627\u0644\u0646\u0635\u0648\u0635 \u0648\u062d\u0633\u0627\u0628 \u0627\u0644\u062a\u0631\u062f\u062f\u0627\u062a \u062f\u0627\u062e\u0644\u064a\u064b\u0627. \u0648\u0647\u0648 \u0645\u0634\u0627\u0628\u0647\n\u0644\u0640 :func:`~sklearn.feature_extraction.DictVectorizer` \u0639\u0646\u062f \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647 \u0645\u0639\n\u062f\u0627\u0644\u0629 `token_freqs` \u0627\u0644\u0645\u062e\u0635\u0635\u0629 \u0643\u0645\u0627 \u0647\u0648 \u0627\u0644\u062d\u0627\u0644 \u0641\u064a \u0627\u0644\u0642\u0633\u0645 \u0627\u0644\u0633\u0627\u0628\u0642. \u0627\u0644\u0641\u0631\u0642 \u0647\u0648 \u0623\u0646\n:func:`~sklearn.feature_extraction.text.CountVectorizer` \u0623\u0643\u062b\u0631 \u0645\u0631\u0648\u0646\u0629.\n\u0639\u0644\u0649 \u0648\u062c\u0647 \u0627\u0644\u062e\u0635\u0648\u0635\u060c \u064a\u0642\u0628\u0644 \u0623\u0646\u0645\u0627\u0637 \u062a\u0639\u0628\u064a\u0631\u0627\u062a \u0639\u0627\u062f\u064a\u0629 \u0645\u062e\u062a\u0644\u0641\u0629 \u0645\u0646 \u062e\u0644\u0627\u0644 \u0645\u0639\u0627\u0645\u0644\n`token_pattern`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n\nt0 = time()\nvectorizer = CountVectorizer()\nvectorizer.fit_transform(raw_data)\nduration = time() - t0\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0646\u0644\u0627\u062d\u0638 \u0623\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 :func:`~sklearn.feature_extraction.text.CountVectorizer`\n\u0623\u0633\u0631\u0639 \u0645\u0631\u062a\u064a\u0646 \u062a\u0642\u0631\u064a\u0628\u064b\u0627 \u0645\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\n:func:`~sklearn.feature_extraction.DictVectorizer` \u0645\u0639 \u0627\u0644\u062f\u0627\u0644\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629 \u0627\u0644\u062a\u064a\n\u062d\u062f\u062f\u0646\u0627\u0647\u0627 \u0644\u062a\u0645\u062b\u064a\u0644 \u062e\u0631\u064a\u0637\u0629 \u0627\u0644\u0631\u0645\u0648\u0632. \u0648\u0627\u0644\u0633\u0628\u0628 \u0647\u0648 \u0623\u0646\n:func:`~sklearn.feature_extraction.text.CountVectorizer` \u064a\u062a\u0645 \u062a\u062d\u0633\u064a\u0646\u0647 \u0639\u0646 \u0637\u0631\u064a\u0642\n\u0625\u0639\u0627\u062f\u0629 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u062a\u0639\u0628\u064a\u0631 \u0639\u0627\u062f\u064a \u0645\u062c\u0645\u0639 \u0644\u0644\u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628\u064a\u0629 \u0628\u0627\u0644\u0643\u0627\u0645\u0644 \u0628\u062f\u0644\u0627\u064b \u0645\u0646\n\u0625\u0646\u0634\u0627\u0621 \u0648\u0627\u062d\u062f \u0644\u0643\u0644 \u0648\u062b\u064a\u0642\u0629 \u0643\u0645\u0627 \u0647\u0648 \u0627\u0644\u062d\u0627\u0644 \u0641\u064a \u062f\u0627\u0644\u0629 `tokenize` \u0627\u0644\u0628\u0633\u064a\u0637\u0629 \u0627\u0644\u062a\u064a \u062d\u062f\u062f\u0646\u0627\u0647\u0627.\n\n\u0627\u0644\u0622\u0646\u060c \u0646\u062c\u0631\u064a \u062a\u062c\u0631\u0628\u0629 \u0645\u0645\u0627\u062b\u0644\u0629 \u0645\u0639\n:func:`~sklearn.feature_extraction.text.HashingVectorizer`\u060c \u0648\u0627\u0644\u062a\u064a\n\u062a\u0639\u0627\u062f\u0644 \u0627\u0644\u062c\u0645\u0639 \u0628\u064a\u0646 \"\u062e\u062f\u0639\u0629 \u0627\u0644\u0647\u0627\u0634\" \u0627\u0644\u062a\u064a \u064a\u0646\u0641\u0630\u0647\u0627\n:func:`~sklearn.feature_extraction.FeatureHasher` \u0648\u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0646\u0635\u0648\u0635\n\u0648\u062a\u062c\u0632\u064a\u0626\u0647\u0627 \u0641\u064a\n:func:`~sklearn.feature_extraction.text.CountVectorizer`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n\nt0 = time()\nvectorizer = HashingVectorizer(n_features=2**18)\nvectorizer.fit_transform(raw_data)\nduration = time() - t0\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u064a\u0645\u0643\u0646\u0646\u0627 \u0645\u0644\u0627\u062d\u0638\u0629 \u0623\u0646 \u0647\u0630\u0647 \u0647\u064a \u0623\u0633\u0631\u0639 \u0627\u0633\u062a\u0631\u0627\u062a\u064a\u062c\u064a\u0629 \u0644\u062a\u062c\u0632\u064a\u0621 \u0627\u0644\u0646\u0635\u0648\u0635 \u062d\u062a\u0649 \u0627\u0644\u0622\u0646\u060c\n\u0628\u0627\u0641\u062a\u0631\u0627\u0636 \u0623\u0646 \u0645\u0647\u0645\u0629 \u0627\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0622\u0644\u064a \u0627\u0644\u0646\u0647\u0627\u0626\u064a\u0629 \u064a\u0645\u0643\u0646\u0647\u0627 \u062a\u062d\u0645\u0644 \u0628\u0639\u0636 \u0627\u0644\u0627\u0635\u0637\u062f\u0627\u0645\u0627\u062a.\n\n## TfidfVectorizer\n\n\u0641\u064a \u0645\u062c\u0645\u0648\u0639\u0629 \u0646\u0635\u0648\u0635 \u0643\u0628\u064a\u0631\u0629\u060c \u062a\u0638\u0647\u0631 \u0628\u0639\u0636 \u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0628\u062a\u0631\u062f\u062f \u0623\u0639\u0644\u0649 (\u0645\u062b\u0644 \"the\"\u060c\n\"a\"\u060c \"is\" \u0641\u064a \u0627\u0644\u0644\u063a\u0629 \u0627\u0644\u0625\u0646\u062c\u0644\u064a\u0632\u064a\u0629) \u0648\u0644\u0627 \u062a\u062d\u0645\u0644 \u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0630\u0627\u062a \u0645\u0639\u0646\u0649 \u062d\u0648\u0644 \u0627\u0644\u0645\u062d\u062a\u0648\u0649\n\u0627\u0644\u0641\u0639\u0644\u064a \u0644\u0648\u062b\u064a\u0642\u0629 \u0645\u0627. \u0625\u0630\u0627 \u0643\u0646\u0627 \u0633\u0646\u0642\u0648\u0645 \u0628\u062a\u063a\u0630\u064a\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u062a\u0643\u0631\u0627\u0631 \u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0645\u0628\u0627\u0634\u0631\u0629\u064b\n\u0625\u0644\u0649 \u0645\u0635\u0646\u0641\u060c \u0641\u0625\u0646 \u0647\u0630\u0647 \u0627\u0644\u0645\u0635\u0637\u0644\u062d\u0627\u062a \u0627\u0644\u0634\u0627\u0626\u0639\u0629 \u062c\u062f\u064b\u0627 \u0633\u062a\u0637\u063a\u0649 \u0639\u0644\u0649 \u062a\u0631\u062f\u062f\u0627\u062a \u0627\u0644\u0645\u0635\u0637\u0644\u062d\u0627\u062a\n\u0627\u0644\u0646\u0627\u062f\u0631\u0629 \u0648\u0644\u0643\u0646\u0647\u0627 \u0623\u0643\u062b\u0631 \u0625\u0641\u0627\u062f\u0629. \u0645\u0646 \u0623\u062c\u0644 \u0625\u0639\u0627\u062f\u0629 \u0648\u0632\u0646 \u0645\u064a\u0632\u0627\u062a \u062a\u0643\u0631\u0627\u0631 \u0627\u0644\u0643\u0644\u0645\u0627\u062a \u0625\u0644\u0649\n\u0642\u064a\u0645 \u0630\u0627\u062a \u0623\u0639\u062f\u0627\u062f \u0639\u0627\u0626\u0645\u0629 \u0645\u0646\u0627\u0633\u0628\u0629 \u0644\u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0628\u0648\u0627\u0633\u0637\u0629 \u0645\u0635\u0646\u0641\u060c \u0645\u0646 \u0627\u0644\u0634\u0627\u0626\u0639 \u062c\u062f\u064b\u0627\n\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u062a\u062d\u0648\u064a\u0644 tf-idf \u0643\u0645\u0627 \u064a\u0646\u0641\u0630\u0647\n:func:`~sklearn.feature_extraction.text.TfidfTransformer`. TF \u062a\u0639\u0646\u064a\n\"\u062a\u0631\u062f\u062f \u0627\u0644\u0645\u0635\u0637\u0644\u062d\" \u0628\u064a\u0646\u0645\u0627 \"tf-idf\" \u062a\u0639\u0646\u064a \u062a\u0631\u062f\u062f \u0627\u0644\u0645\u0635\u0637\u0644\u062d \u0645\u0636\u0631\u0648\u0628\u064b\u0627 \u0641\u064a \u0639\u0643\u0633\n\u062a\u0631\u062f\u062f \u0627\u0644\u0648\u062b\u064a\u0642\u0629.\n\n\u0646\u0642\u0648\u0645 \u0627\u0644\u0622\u0646 \u0628\u0627\u062e\u062a\u0628\u0627\u0631 \u0623\u062f\u0627\u0621 :func:`~sklearn.feature_extraction.text.TfidfVectorizer`\u060c\n\u0648\u0627\u0644\u0630\u064a \u064a\u0639\u0627\u062f\u0644 \u0627\u0644\u062c\u0645\u0639 \u0628\u064a\u0646 \u062a\u062c\u0632\u064a\u0621 \u0627\u0644\u0646\u0635\u0648\u0635 \u0648\u062d\u0633\u0627\u0628 \u062a\u0631\u062f\u062f\u0627\u062a\u0647\u0627 \u0641\u064a\n:func:`~sklearn.feature_extraction.text.CountVectorizer` \u0625\u0644\u0649 \u062c\u0627\u0646\u0628\n\u0627\u0644\u062a\u0637\u0628\u064a\u0639 \u0648\u0627\u0644\u0648\u0632\u0646 \u0645\u0646\n:func:`~sklearn.feature_extraction.text.TfidfTransformer`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n\nt0 = time()\nvectorizer = TfidfVectorizer()\nvectorizer.fit_transform(raw_data)\nduration = time() - t0\ndict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\ndict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\nprint(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\nprint(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0645\u0644\u062e\u0635\n\u062f\u0639\u0648\u0646\u0627 \u0646\u062e\u062a\u062a\u0645 \u0647\u0630\u0627 \u0627\u0644\u062f\u0641\u062a\u0631 \u0628\u062a\u0644\u062e\u064a\u0635 \u062c\u0645\u064a\u0639 \u0633\u0631\u0639\u0627\u062a \u0627\u0644\u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0645\u0633\u062c\u0644\u0629\n\u0641\u064a \u0645\u062e\u0637\u0637 \u0648\u0627\u062d\u062f:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 6))\n\ny_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\nax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\nax.set_yticks(y_pos)\nax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\nax.invert_yaxis()\n_ = ax.set_xlabel(\"\u0627\u0644\u0633\u0631\u0639\u0629 (MB/s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0644\u0627\u062d\u0638 \u0645\u0646 \u0627\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a \u0623\u0646\n:func:`~sklearn.feature_extraction.text.TfidfVectorizer` \u0623\u0628\u0637\u0623 \u0642\u0644\u064a\u0644\u0627\u064b\n\u0645\u0646 :func:`~sklearn.feature_extraction.text.CountVectorizer` \u0628\u0633\u0628\u0628\n\u0627\u0644\u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0625\u0636\u0627\u0641\u064a\u0629 \u0627\u0644\u0646\u0627\u062a\u062c\u0629 \u0639\u0646\n:func:`~sklearn.feature_extraction.text.TfidfTransformer`.\n\n\u0644\u0627\u062d\u0638 \u0623\u064a\u0636\u064b\u0627 \u0623\u0646\u0647 \u0645\u0646 \u062e\u0644\u0627\u0644 \u062a\u0639\u064a\u064a\u0646 \u0639\u062f\u062f \u0627\u0644\u0645\u064a\u0632\u0627\u062a `n_features = 2**18`\u060c \u0641\u0625\u0646\n:func:`~sklearn.feature_extraction.text.HashingVectorizer` \u064a\u0639\u0645\u0644 \u0628\u0634\u0643\u0644\n\u0623\u0641\u0636\u0644 \u0645\u0646 :func:`~sklearn.feature_extraction.text.CountVectorizer` \u0639\u0644\u0649\n\u062d\u0633\u0627\u0628 \u0627\u0646\u0639\u0643\u0627\u0633 \u0627\u0644\u062a\u062d\u0648\u064a\u0644 \u0628\u0633\u0628\u0628 \u062a\u0635\u0627\u062f\u0645\u0627\u062a \u0627\u0644\u062a\u062c\u0632\u0626\u0629.\n\n\u0646\u0633\u0644\u0637 \u0627\u0644\u0636\u0648\u0621 \u0639\u0644\u0649 \u0623\u0646 :func:`~sklearn.feature_extraction.text.CountVectorizer` \u0648\n:func:`~sklearn.feature_extraction.text.HashingVectorizer` \u064a\u0639\u0645\u0644\u0627\u0646 \u0628\u0634\u0643\u0644\n\u0623\u0641\u0636\u0644 \u0645\u0646 :func:`~sklearn.feature_extraction.DictVectorizer` \u0648\n:func:`~sklearn.feature_extraction.FeatureHasher` \u0639\u0644\u0649 \u0627\u0644\u0645\u0633\u062a\u0646\u062f\u0627\u062a\n\u0627\u0644\u0645\u0645\u064a\u0632\u0629 \u064a\u062f\u0648\u064a\u064b\u0627 \u0644\u0623\u0646 \u062e\u0637\u0648\u0629 \u0627\u0644\u062a\u0645\u064a\u064a\u0632 \u0627\u0644\u062f\u0627\u062e\u0644\u064a\u0629 \u0644\u0644\u0646\u0627\u0642\u0644\u0627\u062a \u0627\u0644\u0623\u0648\u0644\u0649\n\u062a\u0642\u0648\u0645 \u0628\u062a\u062c\u0645\u064a\u0639 \u062a\u0639\u0628\u064a\u0631 \u0639\u0627\u062f\u064a \u0645\u0631\u0629 \u0648\u0627\u062d\u062f\u0629 \u062b\u0645 \u062a\u0639\u064a\u062f \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647 \u0644\u062c\u0645\u064a\u0639 \u0627\u0644\u0645\u0633\u062a\u0646\u062f\u0627\u062a.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}