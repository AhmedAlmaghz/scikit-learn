{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the scikit-learn examples in JupyterLite is experimental and you may encounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import sklearn` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/scikit-learn/scikit-learn/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u062a\u0642\u062f\u064a\u0631\u0627\u062a Gradient Boosting Out-of-Bag\n\u064a\u0645\u0643\u0646 \u0623\u0646 \u062a\u0643\u0648\u0646 \u062a\u0642\u062f\u064a\u0631\u0627\u062a Out-of-Bag (OOB) \u0648\u0633\u064a\u0644\u0629 \u0645\u0641\u064a\u062f\u0629 \u0644\u062a\u0642\u062f\u064a\u0631 \u0627\u0644\u0639\u062f\u062f \"\u0627\u0644\u0623\u0645\u062b\u0644\" \u0644\u062f\u0648\u0631\u0627\u062a \u0627\u0644\u062a\u0639\u0632\u064a\u0632.\n\u062a\u062a\u0634\u0627\u0628\u0647 \u062a\u0642\u062f\u064a\u0631\u0627\u062a OOB \u062a\u0642\u0631\u064a\u0628\u064b\u0627 \u0645\u0639 \u062a\u0642\u062f\u064a\u0631\u0627\u062a \u0627\u0644\u062a\u062d\u0642\u0642 \u0627\u0644\u0645\u062a\u0642\u0627\u0637\u0639\u060c \u0648\u0644\u0643\u0646 \u064a\u0645\u0643\u0646 \u062d\u0633\u0627\u0628\u0647\u0627 \u0623\u062b\u0646\u0627\u0621 \u0627\u0644\u062a\u0646\u0641\u064a\u0630 \u062f\u0648\u0646 \u0627\u0644\u062d\u0627\u062c\u0629 \u0625\u0644\u0649 \u062a\u0643\u0631\u0627\u0631 \u0645\u0644\u0627\u0621\u0645\u0629 \u0627\u0644\u0646\u0645\u0648\u0630\u062c.\n\u062a\u062a\u0648\u0641\u0631 \u062a\u0642\u062f\u064a\u0631\u0627\u062a OOB \u0641\u0642\u0637 \u0644\u062a\u0639\u0632\u064a\u0632 \u0627\u0644\u062a\u062f\u0631\u062c \u0627\u0644\u0639\u0634\u0648\u0627\u0626\u064a\n(\u0623\u064a ``subsample < 1.0``)\u060c \u0648\u062a\u064f\u0633\u062a\u0645\u062f \u0627\u0644\u062a\u0642\u062f\u064a\u0631\u0627\u062a \u0645\u0646 \u0627\u0644\u062a\u062d\u0633\u0646 \u0641\u064a \u0627\u0644\u062e\u0633\u0627\u0631\u0629 \u0628\u0646\u0627\u0621\u064b \u0639\u0644\u0649 \u0627\u0644\u0623\u0645\u062b\u0644\u0629 \u063a\u064a\u0631 \u0627\u0644\u0645\u062f\u0631\u062c\u0629 \u0641\u064a \u0639\u064a\u0646\u0629 \u0627\u0644\u062a\u0645\u0647\u064a\u062f\n(\u0645\u0627 \u064a\u0633\u0645\u0649 \u0627\u0644\u0623\u0645\u062b\u0644\u0629 \u062e\u0627\u0631\u062c \u0627\u0644\u0643\u064a\u0633).\n\u0627\u0644\u0645\u0642\u062f\u0631 OOB \u0647\u0648 \u0645\u0642\u062f\u0631 \u0645\u062a\u0634\u0627\u0626\u0645 \u0644\u0644\u062e\u0633\u0627\u0631\u0629 \u0627\u0644\u062d\u0642\u064a\u0642\u064a\u0629 \u0644\u0644\u0627\u062e\u062a\u0628\u0627\u0631\u060c \u0648\u0644\u0643\u0646\u0647 \u064a\u0628\u0642\u0649 \u062a\u0642\u0631\u064a\u0628\u064b\u0627 \u062c\u064a\u062f\u064b\u0627 \u0644\u0639\u062f\u062f \u0635\u063a\u064a\u0631 \u0645\u0646 \u0627\u0644\u0623\u0634\u062c\u0627\u0631.\n\u064a\u0648\u0636\u062d \u0627\u0644\u0634\u0643\u0644 \u0627\u0644\u0645\u062c\u0645\u0648\u0639 \u0627\u0644\u062a\u0631\u0627\u0643\u0645\u064a \u0644\u0644\u062a\u062d\u0633\u064a\u0646\u0627\u062a \u0627\u0644\u0633\u0644\u0628\u064a\u0629 \u0644\u0640 OOB\n\u0643\u062f\u0627\u0644\u0629 \u0644\u062f\u0648\u0631\u0629 \u0627\u0644\u062a\u0639\u0632\u064a\u0632. \u0643\u0645\u0627 \u062a\u0631\u0649\u060c \u0641\u0625\u0646\u0647 \u064a\u062a\u062a\u0628\u0639 \u062e\u0633\u0627\u0631\u0629 \u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631 \u0644\u0644\u0645\u0627\u0626\u0629 \u062f\u0648\u0631\u0629 \u0627\u0644\u0623\u0648\u0644\u0649 \u0648\u0644\u0643\u0646 \u0628\u0639\u062f \u0630\u0644\u0643 \u064a\u0646\u062d\u0631\u0641 \u0628\u0637\u0631\u064a\u0642\u0629 \u0645\u062a\u0634\u0627\u0626\u0645\u0629.\n\u064a\u0648\u0636\u062d \u0627\u0644\u0634\u0643\u0644 \u0623\u064a\u0636\u064b\u0627 \u0623\u062f\u0627\u0621 \u0627\u0644\u062a\u062d\u0642\u0642 \u0627\u0644\u0645\u062a\u0642\u0627\u0637\u0639 3-fold \u0627\u0644\u0630\u064a\n\u064a\u0639\u0637\u064a \u0639\u0627\u062f\u0629 \u062a\u0642\u062f\u064a\u0631\u064b\u0627 \u0623\u0641\u0636\u0644 \u0644\u062e\u0633\u0627\u0631\u0629 \u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631\n\u0648\u0644\u0643\u0646\u0647 \u0623\u0643\u062b\u0631 \u062a\u0637\u0644\u0628\u064b\u0627 \u0645\u0646 \u0627\u0644\u0646\u0627\u062d\u064a\u0629 \u0627\u0644\u062d\u0633\u0627\u0628\u064a\u0629.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u0627\u0644\u0645\u0624\u0644\u0641\u0648\u0646: \u0645\u0637\u0648\u0631\u064a scikit-learn\n# \u0645\u0639\u0631\u0641 \u0627\u0644\u062a\u0631\u062e\u064a\u0635: BSD-3-Clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.special import expit\n\nfrom sklearn import ensemble\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold, train_test_split\n\n# Generate data (adapted from G. Ridgeway's gbm example)\nn_samples = 1000\nrandom_state = np.random.RandomState(13)\nx1 = random_state.uniform(size=n_samples)\nx2 = random_state.uniform(size=n_samples)\nx3 = random_state.randint(0, 4, size=n_samples)\n\np = expit(np.sin(3 * x1) - 4 * x2 + x3)\ny = random_state.binomial(1, p, size=n_samples)\n\nX = np.c_[x1, x2, x3]\n\nX = X.astype(np.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=9)\n\n# Fit classifier with out-of-bag estimates\nparams = {\n    \"n_estimators\": 1200,\n    \"max_depth\": 3,\n    \"subsample\": 0.5,\n    \"learning_rate\": 0.01,\n    \"min_samples_leaf\": 1,\n    \"random_state\": 3,\n}\nclf = ensemble.GradientBoostingClassifier(**params)\n\nclf.fit(X_train, y_train)\nacc = clf.score(X_test, y_test)\nprint(\"Accuracy: {:.4f}\".format(acc))\n\nn_estimators = params[\"n_estimators\"]\nx = np.arange(n_estimators) + 1\n\n\ndef heldout_score(clf, X_test, y_test):\n    \"\"\"compute deviance scores on ``X_test`` and ``y_test``.\"\"\"\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        score[i] = 2 * log_loss(y_test, y_proba[:, 1])\n    return score\n\n\ndef cv_estimate(n_splits=None):\n    cv = KFold(n_splits=n_splits)\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n    for train, test in cv.split(X_train, y_train):\n        cv_clf.fit(X_train[train], y_train[train])\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n    val_scores /= n_splits\n    return val_scores\n\n\n# Estimate best n_estimator using cross-validation\ncv_score = cv_estimate(3)\n\n# Compute best n_estimator for test data\ntest_score = heldout_score(clf, X_test, y_test)\n\n# negative cumulative sum of oob improvements\ncumsum = -np.cumsum(clf.oob_improvement_)\n\n# min loss according to OOB\noob_best_iter = x[np.argmin(cumsum)]\n\n# min loss according to test (normalize such that first loss is 0)\ntest_score -= test_score[0]\ntest_best_iter = x[np.argmin(test_score)]\n\n# min loss according to cv (normalize such that first loss is 0)\ncv_score -= cv_score[0]\ncv_best_iter = x[np.argmin(cv_score)]\n\n# color brew for the three curves\noob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\ntest_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\ncv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n\n# line type for the three curves\noob_line = \"dashed\"\ntest_line = \"solid\"\ncv_line = \"dashdot\"\n# line type for the three curves\noob_line = \"dashed\"\ntest_line = \"solid\"\ncv_line = \"dashdot\"\n\n# plot curves and vertical lines for best iterations\nplt.figure(figsize=(8, 4.8))\nplt.plot(x, cumsum, label=\"OOB loss\", color=oob_color, linestyle=oob_line)\nplt.plot(x, test_score, label=\"Test loss\", color=test_color, linestyle=test_line)\nplt.plot(x, cv_score, label=\"CV loss\", color=cv_color, linestyle=cv_line)\nplt.axvline(x=oob_best_iter, color=oob_color, linestyle=oob_line)\nplt.axvline(x=test_best_iter, color=test_color, linestyle=test_line)\nplt.axvline(x=cv_best_iter, color=cv_color, linestyle=cv_line)\n\n# add three vertical lines to xticks\nxticks = plt.xticks()\nxticks_pos = np.array(\n    xticks[0].tolist() + [oob_best_iter, cv_best_iter, test_best_iter]\n)\nxticks_label = np.array(list(map(lambda t: int(t), xticks[0])) + [\"OOB\", \"CV\", \"Test\"])\nind = np.argsort(xticks_pos)\nxticks_pos = xticks_pos[ind]\nxticks_label = xticks_label[ind]\nplt.xticks(xticks_pos, xticks_label, rotation=90)\n\nplt.legend(loc=\"upper center\")\nplt.ylabel(\"normalized loss\")\nplt.xlabel(\"number of iterations\")\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}