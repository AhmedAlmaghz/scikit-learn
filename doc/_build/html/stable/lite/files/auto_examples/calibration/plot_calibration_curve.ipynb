{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the scikit-learn examples in JupyterLite is experimental and you may encounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import sklearn` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/scikit-learn/scikit-learn/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Probability Calibration curves\n\nWhen performing classification one often wants to predict not only the class\nlabel, but also the associated probability. This probability gives some\nkind of confidence on the prediction. This example demonstrates how to\nvisualize how well calibrated the predicted probabilities are using calibration\ncurves, also known as reliability diagrams. Calibration of an uncalibrated\nclassifier will also be demonstrated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n\nWe will use a synthetic binary classification dataset with 100,000 samples\nand 20 features. Of the 20 features, only 2 are informative, 10 are\nredundant (random combinations of the informative features) and the\nremaining 8 are uninformative (random numbers). Of the 100,000 samples, 1,000\nwill be used for model fitting and the rest for testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(\n    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.99, random_state=42\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibration curves\n\n### Gaussian Naive Bayes\n\nFirst, we will compare:\n\n* :class:`~sklearn.linear_model.LogisticRegression` (used as baseline\n  since very often, properly regularized logistic regression is well\n  calibrated by default thanks to the use of the log-loss)\n* Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB`\n* :class:`~sklearn.naive_bayes.GaussianNB` with isotonic and sigmoid\n  calibration (see `User Guide <calibration>`)\n\nCalibration curves for all 4 conditions are plotted below, with the average\npredicted probability for each bin on the x-axis and the fraction of positive\nclasses in each bin on the y-axis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nfrom sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nlr = LogisticRegression(C=1.0)\ngnb = GaussianNB()\ngnb_isotonic = CalibratedClassifierCV(gnb, cv=2, method=\"isotonic\")\ngnb_sigmoid = CalibratedClassifierCV(gnb, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (gnb, \"Naive Bayes\"),\n    (gnb_isotonic, \"Naive Bayes + Isotonic\"),\n    (gnb_sigmoid, \"Naive Bayes + Sigmoid\"),\n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\ngs = GridSpec(4, 2)\ncolors = plt.get_cmap(\"Dark2\")\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = CalibrationDisplay.from_estimator(\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"Calibration plots (Naive Bayes)\")\n\n# Add histogram\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB` is poorly calibrated\nbecause of\nthe redundant features which violate the assumption of feature-independence\nand result in an overly confident classifier, which is indicated by the\ntypical transposed-sigmoid curve. Calibration of the probabilities of\n:class:`~sklearn.naive_bayes.GaussianNB` with `isotonic` can fix\nthis issue as can be seen from the nearly diagonal calibration curve.\n`Sigmoid regression <sigmoid_regressor>` also improves calibration\nslightly,\nalbeit not as strongly as the non-parametric isotonic regression. This can be\nattributed to the fact that we have plenty of calibration data such that the\ngreater flexibility of the non-parametric model can be exploited.\n\nBelow we will make a quantitative analysis considering several classification\nmetrics: `brier_score_loss`, `log_loss`,\n`precision, recall, F1 score <precision_recall_f_measure_metrics>` and\n`ROC AUC <roc_metrics>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nimport pandas as pd\n\nfrom sklearn.metrics import (\n    brier_score_loss,\n    f1_score,\n    log_loss,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n)\n\nscores = defaultdict(list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"Classifier\"].append(name)\n\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [precision_score, recall_score, f1_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n    score_df.round(decimals=3)\n\nscore_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that although calibration improves the `brier_score_loss` (a\nmetric composed\nof calibration term and refinement term) and `log_loss`, it does not\nsignificantly alter the prediction accuracy measures (precision, recall and\nF1 score).\nThis is because calibration should not significantly change prediction\nprobabilities at the location of the decision threshold (at x = 0.5 on the\ngraph). Calibration should however, make the predicted probabilities more\naccurate and thus more useful for making allocation decisions under\nuncertainty.\nFurther, ROC AUC, should not change at all because calibration is a\nmonotonic transformation. Indeed, no rank metrics are affected by\ncalibration.\n\n### Linear support vector classifier\nNext, we will compare:\n\n* :class:`~sklearn.linear_model.LogisticRegression` (baseline)\n* Uncalibrated :class:`~sklearn.svm.LinearSVC`. Since SVC does not output\n  probabilities by default, we naively scale the output of the\n  :term:`decision_function` into [0, 1] by applying min-max scaling.\n* :class:`~sklearn.svm.LinearSVC` with isotonic and sigmoid\n  calibration (see `User Guide <calibration>`)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom sklearn.svm import LinearSVC\n\n\nclass NaivelyCalibratedLinearSVC(LinearSVC):\n    \"\"\"LinearSVC with `predict_proba` method that naively scales\n    `decision_function` output for binary classification.\"\"\"\n\n    def fit(self, X, y):\n        super().fit(X, y)\n        df = self.decision_function(X)\n        self.df_min_ = df.min()\n        self.df_max_ = df.max()\n\n    def predict_proba(self, X):\n        \"\"\"Min-max scale output of `decision_function` to [0, 1].\"\"\"\n        df = self.decision_function(X)\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\n        proba_pos_class = np.clip(calibrated_df, 0, 1)\n        proba_neg_class = 1 - proba_pos_class\n        proba = np.c_[proba_neg_class, proba_pos_class]\n        return proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(C=1.0)\nsvc = NaivelyCalibratedLinearSVC(max_iter=10_000)\nsvc_isotonic = CalibratedClassifierCV(svc, cv=2, method=\"isotonic\")\nsvc_sigmoid = CalibratedClassifierCV(svc, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (svc, \"SVC\"),\n    (svc_isotonic, \"SVC + Isotonic\"),\n    (svc_sigmoid, \"SVC + Sigmoid\"),\n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\ngs = GridSpec(4, 2)\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = CalibrationDisplay.from_estimator(\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"Calibration plots (SVC)\")\n\n# Add histogram\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`~sklearn.svm.LinearSVC` shows the opposite\nbehavior to :class:`~sklearn.naive_bayes.GaussianNB`; the calibration\ncurve has a sigmoid shape, which is typical for an under-confident\nclassifier. In the case of :class:`~sklearn.svm.LinearSVC`, this is caused\nby the margin property of the hinge loss, which focuses on samples that are\nclose to the decision boundary (support vectors). Samples that are far\naway from the decision boundary do not impact the hinge loss. It thus makes\nsense that :class:`~sklearn.svm.LinearSVC` does not try to separate samples\nin the high confidence region regions. This leads to flatter calibration\ncurves near 0 and 1 and is empirically shown with a variety of datasets\nin Niculescu-Mizil & Caruana [1]_.\n\nBoth kinds of calibration (sigmoid and isotonic) can fix this issue and\nyield similar results.\n\nAs before, we show the `brier_score_loss`, `log_loss`,\n`precision, recall, F1 score <precision_recall_f_measure_metrics>` and\n`ROC AUC <roc_metrics>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = defaultdict(list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"Classifier\"].append(name)\n\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [precision_score, recall_score, f1_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n    score_df.round(decimals=3)\n\nscore_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As with :class:`~sklearn.naive_bayes.GaussianNB` above, calibration improves\nboth `brier_score_loss` and `log_loss` but does not alter the\nprediction accuracy measures (precision, recall and F1 score) much.\n\n## Summary\n\nParametric sigmoid calibration can deal with situations where the calibration\ncurve of the base classifier is sigmoid (e.g., for\n:class:`~sklearn.svm.LinearSVC`) but not where it is transposed-sigmoid\n(e.g., :class:`~sklearn.naive_bayes.GaussianNB`). Non-parametric\nisotonic calibration can deal with both situations but may require more\ndata to produce good results.\n\n## References\n\n.. [1] [Predicting Good Probabilities with Supervised Learning](https://dl.acm.org/doi/pdf/10.1145/1102351.1102430),\n       A. Niculescu-Mizil & R. Caruana, ICML 2005\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n==============================\n\u0645\u0646\u062d\u0646\u064a\u0627\u062a \u0645\u0639\u0627\u064a\u0631\u0629 \u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644\u064a\u0629\n==============================\n\n\u0639\u0646\u062f \u0625\u062c\u0631\u0627\u0621 \u0627\u0644\u062a\u0635\u0646\u064a\u0641\u060c \u063a\u0627\u0644\u0628\u064b\u0627 \u0645\u0627 \u064a\u0631\u063a\u0628 \u0627\u0644\u0645\u0631\u0621 \u0641\u064a \u0627\u0644\u062a\u0646\u0628\u0624 \u0644\u064a\u0633 \u0641\u0642\u0637 \u0628\u062a\u0633\u0645\u064a\u0629 \u0627\u0644\u0641\u0626\u0629\u060c\n\u0648\u0644\u0643\u0646 \u0623\u064a\u0636\u064b\u0627 \u0628\u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644\u064a\u0629 \u0627\u0644\u0645\u0631\u062a\u0628\u0637\u0629 \u0628\u0647\u0627. \u064a\u0639\u0637\u064a \u0647\u0630\u0627 \u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644 \u0646\u0648\u0639\u064b\u0627 \u0645\u0646\n\u0627\u0644\u062b\u0642\u0629 \u0641\u064a \u0627\u0644\u062a\u0646\u0628\u0624. \u064a\u0648\u0636\u062d \u0647\u0630\u0627 \u0627\u0644\u0645\u062b\u0627\u0644 \u0643\u064a\u0641\u064a\u0629\n\u062a\u0635\u0648\u0631 \u0645\u062f\u0649 \u062c\u0648\u062f\u0629 \u0645\u0639\u0627\u064a\u0631\u0629 \u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a \u0627\u0644\u0645\u062a\u0648\u0642\u0639\u0629 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0645\u0646\u062d\u0646\u064a\u0627\u062a\n\u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629\u060c \u0648\u0627\u0644\u0645\u0639\u0631\u0648\u0641\u0629 \u0623\u064a\u0636\u064b\u0627 \u0628\u0627\u0633\u0645 \u0645\u062e\u0637\u0637\u0627\u062a \u0627\u0644\u0645\u0648\u062b\u0648\u0642\u064a\u0629. \u0633\u064a\u062a\u0645 \u0623\u064a\u0636\u064b\u0627 \u062a\u0648\u0636\u064a\u062d\n\u0645\u0639\u0627\u064a\u0631\u0629 \u0645\u0635\u0646\u0641 \u063a\u064a\u0631 \u0645\u0639\u0627\u064a\u0631.\n\n\"\"\"\n\n# \u0627\u0644\u0645\u0624\u0644\u0641\u0648\u0646: \u0645\u0637\u0648\u0631\u0648 scikit-learn\n# \u0645\u064f\u0639\u0631\u0651\u0650\u0641 \u062a\u0631\u062e\u064a\u0635 SPDX: BSD-3-Clause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\n\n\u0633\u0646\u0633\u062a\u062e\u062f\u0645 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u062a\u0635\u0646\u064a\u0641 \u062b\u0646\u0627\u0626\u064a\u0629 \u062a\u0631\u0643\u064a\u0628\u064a\u0629 \u0645\u0639 100000 \u0639\u064a\u0646\u0629\n\u0648 20 \u0645\u064a\u0632\u0629. \u0645\u0646 \u0628\u064a\u0646 \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u0639\u0634\u0631\u064a\u0646\u060c \u0647\u0646\u0627\u0643 2 \u0641\u0642\u0637 \u063a\u0646\u064a\u0629 \u0628\u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0627\u062a\u060c \u0648 10\n\u0632\u0627\u0626\u062f\u0629 \u0639\u0646 \u0627\u0644\u062d\u0627\u062c\u0629 (\u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0639\u0634\u0648\u0627\u0626\u064a\u0629 \u0645\u0646 \u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u063a\u0646\u064a\u0629 \u0628\u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0627\u062a) \u0648\n\u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u062b\u0645\u0627\u0646\u064a\u0629 \u0627\u0644\u0645\u062a\u0628\u0642\u064a\u0629 \u063a\u064a\u0631 \u0645\u0641\u064a\u062f\u0629 (\u0623\u0631\u0642\u0627\u0645 \u0639\u0634\u0648\u0627\u0626\u064a\u0629). \u0645\u0646 \u0628\u064a\u0646 100000 \u0639\u064a\u0646\u0629\u060c\n\u0633\u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 1000 \u0639\u064a\u0646\u0629 \u0644\u0645\u0644\u0627\u0621\u0645\u0629 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0648\u0627\u0644\u0628\u0627\u0642\u064a \u0644\u0644\u0627\u062e\u062a\u0628\u0627\u0631.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(\n    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.99, random_state=42\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u0645\u0646\u062d\u0646\u064a\u0627\u062a \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629\n\n### Gaussian Naive Bayes\n\n\u0623\u0648\u0644\u0627\u064b\u060c \u0633\u0646\u0642\u0627\u0631\u0646:\n\n* :class:`~sklearn.linear_model.LogisticRegression` (\u064a\u064f\u0633\u062a\u062e\u062f\u0645 \u0643\u062e\u0637 \u0623\u0633\u0627\u0633\n  \u0644\u0623\u0646\u0647 \u0641\u064a \u0643\u062b\u064a\u0631 \u0645\u0646 \u0627\u0644\u0623\u062d\u064a\u0627\u0646\u060c \u064a\u0643\u0648\u0646 \u0627\u0646\u062d\u062f\u0627\u0631 \u0644\u0648\u062c\u0633\u062a\u064a \u0645\u064f\u0646\u0638\u0651\u064e\u0645 \u0628\u0634\u0643\u0644 \u0635\u062d\u064a\u062d\n  \u0645\u0639\u0627\u064a\u0631\u064b\u0627 \u062c\u064a\u062f\u064b\u0627 \u0627\u0641\u062a\u0631\u0627\u0636\u064a\u064b\u0627 \u0628\u0641\u0636\u0644 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0641\u0642\u062f\u0627\u0646 \u0627\u0644\u0633\u062c\u0644)\n* :class:`~sklearn.naive_bayes.GaussianNB` \u063a\u064a\u0631 \u0645\u0639\u0627\u064a\u0631\n* :class:`~sklearn.naive_bayes.GaussianNB` \u0645\u0639 \u0645\u0639\u0627\u064a\u0631\u0629 \u0645\u062a\u0633\u0627\u0648\u064a\u0629 \u0627\u0644\u062a\u0648\u062a\u0631 \u0648\u0633\u064a\u062c\u0645\u0648\u064a\u062f\n  (\u0627\u0646\u0638\u0631 `\u062f\u0644\u064a\u0644 \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645 <calibration>`)\n\n\u062a\u0645 \u0631\u0633\u0645 \u0645\u0646\u062d\u0646\u064a\u0627\u062a \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0644\u062c\u0645\u064a\u0639 \u0627\u0644\u0634\u0631\u0648\u0637 \u0627\u0644\u0623\u0631\u0628\u0639\u0629 \u0623\u062f\u0646\u0627\u0647\u060c \u0645\u0639 \u0645\u062a\u0648\u0633\u0637\n\u200b\u200b\u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644 \u0627\u0644\u0645\u062a\u0648\u0642\u0639 \u0644\u0643\u0644 \u0635\u0646\u062f\u0648\u0642 \u0639\u0644\u0649 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0633\u064a\u0646\u064a \u0648\u0646\u0633\u0628\u0629 \u0627\u0644\u0641\u0626\u0627\u062a \u0627\u0644\u0625\u064a\u062c\u0627\u0628\u064a\u0629\n\u0641\u064a \u0643\u0644 \u0635\u0646\u062f\u0648\u0642 \u0639\u0644\u0649 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0635\u0627\u062f\u064a.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nfrom sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nlr = LogisticRegression(C=1.0)\ngnb = GaussianNB()\ngnb_isotonic = CalibratedClassifierCV(gnb, cv=2, method=\"isotonic\")\ngnb_sigmoid = CalibratedClassifierCV(gnb, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"\u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0627\u0644\u0644\u0648\u062c\u0633\u062a\u064a\"),\n    (gnb, \"Naive Bayes\"),\n    (gnb_isotonic, \"Naive Bayes + \u0645\u062a\u0633\u0627\u0648\u064a \u0627\u0644\u062a\u0648\u062a\u0631\"),\n    (gnb_sigmoid, \"Naive Bayes + \u0633\u064a\u062c\u0645\u0648\u064a\u062f\"),\n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\ngs = GridSpec(4, 2)\ncolors = plt.get_cmap(\"Dark2\")\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = CalibrationDisplay.from_estimator(\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"\u0645\u062e\u0637\u0637\u0627\u062a \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 (Naive Bayes)\")\n\n# \u0625\u0636\u0627\u0641\u0629 \u0627\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"\u0645\u062a\u0648\u0633\u0637 \u200b\u200b\u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644 \u0627\u0644\u0645\u062a\u0648\u0642\u0639\", ylabel=\"\u0627\u0644\u0639\u062f\u062f\")\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`~sklearn.naive_bayes.GaussianNB` \u063a\u064a\u0631 \u0627\u0644\u0645\u0639\u0627\u064a\u0631 \u0645\u0639\u0627\u064a\u0631 \u0628\u0634\u0643\u0644 \u0633\u064a\u0626\n\u0628\u0633\u0628\u0628\n\u0627\u0644\u0645\u064a\u0632\u0627\u062a \u0627\u0644\u0632\u0627\u0626\u062f\u0629 \u0639\u0646 \u0627\u0644\u062d\u0627\u062c\u0629 \u0627\u0644\u062a\u064a \u062a\u0646\u062a\u0647\u0643 \u0627\u0641\u062a\u0631\u0627\u0636 \u0627\u0633\u062a\u0642\u0644\u0627\u0644 \u0627\u0644\u0645\u064a\u0632\u0627\u062a\n\u0648\u062a\u0624\u062f\u064a \u0625\u0644\u0649 \u0645\u0635\u0646\u0641 \u0645\u0641\u0631\u0637 \u0627\u0644\u062b\u0642\u0629\u060c \u0648\u0647\u0648 \u0645\u0627 \u064a\u064f\u0634\u0627\u0631 \u0625\u0644\u064a\u0647 \u0628\u0648\u0627\u0633\u0637\u0629\n\u0645\u0646\u062d\u0646\u0649 \u0633\u064a\u062c\u0645\u0648\u064a\u062f \u0627\u0644\u0645\u0646\u0642\u0648\u0644 \u0627\u0644\u0646\u0645\u0648\u0630\u062c\u064a. \u064a\u0645\u0643\u0646 \u0623\u0646 \u062a\u064f\u0639\u0627\u0644\u062c \u0645\u0639\u0627\u064a\u0631\u0629 \u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a\n\u0644\u0640 :class:`~sklearn.naive_bayes.GaussianNB` \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 `isotonic` \u0647\u0630\u0647 \u0627\u0644\u0645\u0634\u0643\u0644\u0629\n\u0643\u0645\u0627 \u064a\u062a\u0636\u062d \u0645\u0646 \u0645\u0646\u062d\u0646\u0649 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0627\u0644\u0642\u0637\u0631\u064a \u062a\u0642\u0631\u064a\u0628\u064b\u0627.\n`\u0627\u0646\u062d\u062f\u0627\u0631 \u0633\u064a\u062c\u0645\u0648\u064a\u062f <sigmoid_regressor>` \u064a\u064f\u062d\u0633\u0651\u0650\u0646 \u0623\u064a\u0636\u064b\u0627 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629\n\u0642\u0644\u064a\u0644\u0627\u064b\u060c\n\u0648\u0625\u0646 \u0644\u0645 \u064a\u0643\u0646 \u0628\u0646\u0641\u0633 \u0642\u0648\u0629 \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0627\u0644\u0645\u062a\u0633\u0627\u0648\u064a \u0627\u0644\u062a\u0648\u062a\u0631 \u063a\u064a\u0631 \u0627\u0644\u0645\u0639\u064a\u0627\u0631\u064a. \u064a\u0645\u0643\u0646\n\u0623\u0646 \u064a\u064f\u0639\u0632\u0649 \u0630\u0644\u0643 \u0625\u0644\u0649 \u062d\u0642\u064a\u0642\u0629 \u0623\u0646 \u0644\u062f\u064a\u0646\u0627 \u0627\u0644\u0643\u062b\u064a\u0631 \u0645\u0646 \u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0628\u062d\u064a\u062b\n\u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u063a\u0644\u0627\u0644 \u0645\u0631\u0648\u0646\u0629 \u0623\u0643\u0628\u0631 \u0644\u0644\u0646\u0645\u0648\u0630\u062c \u063a\u064a\u0631 \u0627\u0644\u0645\u0639\u064a\u0627\u0631\u064a.\n\n\u0623\u062f\u0646\u0627\u0647\u060c \u0633\u0646\u0642\u0648\u0645 \u0628\u0625\u062c\u0631\u0627\u0621 \u062a\u062d\u0644\u064a\u0644 \u0643\u0645\u064a \u0645\u0639 \u0627\u0644\u0623\u062e\u0630 \u0641\u064a \u0627\u0644\u0627\u0639\u062a\u0628\u0627\u0631 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0645\u0642\u0627\u064a\u064a\u0633\n\u0627\u0644\u062a\u0635\u0646\u064a\u0641: `brier_score_loss`\u060c `log_loss`\u060c\n`\u0627\u0644\u062f\u0642\u0629\u060c \u0627\u0644\u0627\u0633\u062a\u062f\u0639\u0627\u0621\u060c \u062f\u0631\u062c\u0629 F1 <precision_recall_f_measure_metrics>` \u0648\n`ROC AUC <roc_metrics>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nimport pandas as pd\n\nfrom sklearn.metrics import (\n    brier_score_loss,\n    f1_score,\n    log_loss,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n)\n\nscores = defaultdict(list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"\u0627\u0644\u0645\u0635\u0646\u0641\"].append(name)\n\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [precision_score, recall_score, f1_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = pd.DataFrame(scores).set_index(\"\u0627\u0644\u0645\u0635\u0646\u0641\")\n    score_df.round(decimals=3)\n\nscore_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0644\u0627\u062d\u0638 \u0623\u0646\u0647 \u0639\u0644\u0649 \u0627\u0644\u0631\u063a\u0645 \u0645\u0646 \u0623\u0646 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u062a\u064f\u062d\u0633\u0651\u0650\u0646 `brier_score_loss` (\u0645\u0642\u064a\u0627\u0633\n\u064a\u062a\u0643\u0648\u0646 \u0645\u0646 \u0645\u0635\u0637\u0644\u062d \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0648\u0645\u0635\u0637\u0644\u062d \u0627\u0644\u062a\u062d\u0633\u064a\u0646) \u0648 `log_loss`\u060c \u0641\u0625\u0646\u0647\u0627 \u0644\u0627\n\u062a\u064f\u063a\u064a\u0651\u0650\u0631 \u0645\u0642\u0627\u064a\u064a\u0633 \u062f\u0642\u0629 \u0627\u0644\u062a\u0646\u0628\u0624 (\u0627\u0644\u062f\u0642\u0629 \u0648\u0627\u0644\u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0648\n\u062f\u0631\u062c\u0629 F1) \u0628\u0634\u0643\u0644 \u0643\u0628\u064a\u0631.\n\u0648\u0630\u0644\u0643 \u0644\u0623\u0646 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u064a\u062c\u0628 \u0623\u0644\u0627 \u062a\u064f\u063a\u064a\u0651\u0650\u0631 \u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a \u0627\u0644\u062a\u0646\u0628\u0624 \u0628\u0634\u0643\u0644 \u0643\u0628\u064a\u0631 \u0641\u064a\n\u0645\u0648\u0642\u0639 \u0639\u062a\u0628\u0629 \u0627\u0644\u0642\u0631\u0627\u0631 (\u0639\u0646\u062f x = 0.5 \u0639\u0644\u0649\n\u0627\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a). \u0648\u0645\u0639 \u0630\u0644\u0643\u060c \u064a\u062c\u0628 \u0623\u0646 \u062a\u062c\u0639\u0644 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a \u0627\u0644\u062a\u0646\u0628\u0624 \u0623\u0643\u062b\u0631\n\u062f\u0642\u0629 \u0648\u0628\u0627\u0644\u062a\u0627\u0644\u064a \u0623\u0643\u062b\u0631 \u0641\u0627\u0626\u062f\u0629 \u0644\u0627\u062a\u062e\u0627\u0630 \u0642\u0631\u0627\u0631\u0627\u062a \u0627\u0644\u062a\u062e\u0635\u064a\u0635 \u0641\u064a \u0638\u0644\n\u0639\u062f\u0645 \u0627\u0644\u064a\u0642\u064a\u0646.\n\u0639\u0644\u0627\u0648\u0629 \u0639\u0644\u0649 \u0630\u0644\u0643\u060c \u064a\u062c\u0628 \u0623\u0644\u0627 \u064a\u062a\u063a\u064a\u0631 ROC AUC \u0639\u0644\u0649 \u0627\u0644\u0625\u0637\u0644\u0627\u0642 \u0644\u0623\u0646 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0647\u064a\n\u062a\u062d\u0648\u064a\u0644 \u0631\u062a\u064a\u0628. \u0641\u064a \u0627\u0644\u0648\u0627\u0642\u0639\u060c \u0644\u0627 \u062a\u062a\u0623\u062b\u0631 \u0645\u0642\u0627\u064a\u064a\u0633 \u0627\u0644\u0631\u062a\u0628\u0629 \u0628\u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629.\n\n### \u0645\u064f\u0635\u0646\u0651\u0650\u0641 \u0645\u062a\u062c\u0647 \u0627\u0644\u062f\u0639\u0645 \u0627\u0644\u062e\u0637\u064a\n\u0628\u0639\u062f \u0630\u0644\u0643\u060c \u0633\u0646\u0642\u0627\u0631\u0646:\n\n* :class:`~sklearn.linear_model.LogisticRegression` (\u0627\u0644\u062e\u0637 \u0627\u0644\u0623\u0633\u0627\u0633\u064a)\n* :class:`~sklearn.svm.LinearSVC` \u063a\u064a\u0631 \u0645\u0639\u0627\u064a\u0631. \u0646\u0638\u0631\u064b\u0627 \u0644\u0623\u0646 SVC \u0644\u0627 \u064a\u064f\u062e\u0631\u062c\n  \u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a \u0627\u0641\u062a\u0631\u0627\u0636\u064a\u064b\u0627\u060c \u0641\u0625\u0646\u0646\u0627 \u0646\u0642\u0648\u0645 \u0628\u062a\u063a\u064a\u064a\u0631 \u0646\u0637\u0627\u0642 \u0646\u0627\u062a\u062c\n  :term:`decision_function` \u0628\u0633\u0630\u0627\u062c\u0629 \u0625\u0644\u0649 [0\u060c 1] \u0639\u0646 \u0637\u0631\u064a\u0642 \u062a\u0637\u0628\u064a\u0642 \u062a\u063a\u064a\u064a\u0631 \u0646\u0637\u0627\u0642 \u0627\u0644\u062d\u062f \u0627\u0644\u0623\u062f\u0646\u0649-\u0627\u0644\u062d\u062f \u0627\u0644\u0623\u0642\u0635\u0649.\n* :class:`~sklearn.svm.LinearSVC` \u0645\u0639 \u0645\u0639\u0627\u064a\u0631\u0629 \u0645\u062a\u0633\u0627\u0648\u064a\u0629 \u0627\u0644\u062a\u0648\u062a\u0631 \u0648\u0633\u064a\u062c\u0645\u0648\u064a\u062f\n  (\u0627\u0646\u0638\u0631 `\u062f\u0644\u064a\u0644 \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645 <calibration>`)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom sklearn.svm import LinearSVC\n\n\nclass NaivelyCalibratedLinearSVC(LinearSVC):\n    \"\"\"LinearSVC \u0645\u0639 \u0623\u0633\u0644\u0648\u0628 `predict_proba` \u0627\u0644\u0630\u064a \u064a\u064f\u063a\u064a\u0651\u0650\u0631 \u0646\u0637\u0627\u0642\n    \u0646\u0627\u062a\u062c `decision_function` \u0628\u0633\u0630\u0627\u062c\u0629 \u0644\u0644\u062a\u0635\u0646\u064a\u0641 \u0627\u0644\u062b\u0646\u0627\u0626\u064a.\"\"\"\n\n    def fit(self, X, y):\n        super().fit(X, y)\n        df = self.decision_function(X)\n        self.df_min_ = df.min()\n        self.df_max_ = df.max()\n\n    def predict_proba(self, X):\n        \"\"\"\u062a\u063a\u064a\u064a\u0631 \u0646\u0637\u0627\u0642 \u0646\u0627\u062a\u062c `decision_function` \u0645\u0646 \u0627\u0644\u062d\u062f \u0627\u0644\u0623\u062f\u0646\u0649 \u0644\u0644\u062d\u062f \u0627\u0644\u0623\u0642\u0635\u0649 \u0625\u0644\u0649 [0\u060c 1].\"\"\"\n        df = self.decision_function(X)\n        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)\n        proba_pos_class = np.clip(calibrated_df, 0, 1)\n        proba_neg_class = 1 - proba_pos_class\n        proba = np.c_[proba_neg_class, proba_pos_class]\n        return proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(C=1.0)\nsvc = NaivelyCalibratedLinearSVC(max_iter=10_000)\nsvc_isotonic = CalibratedClassifierCV(svc, cv=2, method=\"isotonic\")\nsvc_sigmoid = CalibratedClassifierCV(svc, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"\u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0627\u0644\u0644\u0648\u062c\u0633\u062a\u064a\"),\n    (svc, \"SVC\"),\n    (svc_isotonic, \"SVC + \u0645\u062a\u0633\u0627\u0648\u064a \u0627\u0644\u062a\u0648\u062a\u0631\"),\n    (svc_sigmoid, \"SVC + \u0633\u064a\u062c\u0645\u0648\u064a\u062f\"),\n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\ngs = GridSpec(4, 2)\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = CalibrationDisplay.from_estimator(\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"\u0645\u062e\u0637\u0637\u0627\u062a \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 (SVC)\")\n\n# \u0625\u0636\u0627\u0641\u0629 \u0627\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"\u0645\u062a\u0648\u0633\u0637 \u200b\u200b\u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644 \u0627\u0644\u0645\u062a\u0648\u0642\u0639\", ylabel=\"\u0627\u0644\u0639\u062f\u062f\")\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u064a\u064f\u0638\u0647\u0631 :class:`~sklearn.svm.LinearSVC` \u0633\u0644\u0648\u0643\u064b\u0627 \u0645\u0639\u0627\u0643\u0633\u064b\u0627\n\u0644\u0640 :class:`~sklearn.naive_bayes.GaussianNB`\u061b \u0645\u0646\u062d\u0646\u0649\n\u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0644\u0647 \u0634\u0643\u0644 \u0633\u064a\u062c\u0645\u0648\u064a\u062f\u060c \u0648\u0647\u0648 \u0646\u0645\u0648\u0630\u062c\u064a \u0644\u0645\u0635\u0646\u0641 \u063a\u064a\u0631 \u0648\u0627\u062b\u0642. \u0641\u064a \u062d\u0627\u0644\u0629\n:class:`~sklearn.svm.LinearSVC`\u060c \u064a\u062d\u062f\u062b \u0647\u0630\u0627 \u0628\u0633\u0628\u0628\n\u062e\u0627\u0635\u064a\u0629 \u0627\u0644\u0647\u0627\u0645\u0634 \u0644\u0641\u0642\u062f\u0627\u0646 \u0627\u0644\u0645\u0641\u0635\u0644\u0629\u060c \u0627\u0644\u062a\u064a \u062a\u064f\u0631\u0643\u0632 \u0639\u0644\u0649 \u0627\u0644\u0639\u064a\u0646\u0627\u062a \u0627\u0644\u0642\u0631\u064a\u0628\u0629\n\u0645\u0646 \u062d\u062f \u0627\u0644\u0642\u0631\u0627\u0631 (\u0645\u062a\u062c\u0647\u0627\u062a \u0627\u0644\u062f\u0639\u0645). \u0644\u0627 \u062a\u0624\u062b\u0631 \u0627\u0644\u0639\u064a\u0646\u0627\u062a \u0627\u0644\u0628\u0639\u064a\u062f\u0629\n\u0639\u0646 \u062d\u062f \u0627\u0644\u0642\u0631\u0627\u0631 \u0639\u0644\u0649 \u0641\u0642\u062f\u0627\u0646 \u0627\u0644\u0645\u0641\u0635\u0644\u0629. \u0648\u0628\u0627\u0644\u062a\u0627\u0644\u064a\u060c \u0641\u0645\u0646\n\u0627\u0644\u0645\u0646\u0637\u0642\u064a \u0623\u0646 :class:`~sklearn.svm.LinearSVC` \u0644\u0627 \u064a\u062d\u0627\u0648\u0644 \u0641\u0635\u0644 \u0627\u0644\u0639\u064a\u0646\u0627\u062a\n\u0641\u064a \u0645\u0646\u0627\u0637\u0642 \u0645\u0646\u0637\u0642\u0629 \u0627\u0644\u062b\u0642\u0629 \u0627\u0644\u0639\u0627\u0644\u064a\u0629. \u064a\u0624\u062f\u064a \u0647\u0630\u0627 \u0625\u0644\u0649 \u062a\u0633\u0637\u064a\u062d \u0645\u0646\u062d\u0646\u064a\u0627\u062a\n\u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0628\u0627\u0644\u0642\u0631\u0628 \u0645\u0646 0 \u0648 1 \u0648\u064a\u0638\u0647\u0631 \u062a\u062c\u0631\u064a\u0628\u064a\u064b\u0627 \u0645\u0639 \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u062a\u0646\u0648\u0639\u0629 \u0645\u0646\n\u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0641\u064a Niculescu-Mizil & Caruana [1]_.\n\n\u064a\u0645\u0643\u0646 \u0644\u0643\u0644\u0627 \u0627\u0644\u0646\u0648\u0639\u064a\u0646 \u0645\u0646 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 (\u0633\u064a\u062c\u0645\u0648\u064a\u062f \u0648\u0645\u062a\u0633\u0627\u0648\u064a \u0627\u0644\u062a\u0648\u062a\u0631) \u0625\u0635\u0644\u0627\u062d \u0647\u0630\u0647 \u0627\u0644\u0645\u0634\u0643\u0644\u0629\n\u0648\u062a\u062d\u0642\u064a\u0642 \u0646\u062a\u0627\u0626\u062c \u0645\u0645\u0627\u062b\u0644\u0629.\n\n\u0643\u0645\u0627 \u0643\u0627\u0646 \u0645\u0646 \u0642\u0628\u0644\u060c \u0646\u0639\u0631\u0636 `brier_score_loss`\u060c `log_loss`\u060c\n`\u0627\u0644\u062f\u0642\u0629\u060c \u0627\u0644\u0627\u0633\u062a\u062f\u0639\u0627\u0621\u060c \u062f\u0631\u062c\u0629 F1 <precision_recall_f_measure_metrics>` \u0648\n`ROC AUC <roc_metrics>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = defaultdict(list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"\u0627\u0644\u0645\u0635\u0646\u0641\"].append(name)\n\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [precision_score, recall_score, f1_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n\n    score_df = pd.DataFrame(scores).set_index(\"\u0627\u0644\u0645\u0635\u0646\u0641\")\n    score_df.round(decimals=3)\n\nscore_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u0643\u0645\u0627 \u0647\u0648 \u0627\u0644\u062d\u0627\u0644 \u0645\u0639 :class:`~sklearn.naive_bayes.GaussianNB` \u0623\u0639\u0644\u0627\u0647\u060c \u062a\u064f\u062d\u0633\u0651\u0650\u0646 \u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629\n\u0643\u0644\u0627\u064b \u0645\u0646 `brier_score_loss` \u0648 `log_loss` \u0648\u0644\u0643\u0646\u0647\u0627 \u0644\u0627 \u062a\u064f\u063a\u064a\u0651\u0650\u0631\n\u0645\u0642\u0627\u064a\u064a\u0633 \u062f\u0642\u0629 \u0627\u0644\u062a\u0646\u0628\u0624 (\u0627\u0644\u062f\u0642\u0629 \u0648\u0627\u0644\u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0648\u062f\u0631\u062c\u0629 F1) \u0643\u062b\u064a\u0631\u064b\u0627.\n\n## \u0627\u0644\u0645\u0644\u062e\u0635\n\n\u064a\u0645\u0643\u0646 \u0623\u0646 \u062a\u062a\u0639\u0627\u0645\u0644 \u0645\u0639\u0627\u064a\u0631\u0629 \u0633\u064a\u062c\u0645\u0648\u064a\u062f \u0627\u0644\u0645\u0639\u0644\u0645\u064a\u0629 \u0645\u0639 \u0627\u0644\u0645\u0648\u0627\u0642\u0641 \u0627\u0644\u062a\u064a \u064a\u0643\u0648\u0646 \u0641\u064a\u0647\u0627 \u0645\u0646\u062d\u0646\u0649\n\u0627\u0644\u0645\u0639\u0627\u064a\u0631\u0629 \u0644\u0644\u0645\u0635\u0646\u0641 \u0627\u0644\u0623\u0633\u0627\u0633\u064a \u0633\u064a\u062c\u0645\u0648\u064a\u062f (\u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644\u060c \u0644\u0640\n:class:`~sklearn.svm.LinearSVC`) \u0648\u0644\u0643\u0646 \u0644\u064a\u0633 \u0639\u0646\u062f\u0645\u0627 \u064a\u0643\u0648\u0646 \u0633\u064a\u062c\u0645\u0648\u064a\u062f \u0645\u0646\u0642\u0648\u0644\n(\u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644\u060c :class:`~sklearn.naive_bayes.GaussianNB`). \u064a\u0645\u0643\u0646 \u0644\u0644\u0645\u0639\u0627\u064a\u0631\u0629\n\u0627\u0644\u0645\u062a\u0633\u0627\u0648\u064a\u0629 \u0627\u0644\u062a\u0648\u062a\u0631 \u063a\u064a\u0631 \u0627\u0644\u0645\u0639\u0644\u0645\u064a\u0629 \u0627\u0644\u062a\u0639\u0627\u0645\u0644 \u0645\u0639 \u0643\u0644\u0627 \u0627\u0644\u0645\u0648\u0642\u0641\u064a\u0646 \u0648\u0644\u0643\u0646\u0647\u0627 \u0642\u062f \u062a\u062a\u0637\u0644\u0628 \u0627\u0644\u0645\u0632\u064a\u062f\n\u0645\u0646 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0644\u0625\u0646\u062a\u0627\u062c \u0646\u062a\u0627\u0626\u062c \u062c\u064a\u062f\u0629.\n\n## \u0627\u0644\u0645\u0631\u0627\u062c\u0639\n\n.. [1] [\u0627\u0644\u062a\u0646\u0628\u0624 \u0628\u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a \u062c\u064a\u062f\u0629 \u0645\u0639 \u0627\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u062e\u0627\u0636\u0639 \u0644\u0644\u0625\u0634\u0631\u0627\u0641](https://dl.acm.org/doi/pdf/10.1145/1102351.1102430)\u060c\n       A. Niculescu-Mizil & R. Caruana\u060c ICML 2005\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}